{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colab Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "435g6C3gY9Yb",
        "outputId": "41847c81-8648-45df-d67f-9e79e99519a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uu7HSY5hY5Pj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "import tiktoken\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XpCVmPWQWDLO",
        "outputId": "3e2b02c2-221a-4dde-f4b6-257d2a883e98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Size: 135.47M parameters\n",
            "Estimated Model Memory Usage: 516.79 MB\n",
            "Epoch 1/100, Batch 1, Step 1: Loss = 10.984265\n",
            "Epoch 1/100, Batch 2, Step 2: Loss = 9.836699\n",
            "Epoch 1/100, Batch 3, Step 3: Loss = 9.416938\n",
            "Epoch 1/100, Batch 4, Step 4: Loss = 9.196118\n",
            "Epoch 1/100, Batch 5, Step 5: Loss = 8.933338\n",
            "Epoch 1/100, Batch 6, Step 6: Loss = 9.046812\n",
            "Epoch 1/100, Batch 7, Step 7: Loss = 8.860850\n",
            "Epoch 1/100, Batch 8, Step 8: Loss = 8.885395\n",
            "Epoch 1/100, Batch 9, Step 9: Loss = 8.777838\n",
            "Epoch 1/100, Batch 10, Step 10: Loss = 8.579698\n",
            "Epoch 1/100, Batch 11, Step 11: Loss = 8.542672\n",
            "Epoch 1/100, Batch 12, Step 12: Loss = 8.391079\n",
            "Epoch 1/100, Batch 13, Step 13: Loss = 8.531050\n",
            "Epoch 1/100, Batch 14, Step 14: Loss = 8.376135\n",
            "Epoch 1/100, Batch 15, Step 15: Loss = 8.302006\n",
            "Epoch 1/100, Batch 16, Step 16: Loss = 8.169391\n",
            "Epoch 1/100, Batch 17, Step 17: Loss = 8.161842\n",
            "Epoch 1/100, Batch 18, Step 18: Loss = 8.148483\n",
            "Epoch 1/100, Batch 19, Step 19: Loss = 7.996818\n",
            "Epoch 1/100, Batch 20, Step 20: Loss = 7.740829\n",
            "Epoch 1/100, Batch 21, Step 21: Loss = 7.707726\n",
            "Epoch 1/100, Batch 22, Step 22: Loss = 7.642082\n",
            "Epoch 1/100, Batch 23, Step 23: Loss = 7.636308\n",
            "Epoch 1/100, Batch 24, Step 24: Loss = 7.584525\n",
            "Epoch 1/100, Batch 25, Step 25: Loss = 7.506754\n",
            "Epoch 1/100, Batch 26, Step 26: Loss = 7.418399\n",
            "Epoch 1/100, Batch 27, Step 27: Loss = 7.251439\n",
            "Epoch 1/100, Batch 28, Step 28: Loss = 7.495007\n",
            "Epoch 1/100, Batch 29, Step 29: Loss = 7.216009\n",
            "Epoch 1/100, Batch 30, Step 30: Loss = 7.096806\n",
            "Epoch 1/100, Batch 31, Step 31: Loss = 7.322991\n",
            "Epoch 1/100, Batch 32, Step 32: Loss = 7.058913\n",
            "Epoch 1/100, Batch 33, Step 33: Loss = 6.897033\n",
            "Epoch 1/100, Batch 34, Step 34: Loss = 7.036500\n",
            "Epoch 1/100, Batch 35, Step 35: Loss = 6.872997\n",
            "Epoch 1/100, Batch 36, Step 36: Loss = 6.989176\n",
            "Epoch 1/100, Batch 37, Step 37: Loss = 6.874200\n",
            "Epoch 1/100, Batch 38, Step 38: Loss = 6.881425\n",
            "Epoch 1/100, Batch 39, Step 39: Loss = 6.725836\n",
            "Epoch 1/100, Batch 40, Step 40: Loss = 6.626062\n",
            "Epoch 1/100, Batch 41, Step 41: Loss = 6.925688\n",
            "Epoch 1/100, Batch 42, Step 42: Loss = 6.704953\n",
            "Epoch 1/100, Batch 43, Step 43: Loss = 6.352439\n",
            "Epoch 1/100, Batch 44, Step 44: Loss = 6.517986\n",
            "Epoch 1/100, Batch 45, Step 45: Loss = 6.435348\n",
            "Epoch 1/100, Batch 46, Step 46: Loss = 6.596599\n",
            "Epoch 1/100, Batch 47, Step 47: Loss = 6.480563\n",
            "Epoch 1/100, Batch 48, Step 48: Loss = 6.495308\n",
            "Epoch 1/100, Batch 49, Step 49: Loss = 6.469456\n",
            "Epoch 1/100, Batch 50, Step 50: Loss = 6.438669\n",
            "Epoch 1/100, Batch 51, Step 51: Loss = 6.500178\n",
            "Epoch 1/100, Batch 52, Step 52: Loss = 6.493972\n",
            "Epoch 1/100, Batch 53, Step 53: Loss = 6.126575\n",
            "Epoch 1/100, Batch 54, Step 54: Loss = 6.635318\n",
            "Epoch 1/100, Batch 55, Step 55: Loss = 6.479607\n",
            "Epoch 1/100, Batch 56, Step 56: Loss = 6.270231\n",
            "Epoch 1/100, Batch 57, Step 57: Loss = 6.476809\n",
            "Epoch 1/100, Batch 58, Step 58: Loss = 6.420895\n",
            "Epoch 1/100, Batch 59, Step 59: Loss = 6.368291\n",
            "Epoch 1/100, Batch 60, Step 60: Loss = 6.166570\n",
            "Epoch 1/100, Batch 61, Step 61: Loss = 6.464921\n",
            "Epoch 1/100, Batch 62, Step 62: Loss = 6.404760\n",
            "Epoch 1/100, Batch 63, Step 63: Loss = 6.253875\n",
            "Epoch 1/100, Batch 64, Step 64: Loss = 6.279296\n",
            "Epoch 1/100, Batch 65, Step 65: Loss = 6.375765\n",
            "Epoch 1/100, Batch 66, Step 66: Loss = 6.244002\n",
            "Epoch 1/100, Batch 67, Step 67: Loss = 6.246364\n",
            "Epoch 1/100, Batch 68, Step 68: Loss = 6.230576\n",
            "Epoch 1/100, Batch 69, Step 69: Loss = 6.184782\n",
            "Epoch 1/100, Batch 70, Step 70: Loss = 6.220193\n",
            "Epoch 1/100, Batch 71, Step 71: Loss = 6.199761\n",
            "Epoch 1/100, Batch 72, Step 72: Loss = 6.091971\n",
            "Epoch 1/100, Batch 73, Step 73: Loss = 6.182896\n",
            "Epoch 1/100, Batch 74, Step 74: Loss = 6.145336\n",
            "Epoch 1/100, Batch 75, Step 75: Loss = 6.318589\n",
            "Epoch 1/100, Batch 76, Step 76: Loss = 6.226841\n",
            "Epoch 1/100, Batch 77, Step 77: Loss = 6.069433\n",
            "Epoch 1/100, Batch 78, Step 78: Loss = 6.049415\n",
            "Epoch 1/100, Batch 79, Step 79: Loss = 6.288392\n",
            "Epoch 1/100, Batch 80, Step 80: Loss = 6.048041\n",
            "Epoch 1/100, Batch 81, Step 81: Loss = 6.011569\n",
            "Epoch 1/100, Batch 82, Step 82: Loss = 5.783920\n",
            "Epoch 1/100, Batch 83, Step 83: Loss = 6.057410\n",
            "Epoch 1/100, Batch 84, Step 84: Loss = 6.338360\n",
            "Epoch 1/100, Batch 85, Step 85: Loss = 5.923770\n",
            "Epoch 1/100, Batch 86, Step 86: Loss = 6.030256\n",
            "Epoch 1/100, Batch 87, Step 87: Loss = 5.865156\n",
            "Epoch 1/100, Batch 88, Step 88: Loss = 6.339555\n",
            "Epoch 1/100, Batch 89, Step 89: Loss = 6.237899\n",
            "Epoch 1/100, Batch 90, Step 90: Loss = 5.675092\n",
            "Epoch 1/100, Batch 91, Step 91: Loss = 6.266622\n",
            "Epoch 1/100, Batch 92, Step 92: Loss = 6.132988\n",
            "Epoch 1/100, Batch 93, Step 93: Loss = 5.794320\n",
            "Epoch 1/100, Batch 94, Step 94: Loss = 5.820337\n",
            "Epoch 1/100, Batch 95, Step 95: Loss = 6.089695\n",
            "Epoch 1/100, Batch 96, Step 96: Loss = 6.129759\n",
            "Epoch 1/100, Batch 97, Step 97: Loss = 5.965039\n",
            "Epoch 1/100, Batch 98, Step 98: Loss = 6.153901\n",
            "Epoch 1/100, Batch 99, Step 99: Loss = 5.633783\n",
            "Epoch 1/100, Batch 100, Step 100: Loss = 6.218913\n",
            "Epoch 1/100, Batch 101, Step 101: Loss = 6.192443\n",
            "Epoch 1/100, Batch 102, Step 102: Loss = 5.960846\n",
            "Epoch 1/100, Batch 103, Step 103: Loss = 5.698939\n",
            "Epoch 1/100, Batch 104, Step 104: Loss = 6.092143\n",
            "Epoch 1/100, Batch 105, Step 105: Loss = 5.902779\n",
            "Epoch 1/100, Batch 106, Step 106: Loss = 6.018449\n",
            "Epoch 1/100, Batch 107, Step 107: Loss = 5.966577\n",
            "Epoch 1/100, Batch 108, Step 108: Loss = 5.985412\n",
            "Epoch 1/100, Batch 109, Step 109: Loss = 5.694167\n",
            "Epoch 1/100, Batch 110, Step 110: Loss = 6.044925\n",
            "Epoch 1/100, Batch 111, Step 111: Loss = 6.063660\n",
            "Epoch 1/100, Batch 112, Step 112: Loss = 5.825193\n",
            "Epoch 1/100, Batch 113, Step 113: Loss = 6.073183\n",
            "Epoch 1/100, Batch 114, Step 114: Loss = 5.851182\n",
            "Epoch 1/100, Batch 115, Step 115: Loss = 5.898951\n",
            "Epoch 1/100, Batch 116, Step 116: Loss = 5.946446\n",
            "Epoch 1/100, Batch 117, Step 117: Loss = 5.837265\n",
            "Epoch 1/100, Batch 118, Step 118: Loss = 5.706248\n",
            "Epoch 1/100, Batch 119, Step 119: Loss = 5.845573\n",
            "Epoch 1/100, Batch 120, Step 120: Loss = 5.837553\n",
            "Epoch 1/100, Batch 121, Step 121: Loss = 5.773963\n",
            "Epoch 1/100, Batch 122, Step 122: Loss = 5.751563\n",
            "Epoch 1/100, Batch 123, Step 123: Loss = 5.912696\n",
            "Epoch 1/100, Batch 124, Step 124: Loss = 6.152356\n",
            "Epoch 1/100, Batch 125, Step 125: Loss = 6.034974\n",
            "Epoch 1/100, Batch 126, Step 126: Loss = 5.868842\n",
            "Epoch 1/100, Batch 127, Step 127: Loss = 5.682243\n",
            "Epoch 1/100, Batch 128, Step 128: Loss = 5.771248\n",
            "Epoch 1/100, Batch 129, Step 129: Loss = 5.763918\n",
            "Epoch 1/100, Batch 130, Step 130: Loss = 5.790916\n",
            "Epoch 1/100, Batch 131, Step 131: Loss = 5.803996\n",
            "Epoch 1/100, Batch 132, Step 132: Loss = 5.603481\n",
            "Epoch 1/100, Batch 133, Step 133: Loss = 6.025411\n",
            "Epoch 1/100, Batch 134, Step 134: Loss = 5.589902\n",
            "Epoch 1/100, Batch 135, Step 135: Loss = 5.610252\n",
            "Epoch 1/100, Batch 136, Step 136: Loss = 5.623104\n",
            "Epoch 1/100, Batch 137, Step 137: Loss = 5.714048\n",
            "Epoch 1/100, Batch 138, Step 138: Loss = 5.680900\n",
            "Epoch 1/100, Batch 139, Step 139: Loss = 5.666960\n",
            "Epoch 1/100, Batch 140, Step 140: Loss = 5.833179\n",
            "Epoch 1/100, Batch 141, Step 141: Loss = 5.763080\n",
            "Epoch 1/100, Batch 142, Step 142: Loss = 5.641892\n",
            "Epoch 1/100, Batch 143, Step 143: Loss = 5.789504\n",
            "Epoch 1/100, Batch 144, Step 144: Loss = 5.494477\n",
            "Epoch 1/100, Batch 145, Step 145: Loss = 5.789251\n",
            "Epoch 1/100, Batch 146, Step 146: Loss = 5.698915\n",
            "Epoch 1/100, Batch 147, Step 147: Loss = 5.611324\n",
            "Epoch 1/100, Batch 148, Step 148: Loss = 5.572070\n",
            "Epoch 1/100, Batch 149, Step 149: Loss = 5.708922\n",
            "Epoch 1/100, Batch 150, Step 150: Loss = 5.535668\n",
            "Epoch 1/100, Batch 151, Step 151: Loss = 5.661450\n",
            "Epoch 1/100, Batch 152, Step 152: Loss = 5.683342\n",
            "Epoch 1/100, Batch 153, Step 153: Loss = 5.492239\n",
            "Epoch 1/100, Batch 154, Step 154: Loss = 5.419695\n",
            "Epoch 1/100, Batch 155, Step 155: Loss = 5.588068\n",
            "Epoch 1/100, Batch 156, Step 156: Loss = 5.325202\n",
            "Epoch 1/100, Batch 157, Step 157: Loss = 5.946568\n",
            "Epoch 1/100, Batch 158, Step 158: Loss = 5.048223\n",
            "Epoch 1/100, Batch 159, Step 159: Loss = 5.682670\n",
            "Epoch 1/100, Batch 160, Step 160: Loss = 5.813457\n",
            "Epoch 1/100, Batch 161, Step 161: Loss = 5.629688\n",
            "Epoch 1/100, Batch 162, Step 162: Loss = 5.769181\n",
            "Epoch 1/100, Batch 163, Step 163: Loss = 5.254612\n",
            "Epoch 1/100, Batch 164, Step 164: Loss = 5.580303\n",
            "Epoch 1/100, Batch 165, Step 165: Loss = 5.405444\n",
            " Epoch 1/100, Avg Loss: 6.467082\n",
            "Checkpoint saved..\n",
            "Epoch 2/100, Batch 1, Step 166: Loss = 5.400275\n",
            "Epoch 2/100, Batch 2, Step 167: Loss = 5.210217\n",
            "Epoch 2/100, Batch 3, Step 168: Loss = 5.030975\n",
            "Epoch 2/100, Batch 4, Step 169: Loss = 5.479399\n",
            "Epoch 2/100, Batch 5, Step 170: Loss = 5.301607\n",
            "Epoch 2/100, Batch 6, Step 171: Loss = 5.388960\n",
            "Epoch 2/100, Batch 7, Step 172: Loss = 5.331382\n",
            "Epoch 2/100, Batch 8, Step 173: Loss = 5.676320\n",
            "Epoch 2/100, Batch 9, Step 174: Loss = 5.606242\n",
            "Epoch 2/100, Batch 10, Step 175: Loss = 5.344563\n",
            "Epoch 2/100, Batch 11, Step 176: Loss = 5.416025\n",
            "Epoch 2/100, Batch 12, Step 177: Loss = 5.485318\n",
            "Epoch 2/100, Batch 13, Step 178: Loss = 5.522170\n",
            "Epoch 2/100, Batch 14, Step 179: Loss = 5.415205\n",
            "Epoch 2/100, Batch 15, Step 180: Loss = 5.605477\n",
            "Epoch 2/100, Batch 16, Step 181: Loss = 5.512009\n",
            "Epoch 2/100, Batch 17, Step 182: Loss = 5.580193\n",
            "Epoch 2/100, Batch 18, Step 183: Loss = 5.555241\n",
            "Epoch 2/100, Batch 19, Step 184: Loss = 5.208999\n",
            "Epoch 2/100, Batch 20, Step 185: Loss = 5.529173\n",
            "Epoch 2/100, Batch 21, Step 186: Loss = 5.570330\n",
            "Epoch 2/100, Batch 22, Step 187: Loss = 5.085863\n",
            "Epoch 2/100, Batch 23, Step 188: Loss = 5.388609\n",
            "Epoch 2/100, Batch 24, Step 189: Loss = 5.270095\n",
            "Epoch 2/100, Batch 25, Step 190: Loss = 5.698054\n",
            "Epoch 2/100, Batch 26, Step 191: Loss = 5.420886\n",
            "Epoch 2/100, Batch 27, Step 192: Loss = 5.335091\n",
            "Epoch 2/100, Batch 28, Step 193: Loss = 5.444567\n",
            "Epoch 2/100, Batch 29, Step 194: Loss = 5.209024\n",
            "Epoch 2/100, Batch 30, Step 195: Loss = 5.230363\n",
            "Epoch 2/100, Batch 31, Step 196: Loss = 5.408251\n",
            "Epoch 2/100, Batch 32, Step 197: Loss = 5.837064\n",
            "Epoch 2/100, Batch 33, Step 198: Loss = 5.608808\n",
            "Epoch 2/100, Batch 34, Step 199: Loss = 5.495752\n",
            "Epoch 2/100, Batch 35, Step 200: Loss = 5.190358\n",
            "Epoch 2/100, Batch 36, Step 201: Loss = 5.283498\n",
            "Epoch 2/100, Batch 37, Step 202: Loss = 5.057566\n",
            "Epoch 2/100, Batch 38, Step 203: Loss = 5.240408\n",
            "Epoch 2/100, Batch 39, Step 204: Loss = 5.232416\n",
            "Epoch 2/100, Batch 40, Step 205: Loss = 4.846973\n",
            "Epoch 2/100, Batch 41, Step 206: Loss = 5.275617\n",
            "Epoch 2/100, Batch 42, Step 207: Loss = 5.575194\n",
            "Epoch 2/100, Batch 43, Step 208: Loss = 5.119082\n",
            "Epoch 2/100, Batch 44, Step 209: Loss = 5.218630\n",
            "Epoch 2/100, Batch 45, Step 210: Loss = 5.054770\n",
            "Epoch 2/100, Batch 46, Step 211: Loss = 5.254481\n",
            "Epoch 2/100, Batch 47, Step 212: Loss = 5.093360\n",
            "Epoch 2/100, Batch 48, Step 213: Loss = 4.921790\n",
            "Epoch 2/100, Batch 49, Step 214: Loss = 5.142423\n",
            "Epoch 2/100, Batch 50, Step 215: Loss = 5.165311\n",
            "Epoch 2/100, Batch 51, Step 216: Loss = 5.447793\n",
            "Epoch 2/100, Batch 52, Step 217: Loss = 5.246003\n",
            "Epoch 2/100, Batch 53, Step 218: Loss = 5.537763\n",
            "Epoch 2/100, Batch 54, Step 219: Loss = 5.140999\n",
            "Epoch 2/100, Batch 55, Step 220: Loss = 5.133601\n",
            "Epoch 2/100, Batch 56, Step 221: Loss = 4.985489\n",
            "Epoch 2/100, Batch 57, Step 222: Loss = 5.267837\n",
            "Epoch 2/100, Batch 58, Step 223: Loss = 5.111063\n",
            "Epoch 2/100, Batch 59, Step 224: Loss = 5.071732\n",
            "Epoch 2/100, Batch 60, Step 225: Loss = 4.975667\n",
            "Epoch 2/100, Batch 61, Step 226: Loss = 5.325587\n",
            "Epoch 2/100, Batch 62, Step 227: Loss = 5.267282\n",
            "Epoch 2/100, Batch 63, Step 228: Loss = 5.181169\n",
            "Epoch 2/100, Batch 64, Step 229: Loss = 4.925642\n",
            "Epoch 2/100, Batch 65, Step 230: Loss = 5.275839\n",
            "Epoch 2/100, Batch 66, Step 231: Loss = 5.364878\n",
            "Epoch 2/100, Batch 67, Step 232: Loss = 5.338638\n",
            "Epoch 2/100, Batch 68, Step 233: Loss = 5.083038\n",
            "Epoch 2/100, Batch 69, Step 234: Loss = 5.089494\n",
            "Epoch 2/100, Batch 70, Step 235: Loss = 4.876421\n",
            "Epoch 2/100, Batch 71, Step 236: Loss = 5.174829\n",
            "Epoch 2/100, Batch 72, Step 237: Loss = 5.239392\n",
            "Epoch 2/100, Batch 73, Step 238: Loss = 4.821170\n",
            "Epoch 2/100, Batch 74, Step 239: Loss = 4.872294\n",
            "Epoch 2/100, Batch 75, Step 240: Loss = 5.237267\n",
            "Epoch 2/100, Batch 76, Step 241: Loss = 5.070024\n",
            "Epoch 2/100, Batch 77, Step 242: Loss = 5.147151\n",
            "Epoch 2/100, Batch 78, Step 243: Loss = 5.015306\n",
            "Epoch 2/100, Batch 79, Step 244: Loss = 5.112850\n",
            "Epoch 2/100, Batch 80, Step 245: Loss = 5.206693\n",
            "Epoch 2/100, Batch 81, Step 246: Loss = 4.766737\n",
            "Epoch 2/100, Batch 82, Step 247: Loss = 5.180664\n",
            "Epoch 2/100, Batch 83, Step 248: Loss = 4.942545\n",
            "Epoch 2/100, Batch 84, Step 249: Loss = 4.959394\n",
            "Epoch 2/100, Batch 85, Step 250: Loss = 5.010187\n",
            "Epoch 2/100, Batch 86, Step 251: Loss = 5.011782\n",
            "Epoch 2/100, Batch 87, Step 252: Loss = 5.008584\n",
            "Epoch 2/100, Batch 88, Step 253: Loss = 5.277394\n",
            "Epoch 2/100, Batch 89, Step 254: Loss = 5.158736\n",
            "Epoch 2/100, Batch 90, Step 255: Loss = 4.905794\n",
            "Epoch 2/100, Batch 91, Step 256: Loss = 4.796608\n",
            "Epoch 2/100, Batch 92, Step 257: Loss = 4.977595\n",
            "Epoch 2/100, Batch 93, Step 258: Loss = 5.193964\n",
            "Epoch 2/100, Batch 94, Step 259: Loss = 4.839865\n",
            "Epoch 2/100, Batch 95, Step 260: Loss = 5.194246\n",
            "Epoch 2/100, Batch 96, Step 261: Loss = 5.091141\n",
            "Epoch 2/100, Batch 97, Step 262: Loss = 5.232343\n",
            "Epoch 2/100, Batch 98, Step 263: Loss = 4.821123\n",
            "Epoch 2/100, Batch 99, Step 264: Loss = 5.130513\n",
            "Epoch 2/100, Batch 100, Step 265: Loss = 4.985262\n",
            "Epoch 2/100, Batch 101, Step 266: Loss = 5.142445\n",
            "Epoch 2/100, Batch 102, Step 267: Loss = 5.129213\n",
            "Epoch 2/100, Batch 103, Step 268: Loss = 5.154056\n",
            "Epoch 2/100, Batch 104, Step 269: Loss = 4.899857\n",
            "Epoch 2/100, Batch 105, Step 270: Loss = 5.072346\n",
            "Epoch 2/100, Batch 106, Step 271: Loss = 5.217208\n",
            "Epoch 2/100, Batch 107, Step 272: Loss = 5.191193\n",
            "Epoch 2/100, Batch 108, Step 273: Loss = 4.953362\n",
            "Epoch 2/100, Batch 109, Step 274: Loss = 5.091057\n",
            "Epoch 2/100, Batch 110, Step 275: Loss = 4.901573\n",
            "Epoch 2/100, Batch 111, Step 276: Loss = 5.081677\n",
            "Epoch 2/100, Batch 112, Step 277: Loss = 4.977010\n",
            "Epoch 2/100, Batch 113, Step 278: Loss = 5.113055\n",
            "Epoch 2/100, Batch 114, Step 279: Loss = 4.728046\n",
            "Epoch 2/100, Batch 115, Step 280: Loss = 4.695561\n",
            "Epoch 2/100, Batch 116, Step 281: Loss = 4.902555\n",
            "Epoch 2/100, Batch 117, Step 282: Loss = 4.994187\n",
            "Epoch 2/100, Batch 118, Step 283: Loss = 4.778045\n",
            "Epoch 2/100, Batch 119, Step 284: Loss = 4.840930\n",
            "Epoch 2/100, Batch 120, Step 285: Loss = 5.156098\n",
            "Epoch 2/100, Batch 121, Step 286: Loss = 5.223701\n",
            "Epoch 2/100, Batch 122, Step 287: Loss = 5.135368\n",
            "Epoch 2/100, Batch 123, Step 288: Loss = 5.491574\n",
            "Epoch 2/100, Batch 124, Step 289: Loss = 5.125359\n",
            "Epoch 2/100, Batch 125, Step 290: Loss = 5.177505\n",
            "Epoch 2/100, Batch 126, Step 291: Loss = 5.060969\n",
            "Epoch 2/100, Batch 127, Step 292: Loss = 4.763708\n",
            "Epoch 2/100, Batch 128, Step 293: Loss = 4.806177\n",
            "Epoch 2/100, Batch 129, Step 294: Loss = 5.019949\n",
            "Epoch 2/100, Batch 130, Step 295: Loss = 4.714476\n",
            "Epoch 2/100, Batch 131, Step 296: Loss = 5.047138\n",
            "Epoch 2/100, Batch 132, Step 297: Loss = 5.065620\n",
            "Epoch 2/100, Batch 133, Step 298: Loss = 4.984176\n",
            "Epoch 2/100, Batch 134, Step 299: Loss = 4.924082\n",
            "Epoch 2/100, Batch 135, Step 300: Loss = 4.894290\n",
            "Epoch 2/100, Batch 136, Step 301: Loss = 5.135735\n",
            "Epoch 2/100, Batch 137, Step 302: Loss = 5.289617\n",
            "Epoch 2/100, Batch 138, Step 303: Loss = 5.229265\n",
            "Epoch 2/100, Batch 139, Step 304: Loss = 5.285534\n",
            "Epoch 2/100, Batch 140, Step 305: Loss = 4.859945\n",
            "Epoch 2/100, Batch 141, Step 306: Loss = 5.120153\n",
            "Epoch 2/100, Batch 142, Step 307: Loss = 4.968303\n",
            "Epoch 2/100, Batch 143, Step 308: Loss = 4.960096\n",
            "Epoch 2/100, Batch 144, Step 309: Loss = 4.683260\n",
            "Epoch 2/100, Batch 145, Step 310: Loss = 4.898375\n",
            "Epoch 2/100, Batch 146, Step 311: Loss = 5.299467\n",
            "Epoch 2/100, Batch 147, Step 312: Loss = 4.859695\n",
            "Epoch 2/100, Batch 148, Step 313: Loss = 5.208926\n",
            "Epoch 2/100, Batch 149, Step 314: Loss = 4.764185\n",
            "Epoch 2/100, Batch 150, Step 315: Loss = 4.996277\n",
            "Epoch 2/100, Batch 151, Step 316: Loss = 5.083549\n",
            "Epoch 2/100, Batch 152, Step 317: Loss = 5.065598\n",
            "Epoch 2/100, Batch 153, Step 318: Loss = 4.871222\n",
            "Epoch 2/100, Batch 154, Step 319: Loss = 5.472570\n",
            "Epoch 2/100, Batch 155, Step 320: Loss = 4.894985\n",
            "Epoch 2/100, Batch 156, Step 321: Loss = 5.100434\n",
            "Epoch 2/100, Batch 157, Step 322: Loss = 4.893509\n",
            "Epoch 2/100, Batch 158, Step 323: Loss = 4.936324\n",
            "Epoch 2/100, Batch 159, Step 324: Loss = 5.084997\n",
            "Epoch 2/100, Batch 160, Step 325: Loss = 4.717176\n",
            "Epoch 2/100, Batch 161, Step 326: Loss = 5.066906\n",
            "Epoch 2/100, Batch 162, Step 327: Loss = 5.087840\n",
            "Epoch 2/100, Batch 163, Step 328: Loss = 4.635525\n",
            "Epoch 2/100, Batch 164, Step 329: Loss = 5.287911\n",
            "Epoch 2/100, Batch 165, Step 330: Loss = 4.861295\n",
            " Epoch 2/100, Avg Loss: 5.141661\n",
            "Checkpoint saved..\n",
            "Epoch 3/100, Batch 1, Step 331: Loss = 4.716679\n",
            "Epoch 3/100, Batch 2, Step 332: Loss = 4.521471\n",
            "Epoch 3/100, Batch 3, Step 333: Loss = 4.730412\n",
            "Epoch 3/100, Batch 4, Step 334: Loss = 4.545628\n",
            "Epoch 3/100, Batch 5, Step 335: Loss = 4.718503\n",
            "Epoch 3/100, Batch 6, Step 336: Loss = 4.425370\n",
            "Epoch 3/100, Batch 7, Step 337: Loss = 4.952011\n",
            "Epoch 3/100, Batch 8, Step 338: Loss = 4.897858\n",
            "Epoch 3/100, Batch 9, Step 339: Loss = 4.746146\n",
            "Epoch 3/100, Batch 10, Step 340: Loss = 4.542049\n",
            "Epoch 3/100, Batch 11, Step 341: Loss = 4.726114\n",
            "Epoch 3/100, Batch 12, Step 342: Loss = 4.848168\n",
            "Epoch 3/100, Batch 13, Step 343: Loss = 4.807788\n",
            "Epoch 3/100, Batch 14, Step 344: Loss = 5.063125\n",
            "Epoch 3/100, Batch 15, Step 345: Loss = 4.837219\n",
            "Epoch 3/100, Batch 16, Step 346: Loss = 5.054292\n",
            "Epoch 3/100, Batch 17, Step 347: Loss = 5.294945\n",
            "Epoch 3/100, Batch 18, Step 348: Loss = 4.790494\n",
            "Epoch 3/100, Batch 19, Step 349: Loss = 4.977563\n",
            "Epoch 3/100, Batch 20, Step 350: Loss = 4.655979\n",
            "Epoch 3/100, Batch 21, Step 351: Loss = 4.886933\n",
            "Epoch 3/100, Batch 22, Step 352: Loss = 4.966816\n",
            "Epoch 3/100, Batch 23, Step 353: Loss = 4.814395\n",
            "Epoch 3/100, Batch 24, Step 354: Loss = 4.708734\n",
            "Epoch 3/100, Batch 25, Step 355: Loss = 4.691861\n",
            "Epoch 3/100, Batch 26, Step 356: Loss = 4.349206\n",
            "Epoch 3/100, Batch 27, Step 357: Loss = 4.685452\n",
            "Epoch 3/100, Batch 28, Step 358: Loss = 4.342423\n",
            "Epoch 3/100, Batch 29, Step 359: Loss = 4.738753\n",
            "Epoch 3/100, Batch 30, Step 360: Loss = 4.554068\n",
            "Epoch 3/100, Batch 31, Step 361: Loss = 4.579460\n",
            "Epoch 3/100, Batch 32, Step 362: Loss = 4.653560\n",
            "Epoch 3/100, Batch 33, Step 363: Loss = 4.781362\n",
            "Epoch 3/100, Batch 34, Step 364: Loss = 4.195820\n",
            "Epoch 3/100, Batch 35, Step 365: Loss = 4.928443\n",
            "Epoch 3/100, Batch 36, Step 366: Loss = 4.917161\n",
            "Epoch 3/100, Batch 37, Step 367: Loss = 4.888420\n",
            "Epoch 3/100, Batch 38, Step 368: Loss = 4.781374\n",
            "Epoch 3/100, Batch 39, Step 369: Loss = 4.454377\n",
            "Epoch 3/100, Batch 40, Step 370: Loss = 4.565226\n",
            "Epoch 3/100, Batch 41, Step 371: Loss = 4.593759\n",
            "Epoch 3/100, Batch 42, Step 372: Loss = 4.448525\n",
            "Epoch 3/100, Batch 43, Step 373: Loss = 4.732181\n",
            "Epoch 3/100, Batch 44, Step 374: Loss = 4.630424\n",
            "Epoch 3/100, Batch 45, Step 375: Loss = 4.505284\n",
            "Epoch 3/100, Batch 46, Step 376: Loss = 4.739186\n",
            "Epoch 3/100, Batch 47, Step 377: Loss = 4.609245\n",
            "Epoch 3/100, Batch 48, Step 378: Loss = 4.560590\n",
            "Epoch 3/100, Batch 49, Step 379: Loss = 4.607069\n",
            "Epoch 3/100, Batch 50, Step 380: Loss = 4.723539\n",
            "Epoch 3/100, Batch 51, Step 381: Loss = 4.909640\n",
            "Epoch 3/100, Batch 52, Step 382: Loss = 4.751557\n",
            "Epoch 3/100, Batch 53, Step 383: Loss = 4.799092\n",
            "Epoch 3/100, Batch 54, Step 384: Loss = 5.071736\n",
            "Epoch 3/100, Batch 55, Step 385: Loss = 4.739631\n",
            "Epoch 3/100, Batch 56, Step 386: Loss = 4.884253\n",
            "Epoch 3/100, Batch 57, Step 387: Loss = 4.655958\n",
            "Epoch 3/100, Batch 58, Step 388: Loss = 4.733594\n",
            "Epoch 3/100, Batch 59, Step 389: Loss = 4.756346\n",
            "Epoch 3/100, Batch 60, Step 390: Loss = 4.253565\n",
            "Epoch 3/100, Batch 61, Step 391: Loss = 4.766876\n",
            "Epoch 3/100, Batch 62, Step 392: Loss = 4.907159\n",
            "Epoch 3/100, Batch 63, Step 393: Loss = 4.616707\n",
            "Epoch 3/100, Batch 64, Step 394: Loss = 4.686529\n",
            "Epoch 3/100, Batch 65, Step 395: Loss = 4.744793\n",
            "Epoch 3/100, Batch 66, Step 396: Loss = 5.239878\n",
            "Epoch 3/100, Batch 67, Step 397: Loss = 4.779039\n",
            "Epoch 3/100, Batch 68, Step 398: Loss = 4.512659\n",
            "Epoch 3/100, Batch 69, Step 399: Loss = 4.532360\n",
            "Epoch 3/100, Batch 70, Step 400: Loss = 4.855649\n",
            "Epoch 3/100, Batch 71, Step 401: Loss = 4.924621\n",
            "Epoch 3/100, Batch 72, Step 402: Loss = 4.896418\n",
            "Epoch 3/100, Batch 73, Step 403: Loss = 4.406214\n",
            "Epoch 3/100, Batch 74, Step 404: Loss = 4.498667\n",
            "Epoch 3/100, Batch 75, Step 405: Loss = 4.743656\n",
            "Epoch 3/100, Batch 76, Step 406: Loss = 4.964694\n",
            "Epoch 3/100, Batch 77, Step 407: Loss = 4.375008\n",
            "Epoch 3/100, Batch 78, Step 408: Loss = 4.698888\n",
            "Epoch 3/100, Batch 79, Step 409: Loss = 4.152406\n",
            "Epoch 3/100, Batch 80, Step 410: Loss = 4.812556\n",
            "Epoch 3/100, Batch 81, Step 411: Loss = 4.896072\n",
            "Epoch 3/100, Batch 82, Step 412: Loss = 4.760254\n",
            "Epoch 3/100, Batch 83, Step 413: Loss = 4.589742\n",
            "Epoch 3/100, Batch 84, Step 414: Loss = 5.022512\n",
            "Epoch 3/100, Batch 85, Step 415: Loss = 4.690175\n",
            "Epoch 3/100, Batch 86, Step 416: Loss = 4.548465\n",
            "Epoch 3/100, Batch 87, Step 417: Loss = 4.590067\n",
            "Epoch 3/100, Batch 88, Step 418: Loss = 4.540754\n",
            "Epoch 3/100, Batch 89, Step 419: Loss = 4.844343\n",
            "Epoch 3/100, Batch 90, Step 420: Loss = 4.423757\n",
            "Epoch 3/100, Batch 91, Step 421: Loss = 4.781857\n",
            "Epoch 3/100, Batch 92, Step 422: Loss = 4.977468\n",
            "Epoch 3/100, Batch 93, Step 423: Loss = 4.884443\n",
            "Epoch 3/100, Batch 94, Step 424: Loss = 4.852682\n",
            "Epoch 3/100, Batch 95, Step 425: Loss = 4.634531\n",
            "Epoch 3/100, Batch 96, Step 426: Loss = 4.935508\n",
            "Epoch 3/100, Batch 97, Step 427: Loss = 4.522846\n",
            "Epoch 3/100, Batch 98, Step 428: Loss = 4.907205\n",
            "Epoch 3/100, Batch 99, Step 429: Loss = 5.000130\n",
            "Epoch 3/100, Batch 100, Step 430: Loss = 4.486795\n",
            "Epoch 3/100, Batch 101, Step 431: Loss = 4.897266\n",
            "Epoch 3/100, Batch 102, Step 432: Loss = 4.888323\n",
            "Epoch 3/100, Batch 103, Step 433: Loss = 4.230525\n",
            "Epoch 3/100, Batch 104, Step 434: Loss = 4.483972\n",
            "Epoch 3/100, Batch 105, Step 435: Loss = 4.542814\n",
            "Epoch 3/100, Batch 106, Step 436: Loss = 4.818595\n",
            "Epoch 3/100, Batch 107, Step 437: Loss = 4.804076\n",
            "Epoch 3/100, Batch 108, Step 438: Loss = 4.536459\n",
            "Epoch 3/100, Batch 109, Step 439: Loss = 4.611995\n",
            "Epoch 3/100, Batch 110, Step 440: Loss = 4.593957\n",
            "Epoch 3/100, Batch 111, Step 441: Loss = 4.670465\n",
            "Epoch 3/100, Batch 112, Step 442: Loss = 4.428644\n",
            "Epoch 3/100, Batch 113, Step 443: Loss = 4.426877\n",
            "Epoch 3/100, Batch 114, Step 444: Loss = 4.854385\n",
            "Epoch 3/100, Batch 115, Step 445: Loss = 4.335632\n",
            "Epoch 3/100, Batch 116, Step 446: Loss = 4.700976\n",
            "Epoch 3/100, Batch 117, Step 447: Loss = 4.630053\n",
            "Epoch 3/100, Batch 118, Step 448: Loss = 4.798292\n",
            "Epoch 3/100, Batch 119, Step 449: Loss = 4.932621\n",
            "Epoch 3/100, Batch 120, Step 450: Loss = 4.502681\n",
            "Epoch 3/100, Batch 121, Step 451: Loss = 4.661376\n",
            "Epoch 3/100, Batch 122, Step 452: Loss = 4.774376\n",
            "Epoch 3/100, Batch 123, Step 453: Loss = 4.482336\n",
            "Epoch 3/100, Batch 124, Step 454: Loss = 4.275846\n",
            "Epoch 3/100, Batch 125, Step 455: Loss = 4.682502\n",
            "Epoch 3/100, Batch 126, Step 456: Loss = 4.584656\n",
            "Epoch 3/100, Batch 127, Step 457: Loss = 4.534861\n",
            "Epoch 3/100, Batch 128, Step 458: Loss = 4.388928\n",
            "Epoch 3/100, Batch 129, Step 459: Loss = 4.767967\n",
            "Epoch 3/100, Batch 130, Step 460: Loss = 4.444914\n",
            "Epoch 3/100, Batch 131, Step 461: Loss = 4.592211\n",
            "Epoch 3/100, Batch 132, Step 462: Loss = 4.595370\n",
            "Epoch 3/100, Batch 133, Step 463: Loss = 4.727715\n",
            "Epoch 3/100, Batch 134, Step 464: Loss = 4.722485\n",
            "Epoch 3/100, Batch 135, Step 465: Loss = 4.861547\n",
            "Epoch 3/100, Batch 136, Step 466: Loss = 4.841022\n",
            "Epoch 3/100, Batch 137, Step 467: Loss = 4.627409\n",
            "Epoch 3/100, Batch 138, Step 468: Loss = 4.792845\n",
            "Epoch 3/100, Batch 139, Step 469: Loss = 4.763564\n",
            "Epoch 3/100, Batch 140, Step 470: Loss = 5.154001\n",
            "Epoch 3/100, Batch 141, Step 471: Loss = 4.375739\n",
            "Epoch 3/100, Batch 142, Step 472: Loss = 4.272077\n",
            "Epoch 3/100, Batch 143, Step 473: Loss = 4.506482\n",
            "Epoch 3/100, Batch 144, Step 474: Loss = 4.381575\n",
            "Epoch 3/100, Batch 145, Step 475: Loss = 5.040061\n",
            "Epoch 3/100, Batch 146, Step 476: Loss = 4.969493\n",
            "Epoch 3/100, Batch 147, Step 477: Loss = 4.410530\n",
            "Epoch 3/100, Batch 148, Step 478: Loss = 4.668448\n",
            "Epoch 3/100, Batch 149, Step 479: Loss = 4.818027\n",
            "Epoch 3/100, Batch 150, Step 480: Loss = 4.734100\n",
            "Epoch 3/100, Batch 151, Step 481: Loss = 4.517410\n",
            "Epoch 3/100, Batch 152, Step 482: Loss = 4.823406\n",
            "Epoch 3/100, Batch 153, Step 483: Loss = 4.363187\n",
            "Epoch 3/100, Batch 154, Step 484: Loss = 3.934946\n",
            "Epoch 3/100, Batch 155, Step 485: Loss = 4.829945\n",
            "Epoch 3/100, Batch 156, Step 486: Loss = 4.884853\n",
            "Epoch 3/100, Batch 157, Step 487: Loss = 4.738166\n",
            "Epoch 3/100, Batch 158, Step 488: Loss = 4.454808\n",
            "Epoch 3/100, Batch 159, Step 489: Loss = 4.409814\n",
            "Epoch 3/100, Batch 160, Step 490: Loss = 4.737235\n",
            "Epoch 3/100, Batch 161, Step 491: Loss = 4.262720\n",
            "Epoch 3/100, Batch 162, Step 492: Loss = 4.400803\n",
            "Epoch 3/100, Batch 163, Step 493: Loss = 4.798378\n",
            "Epoch 3/100, Batch 164, Step 494: Loss = 4.700023\n",
            "Epoch 3/100, Batch 165, Step 495: Loss = 4.559264\n",
            " Epoch 3/100, Avg Loss: 4.682432\n",
            "Checkpoint saved..\n",
            "Epoch 4/100, Batch 1, Step 496: Loss = 4.473625\n",
            "Epoch 4/100, Batch 2, Step 497: Loss = 4.501884\n",
            "Epoch 4/100, Batch 3, Step 498: Loss = 4.483401\n",
            "Epoch 4/100, Batch 4, Step 499: Loss = 4.657011\n",
            "Epoch 4/100, Batch 5, Step 500: Loss = 4.520075\n",
            "Epoch 4/100, Batch 6, Step 501: Loss = 4.549177\n",
            "Epoch 4/100, Batch 7, Step 502: Loss = 4.445865\n",
            "Epoch 4/100, Batch 8, Step 503: Loss = 4.109180\n",
            "Epoch 4/100, Batch 9, Step 504: Loss = 4.648041\n",
            "Epoch 4/100, Batch 10, Step 505: Loss = 4.528978\n",
            "Epoch 4/100, Batch 11, Step 506: Loss = 4.354713\n",
            "Epoch 4/100, Batch 12, Step 507: Loss = 4.323394\n",
            "Epoch 4/100, Batch 13, Step 508: Loss = 4.271691\n",
            "Epoch 4/100, Batch 14, Step 509: Loss = 4.111565\n",
            "Epoch 4/100, Batch 15, Step 510: Loss = 4.480932\n",
            "Epoch 4/100, Batch 16, Step 511: Loss = 4.470270\n",
            "Epoch 4/100, Batch 17, Step 512: Loss = 4.595125\n",
            "Epoch 4/100, Batch 18, Step 513: Loss = 4.479677\n",
            "Epoch 4/100, Batch 19, Step 514: Loss = 4.490369\n",
            "Epoch 4/100, Batch 20, Step 515: Loss = 4.225047\n",
            "Epoch 4/100, Batch 21, Step 516: Loss = 4.583407\n",
            "Epoch 4/100, Batch 22, Step 517: Loss = 4.145820\n",
            "Epoch 4/100, Batch 23, Step 518: Loss = 4.471653\n",
            "Epoch 4/100, Batch 24, Step 519: Loss = 4.759382\n",
            "Epoch 4/100, Batch 25, Step 520: Loss = 4.189932\n",
            "Epoch 4/100, Batch 26, Step 521: Loss = 4.796647\n",
            "Epoch 4/100, Batch 27, Step 522: Loss = 4.192946\n",
            "Epoch 4/100, Batch 28, Step 523: Loss = 4.071696\n",
            "Epoch 4/100, Batch 29, Step 524: Loss = 4.909439\n",
            "Epoch 4/100, Batch 30, Step 525: Loss = 4.142227\n",
            "Epoch 4/100, Batch 31, Step 526: Loss = 4.119503\n",
            "Epoch 4/100, Batch 32, Step 527: Loss = 4.309651\n",
            "Epoch 4/100, Batch 33, Step 528: Loss = 4.174661\n",
            "Epoch 4/100, Batch 34, Step 529: Loss = 4.430709\n",
            "Epoch 4/100, Batch 35, Step 530: Loss = 4.336372\n",
            "Epoch 4/100, Batch 36, Step 531: Loss = 4.589664\n",
            "Epoch 4/100, Batch 37, Step 532: Loss = 4.205233\n",
            "Epoch 4/100, Batch 38, Step 533: Loss = 3.956841\n",
            "Epoch 4/100, Batch 39, Step 534: Loss = 4.680946\n",
            "Epoch 4/100, Batch 40, Step 535: Loss = 4.421869\n",
            "Epoch 4/100, Batch 41, Step 536: Loss = 4.416403\n",
            "Epoch 4/100, Batch 42, Step 537: Loss = 4.341029\n",
            "Epoch 4/100, Batch 43, Step 538: Loss = 4.639915\n",
            "Epoch 4/100, Batch 44, Step 539: Loss = 4.481519\n",
            "Epoch 4/100, Batch 45, Step 540: Loss = 4.665302\n",
            "Epoch 4/100, Batch 46, Step 541: Loss = 4.690170\n",
            "Epoch 4/100, Batch 47, Step 542: Loss = 4.298642\n",
            "Epoch 4/100, Batch 48, Step 543: Loss = 4.387856\n",
            "Epoch 4/100, Batch 49, Step 544: Loss = 4.352824\n",
            "Epoch 4/100, Batch 50, Step 545: Loss = 4.401577\n",
            "Epoch 4/100, Batch 51, Step 546: Loss = 4.353603\n",
            "Epoch 4/100, Batch 52, Step 547: Loss = 4.616357\n",
            "Epoch 4/100, Batch 53, Step 548: Loss = 4.030522\n",
            "Epoch 4/100, Batch 54, Step 549: Loss = 4.607438\n",
            "Epoch 4/100, Batch 55, Step 550: Loss = 4.177752\n",
            "Epoch 4/100, Batch 56, Step 551: Loss = 4.490292\n",
            "Epoch 4/100, Batch 57, Step 552: Loss = 4.443079\n",
            "Epoch 4/100, Batch 58, Step 553: Loss = 4.051185\n",
            "Epoch 4/100, Batch 59, Step 554: Loss = 4.170723\n",
            "Epoch 4/100, Batch 60, Step 555: Loss = 4.754993\n",
            "Epoch 4/100, Batch 61, Step 556: Loss = 4.401814\n",
            "Epoch 4/100, Batch 62, Step 557: Loss = 4.487574\n",
            "Epoch 4/100, Batch 63, Step 558: Loss = 4.422020\n",
            "Epoch 4/100, Batch 64, Step 559: Loss = 4.526269\n",
            "Epoch 4/100, Batch 65, Step 560: Loss = 4.366808\n",
            "Epoch 4/100, Batch 66, Step 561: Loss = 4.357609\n",
            "Epoch 4/100, Batch 67, Step 562: Loss = 4.414983\n",
            "Epoch 4/100, Batch 68, Step 563: Loss = 4.307492\n",
            "Epoch 4/100, Batch 69, Step 564: Loss = 4.424259\n",
            "Epoch 4/100, Batch 70, Step 565: Loss = 4.458239\n",
            "Epoch 4/100, Batch 71, Step 566: Loss = 4.275400\n",
            "Epoch 4/100, Batch 72, Step 567: Loss = 4.518822\n",
            "Epoch 4/100, Batch 73, Step 568: Loss = 4.661528\n",
            "Epoch 4/100, Batch 74, Step 569: Loss = 4.438442\n",
            "Epoch 4/100, Batch 75, Step 570: Loss = 4.355477\n",
            "Epoch 4/100, Batch 76, Step 571: Loss = 4.383805\n",
            "Epoch 4/100, Batch 77, Step 572: Loss = 4.550589\n",
            "Epoch 4/100, Batch 78, Step 573: Loss = 4.407814\n",
            "Epoch 4/100, Batch 79, Step 574: Loss = 4.564725\n",
            "Epoch 4/100, Batch 80, Step 575: Loss = 4.150389\n",
            "Epoch 4/100, Batch 81, Step 576: Loss = 4.058190\n",
            "Epoch 4/100, Batch 82, Step 577: Loss = 4.544467\n",
            "Epoch 4/100, Batch 83, Step 578: Loss = 4.246915\n",
            "Epoch 4/100, Batch 84, Step 579: Loss = 4.374773\n",
            "Epoch 4/100, Batch 85, Step 580: Loss = 4.583967\n",
            "Epoch 4/100, Batch 86, Step 581: Loss = 4.679283\n",
            "Epoch 4/100, Batch 87, Step 582: Loss = 4.243029\n",
            "Epoch 4/100, Batch 88, Step 583: Loss = 4.451279\n",
            "Epoch 4/100, Batch 89, Step 584: Loss = 4.381134\n",
            "Epoch 4/100, Batch 90, Step 585: Loss = 4.578239\n",
            "Epoch 4/100, Batch 91, Step 586: Loss = 4.381692\n",
            "Epoch 4/100, Batch 92, Step 587: Loss = 3.959906\n",
            "Epoch 4/100, Batch 93, Step 588: Loss = 4.134116\n",
            "Epoch 4/100, Batch 94, Step 589: Loss = 4.348321\n",
            "Epoch 4/100, Batch 95, Step 590: Loss = 4.561354\n",
            "Epoch 4/100, Batch 96, Step 591: Loss = 4.478396\n",
            "Epoch 4/100, Batch 97, Step 592: Loss = 4.589649\n",
            "Epoch 4/100, Batch 98, Step 593: Loss = 4.688097\n",
            "Epoch 4/100, Batch 99, Step 594: Loss = 4.223018\n",
            "Epoch 4/100, Batch 100, Step 595: Loss = 4.640981\n",
            "Epoch 4/100, Batch 101, Step 596: Loss = 4.307032\n",
            "Epoch 4/100, Batch 102, Step 597: Loss = 4.436210\n",
            "Epoch 4/100, Batch 103, Step 598: Loss = 4.483317\n",
            "Epoch 4/100, Batch 104, Step 599: Loss = 4.534301\n",
            "Epoch 4/100, Batch 105, Step 600: Loss = 4.211970\n",
            "Epoch 4/100, Batch 106, Step 601: Loss = 4.397493\n",
            "Epoch 4/100, Batch 107, Step 602: Loss = 4.362976\n",
            "Epoch 4/100, Batch 108, Step 603: Loss = 4.628426\n",
            "Epoch 4/100, Batch 109, Step 604: Loss = 4.399181\n",
            "Epoch 4/100, Batch 110, Step 605: Loss = 4.422215\n",
            "Epoch 4/100, Batch 111, Step 606: Loss = 3.894154\n",
            "Epoch 4/100, Batch 112, Step 607: Loss = 4.533880\n",
            "Epoch 4/100, Batch 113, Step 608: Loss = 4.481916\n",
            "Epoch 4/100, Batch 114, Step 609: Loss = 4.504360\n",
            "Epoch 4/100, Batch 115, Step 610: Loss = 4.033362\n",
            "Epoch 4/100, Batch 116, Step 611: Loss = 4.499392\n",
            "Epoch 4/100, Batch 117, Step 612: Loss = 4.116334\n",
            "Epoch 4/100, Batch 118, Step 613: Loss = 4.272060\n",
            "Epoch 4/100, Batch 119, Step 614: Loss = 4.335206\n",
            "Epoch 4/100, Batch 120, Step 615: Loss = 4.129463\n",
            "Epoch 4/100, Batch 121, Step 616: Loss = 4.224317\n",
            "Epoch 4/100, Batch 122, Step 617: Loss = 4.226038\n",
            "Epoch 4/100, Batch 123, Step 618: Loss = 4.259064\n",
            "Epoch 4/100, Batch 124, Step 619: Loss = 4.597395\n",
            "Epoch 4/100, Batch 125, Step 620: Loss = 4.059747\n",
            "Epoch 4/100, Batch 126, Step 621: Loss = 4.596559\n",
            "Epoch 4/100, Batch 127, Step 622: Loss = 4.162919\n",
            "Epoch 4/100, Batch 128, Step 623: Loss = 4.406778\n",
            "Epoch 4/100, Batch 129, Step 624: Loss = 4.611823\n",
            "Epoch 4/100, Batch 130, Step 625: Loss = 4.605639\n",
            "Epoch 4/100, Batch 131, Step 626: Loss = 4.248047\n",
            "Epoch 4/100, Batch 132, Step 627: Loss = 4.489092\n",
            "Epoch 4/100, Batch 133, Step 628: Loss = 4.265214\n",
            "Epoch 4/100, Batch 134, Step 629: Loss = 4.085079\n",
            "Epoch 4/100, Batch 135, Step 630: Loss = 4.423560\n",
            "Epoch 4/100, Batch 136, Step 631: Loss = 4.200176\n",
            "Epoch 4/100, Batch 137, Step 632: Loss = 4.699590\n",
            "Epoch 4/100, Batch 138, Step 633: Loss = 4.777986\n",
            "Epoch 4/100, Batch 139, Step 634: Loss = 4.361412\n",
            "Epoch 4/100, Batch 140, Step 635: Loss = 4.103334\n",
            "Epoch 4/100, Batch 141, Step 636: Loss = 4.457044\n",
            "Epoch 4/100, Batch 142, Step 637: Loss = 4.458529\n",
            "Epoch 4/100, Batch 143, Step 638: Loss = 4.180273\n",
            "Epoch 4/100, Batch 144, Step 639: Loss = 4.492220\n",
            "Epoch 4/100, Batch 145, Step 640: Loss = 4.323912\n",
            "Epoch 4/100, Batch 146, Step 641: Loss = 4.322973\n",
            "Epoch 4/100, Batch 147, Step 642: Loss = 4.237866\n",
            "Epoch 4/100, Batch 148, Step 643: Loss = 4.651455\n",
            "Epoch 4/100, Batch 149, Step 644: Loss = 4.218679\n",
            "Epoch 4/100, Batch 150, Step 645: Loss = 4.522136\n",
            "Epoch 4/100, Batch 151, Step 646: Loss = 4.462771\n",
            "Epoch 4/100, Batch 152, Step 647: Loss = 4.489720\n",
            "Epoch 4/100, Batch 153, Step 648: Loss = 4.329545\n",
            "Epoch 4/100, Batch 154, Step 649: Loss = 4.373096\n",
            "Epoch 4/100, Batch 155, Step 650: Loss = 4.474805\n",
            "Epoch 4/100, Batch 156, Step 651: Loss = 4.408720\n",
            "Epoch 4/100, Batch 157, Step 652: Loss = 4.357319\n",
            "Epoch 4/100, Batch 158, Step 653: Loss = 4.479661\n",
            "Epoch 4/100, Batch 159, Step 654: Loss = 4.400223\n",
            "Epoch 4/100, Batch 160, Step 655: Loss = 4.417842\n",
            "Epoch 4/100, Batch 161, Step 656: Loss = 4.936377\n",
            "Epoch 4/100, Batch 162, Step 657: Loss = 4.251914\n",
            "Epoch 4/100, Batch 163, Step 658: Loss = 4.789881\n",
            "Epoch 4/100, Batch 164, Step 659: Loss = 4.398337\n",
            "Epoch 4/100, Batch 165, Step 660: Loss = 4.250733\n",
            " Epoch 4/100, Avg Loss: 4.401659\n",
            "Checkpoint saved..\n",
            "Epoch 5/100, Batch 1, Step 661: Loss = 4.047207\n",
            "Epoch 5/100, Batch 2, Step 662: Loss = 4.267321\n",
            "Epoch 5/100, Batch 3, Step 663: Loss = 4.228164\n",
            "Epoch 5/100, Batch 4, Step 664: Loss = 4.461472\n",
            "Epoch 5/100, Batch 5, Step 665: Loss = 4.399340\n",
            "Epoch 5/100, Batch 6, Step 666: Loss = 4.383840\n",
            "Epoch 5/100, Batch 7, Step 667: Loss = 4.044446\n",
            "Epoch 5/100, Batch 8, Step 668: Loss = 4.393445\n",
            "Epoch 5/100, Batch 9, Step 669: Loss = 4.141758\n",
            "Epoch 5/100, Batch 10, Step 670: Loss = 3.863532\n",
            "Epoch 5/100, Batch 11, Step 671: Loss = 4.031912\n",
            "Epoch 5/100, Batch 12, Step 672: Loss = 4.192222\n",
            "Epoch 5/100, Batch 13, Step 673: Loss = 4.161752\n",
            "Epoch 5/100, Batch 14, Step 674: Loss = 4.292109\n",
            "Epoch 5/100, Batch 15, Step 675: Loss = 4.042627\n",
            "Epoch 5/100, Batch 16, Step 676: Loss = 3.907384\n",
            "Epoch 5/100, Batch 17, Step 677: Loss = 4.271936\n",
            "Epoch 5/100, Batch 18, Step 678: Loss = 3.868359\n",
            "Epoch 5/100, Batch 19, Step 679: Loss = 4.041228\n",
            "Epoch 5/100, Batch 20, Step 680: Loss = 4.556263\n",
            "Epoch 5/100, Batch 21, Step 681: Loss = 4.325298\n",
            "Epoch 5/100, Batch 22, Step 682: Loss = 4.089828\n",
            "Epoch 5/100, Batch 23, Step 683: Loss = 4.125511\n",
            "Epoch 5/100, Batch 24, Step 684: Loss = 4.271536\n",
            "Epoch 5/100, Batch 25, Step 685: Loss = 3.976937\n",
            "Epoch 5/100, Batch 26, Step 686: Loss = 4.209146\n",
            "Epoch 5/100, Batch 27, Step 687: Loss = 4.256560\n",
            "Epoch 5/100, Batch 28, Step 688: Loss = 4.106218\n",
            "Epoch 5/100, Batch 29, Step 689: Loss = 4.156099\n",
            "Epoch 5/100, Batch 30, Step 690: Loss = 3.921825\n",
            "Epoch 5/100, Batch 31, Step 691: Loss = 4.619422\n",
            "Epoch 5/100, Batch 32, Step 692: Loss = 4.106265\n",
            "Epoch 5/100, Batch 33, Step 693: Loss = 4.239153\n",
            "Epoch 5/100, Batch 34, Step 694: Loss = 4.257450\n",
            "Epoch 5/100, Batch 35, Step 695: Loss = 4.305375\n",
            "Epoch 5/100, Batch 36, Step 696: Loss = 4.256342\n",
            "Epoch 5/100, Batch 37, Step 697: Loss = 4.256337\n",
            "Epoch 5/100, Batch 38, Step 698: Loss = 4.642078\n",
            "Epoch 5/100, Batch 39, Step 699: Loss = 4.281925\n",
            "Epoch 5/100, Batch 40, Step 700: Loss = 3.931146\n",
            "Epoch 5/100, Batch 41, Step 701: Loss = 4.196700\n",
            "Epoch 5/100, Batch 42, Step 702: Loss = 4.192640\n",
            "Epoch 5/100, Batch 43, Step 703: Loss = 4.349857\n",
            "Epoch 5/100, Batch 44, Step 704: Loss = 3.922047\n",
            "Epoch 5/100, Batch 45, Step 705: Loss = 4.366344\n",
            "Epoch 5/100, Batch 46, Step 706: Loss = 4.133842\n",
            "Epoch 5/100, Batch 47, Step 707: Loss = 4.087252\n",
            "Epoch 5/100, Batch 48, Step 708: Loss = 4.240865\n",
            "Epoch 5/100, Batch 49, Step 709: Loss = 4.092947\n",
            "Epoch 5/100, Batch 50, Step 710: Loss = 4.035301\n",
            "Epoch 5/100, Batch 51, Step 711: Loss = 4.151294\n",
            "Epoch 5/100, Batch 52, Step 712: Loss = 4.393231\n",
            "Epoch 5/100, Batch 53, Step 713: Loss = 4.246963\n",
            "Epoch 5/100, Batch 54, Step 714: Loss = 4.366738\n",
            "Epoch 5/100, Batch 55, Step 715: Loss = 4.153081\n",
            "Epoch 5/100, Batch 56, Step 716: Loss = 4.397046\n",
            "Epoch 5/100, Batch 57, Step 717: Loss = 3.987139\n",
            "Epoch 5/100, Batch 58, Step 718: Loss = 4.215094\n",
            "Epoch 5/100, Batch 59, Step 719: Loss = 3.983073\n",
            "Epoch 5/100, Batch 60, Step 720: Loss = 4.147491\n",
            "Epoch 5/100, Batch 61, Step 721: Loss = 4.274292\n",
            "Epoch 5/100, Batch 62, Step 722: Loss = 4.179285\n",
            "Epoch 5/100, Batch 63, Step 723: Loss = 4.477890\n",
            "Epoch 5/100, Batch 64, Step 724: Loss = 4.285069\n",
            "Epoch 5/100, Batch 65, Step 725: Loss = 4.073757\n",
            "Epoch 5/100, Batch 66, Step 726: Loss = 4.219540\n",
            "Epoch 5/100, Batch 67, Step 727: Loss = 4.086659\n",
            "Epoch 5/100, Batch 68, Step 728: Loss = 4.423323\n",
            "Epoch 5/100, Batch 69, Step 729: Loss = 4.163151\n",
            "Epoch 5/100, Batch 70, Step 730: Loss = 4.372357\n",
            "Epoch 5/100, Batch 71, Step 731: Loss = 4.285476\n",
            "Epoch 5/100, Batch 72, Step 732: Loss = 3.859000\n",
            "Epoch 5/100, Batch 73, Step 733: Loss = 4.230584\n",
            "Epoch 5/100, Batch 74, Step 734: Loss = 3.831443\n",
            "Epoch 5/100, Batch 75, Step 735: Loss = 4.439735\n",
            "Epoch 5/100, Batch 76, Step 736: Loss = 3.975324\n",
            "Epoch 5/100, Batch 77, Step 737: Loss = 3.730641\n",
            "Epoch 5/100, Batch 78, Step 738: Loss = 4.099075\n",
            "Epoch 5/100, Batch 79, Step 739: Loss = 4.218654\n",
            "Epoch 5/100, Batch 80, Step 740: Loss = 4.328619\n",
            "Epoch 5/100, Batch 81, Step 741: Loss = 4.403794\n",
            "Epoch 5/100, Batch 82, Step 742: Loss = 4.253945\n",
            "Epoch 5/100, Batch 83, Step 743: Loss = 4.486962\n",
            "Epoch 5/100, Batch 84, Step 744: Loss = 3.914436\n",
            "Epoch 5/100, Batch 85, Step 745: Loss = 4.173125\n",
            "Epoch 5/100, Batch 86, Step 746: Loss = 3.895675\n",
            "Epoch 5/100, Batch 87, Step 747: Loss = 4.614659\n",
            "Epoch 5/100, Batch 88, Step 748: Loss = 3.948422\n",
            "Epoch 5/100, Batch 89, Step 749: Loss = 4.393536\n",
            "Epoch 5/100, Batch 90, Step 750: Loss = 3.898023\n",
            "Epoch 5/100, Batch 91, Step 751: Loss = 4.440449\n",
            "Epoch 5/100, Batch 92, Step 752: Loss = 4.160691\n",
            "Epoch 5/100, Batch 93, Step 753: Loss = 3.793572\n",
            "Epoch 5/100, Batch 94, Step 754: Loss = 4.208938\n",
            "Epoch 5/100, Batch 95, Step 755: Loss = 4.122703\n",
            "Epoch 5/100, Batch 96, Step 756: Loss = 4.032503\n",
            "Epoch 5/100, Batch 97, Step 757: Loss = 4.246261\n",
            "Epoch 5/100, Batch 98, Step 758: Loss = 4.279265\n",
            "Epoch 5/100, Batch 99, Step 759: Loss = 4.315378\n",
            "Epoch 5/100, Batch 100, Step 760: Loss = 4.164346\n",
            "Epoch 5/100, Batch 101, Step 761: Loss = 4.268850\n",
            "Epoch 5/100, Batch 102, Step 762: Loss = 4.049047\n",
            "Epoch 5/100, Batch 103, Step 763: Loss = 4.193857\n",
            "Epoch 5/100, Batch 104, Step 764: Loss = 4.439126\n",
            "Epoch 5/100, Batch 105, Step 765: Loss = 3.884875\n",
            "Epoch 5/100, Batch 106, Step 766: Loss = 4.330800\n",
            "Epoch 5/100, Batch 107, Step 767: Loss = 3.771692\n",
            "Epoch 5/100, Batch 108, Step 768: Loss = 4.381864\n",
            "Epoch 5/100, Batch 109, Step 769: Loss = 4.226181\n",
            "Epoch 5/100, Batch 110, Step 770: Loss = 4.194978\n",
            "Epoch 5/100, Batch 111, Step 771: Loss = 4.051057\n",
            "Epoch 5/100, Batch 112, Step 772: Loss = 4.120442\n",
            "Epoch 5/100, Batch 113, Step 773: Loss = 4.073312\n",
            "Epoch 5/100, Batch 114, Step 774: Loss = 4.151868\n",
            "Epoch 5/100, Batch 115, Step 775: Loss = 4.186491\n",
            "Epoch 5/100, Batch 116, Step 776: Loss = 4.250224\n",
            "Epoch 5/100, Batch 117, Step 777: Loss = 4.331347\n",
            "Epoch 5/100, Batch 118, Step 778: Loss = 3.991848\n",
            "Epoch 5/100, Batch 119, Step 779: Loss = 3.935399\n",
            "Epoch 5/100, Batch 120, Step 780: Loss = 4.041715\n",
            "Epoch 5/100, Batch 121, Step 781: Loss = 4.309230\n",
            "Epoch 5/100, Batch 122, Step 782: Loss = 4.151978\n",
            "Epoch 5/100, Batch 123, Step 783: Loss = 4.084342\n",
            "Epoch 5/100, Batch 124, Step 784: Loss = 4.454239\n",
            "Epoch 5/100, Batch 125, Step 785: Loss = 4.290091\n",
            "Epoch 5/100, Batch 126, Step 786: Loss = 4.105266\n",
            "Epoch 5/100, Batch 127, Step 787: Loss = 4.089276\n",
            "Epoch 5/100, Batch 128, Step 788: Loss = 4.357745\n",
            "Epoch 5/100, Batch 129, Step 789: Loss = 4.031325\n",
            "Epoch 5/100, Batch 130, Step 790: Loss = 4.401135\n",
            "Epoch 5/100, Batch 131, Step 791: Loss = 4.290844\n",
            "Epoch 5/100, Batch 132, Step 792: Loss = 4.333755\n",
            "Epoch 5/100, Batch 133, Step 793: Loss = 4.422297\n",
            "Epoch 5/100, Batch 134, Step 794: Loss = 4.044449\n",
            "Epoch 5/100, Batch 135, Step 795: Loss = 4.251760\n",
            "Epoch 5/100, Batch 136, Step 796: Loss = 4.153609\n",
            "Epoch 5/100, Batch 137, Step 797: Loss = 4.350047\n",
            "Epoch 5/100, Batch 138, Step 798: Loss = 4.309287\n",
            "Epoch 5/100, Batch 139, Step 799: Loss = 3.813123\n",
            "Epoch 5/100, Batch 140, Step 800: Loss = 4.409586\n",
            "Epoch 5/100, Batch 141, Step 801: Loss = 4.162765\n",
            "Epoch 5/100, Batch 142, Step 802: Loss = 4.016925\n",
            "Epoch 5/100, Batch 143, Step 803: Loss = 4.424218\n",
            "Epoch 5/100, Batch 144, Step 804: Loss = 4.411764\n",
            "Epoch 5/100, Batch 145, Step 805: Loss = 4.325606\n",
            "Epoch 5/100, Batch 146, Step 806: Loss = 4.345927\n",
            "Epoch 5/100, Batch 147, Step 807: Loss = 4.061462\n",
            "Epoch 5/100, Batch 148, Step 808: Loss = 3.858993\n",
            "Epoch 5/100, Batch 149, Step 809: Loss = 3.627510\n",
            "Epoch 5/100, Batch 150, Step 810: Loss = 4.237215\n",
            "Epoch 5/100, Batch 151, Step 811: Loss = 3.930423\n",
            "Epoch 5/100, Batch 152, Step 812: Loss = 4.153873\n",
            "Epoch 5/100, Batch 153, Step 813: Loss = 4.418422\n",
            "Epoch 5/100, Batch 154, Step 814: Loss = 4.533888\n",
            "Epoch 5/100, Batch 155, Step 815: Loss = 3.958264\n",
            "Epoch 5/100, Batch 156, Step 816: Loss = 4.149985\n",
            "Epoch 5/100, Batch 157, Step 817: Loss = 4.211467\n",
            "Epoch 5/100, Batch 158, Step 818: Loss = 4.078968\n",
            "Epoch 5/100, Batch 159, Step 819: Loss = 4.125275\n",
            "Epoch 5/100, Batch 160, Step 820: Loss = 4.231789\n",
            "Epoch 5/100, Batch 161, Step 821: Loss = 4.223724\n",
            "Epoch 5/100, Batch 162, Step 822: Loss = 4.405368\n",
            "Epoch 5/100, Batch 163, Step 823: Loss = 4.402278\n",
            "Epoch 5/100, Batch 164, Step 824: Loss = 3.970678\n",
            "Epoch 5/100, Batch 165, Step 825: Loss = 4.256136\n",
            " Epoch 5/100, Avg Loss: 4.187525\n",
            "Checkpoint saved..\n",
            "Epoch 6/100, Batch 1, Step 826: Loss = 4.046206\n",
            "Epoch 6/100, Batch 2, Step 827: Loss = 4.057527\n",
            "Epoch 6/100, Batch 3, Step 828: Loss = 4.248461\n",
            "Epoch 6/100, Batch 4, Step 829: Loss = 3.945572\n",
            "Epoch 6/100, Batch 5, Step 830: Loss = 3.988747\n",
            "Epoch 6/100, Batch 6, Step 831: Loss = 4.032566\n",
            "Epoch 6/100, Batch 7, Step 832: Loss = 3.855611\n",
            "Epoch 6/100, Batch 8, Step 833: Loss = 3.824136\n",
            "Epoch 6/100, Batch 9, Step 834: Loss = 4.239266\n",
            "Epoch 6/100, Batch 10, Step 835: Loss = 4.087885\n",
            "Epoch 6/100, Batch 11, Step 836: Loss = 3.828620\n",
            "Epoch 6/100, Batch 12, Step 837: Loss = 3.939276\n",
            "Epoch 6/100, Batch 13, Step 838: Loss = 3.900614\n",
            "Epoch 6/100, Batch 14, Step 839: Loss = 4.224978\n",
            "Epoch 6/100, Batch 15, Step 840: Loss = 3.883345\n",
            "Epoch 6/100, Batch 16, Step 841: Loss = 4.017801\n",
            "Epoch 6/100, Batch 17, Step 842: Loss = 4.128762\n",
            "Epoch 6/100, Batch 18, Step 843: Loss = 3.948520\n",
            "Epoch 6/100, Batch 19, Step 844: Loss = 3.910996\n",
            "Epoch 6/100, Batch 20, Step 845: Loss = 4.165577\n",
            "Epoch 6/100, Batch 21, Step 846: Loss = 3.907727\n",
            "Epoch 6/100, Batch 22, Step 847: Loss = 3.715692\n",
            "Epoch 6/100, Batch 23, Step 848: Loss = 4.080387\n",
            "Epoch 6/100, Batch 24, Step 849: Loss = 4.131221\n",
            "Epoch 6/100, Batch 25, Step 850: Loss = 3.969251\n",
            "Epoch 6/100, Batch 26, Step 851: Loss = 3.607363\n",
            "Epoch 6/100, Batch 27, Step 852: Loss = 4.076784\n",
            "Epoch 6/100, Batch 28, Step 853: Loss = 3.741441\n",
            "Epoch 6/100, Batch 29, Step 854: Loss = 3.902065\n",
            "Epoch 6/100, Batch 30, Step 855: Loss = 4.254152\n",
            "Epoch 6/100, Batch 31, Step 856: Loss = 4.072817\n",
            "Epoch 6/100, Batch 32, Step 857: Loss = 4.065336\n",
            "Epoch 6/100, Batch 33, Step 858: Loss = 4.201608\n",
            "Epoch 6/100, Batch 34, Step 859: Loss = 4.328358\n",
            "Epoch 6/100, Batch 35, Step 860: Loss = 3.745681\n",
            "Epoch 6/100, Batch 36, Step 861: Loss = 4.160134\n",
            "Epoch 6/100, Batch 37, Step 862: Loss = 4.099536\n",
            "Epoch 6/100, Batch 38, Step 863: Loss = 4.002729\n",
            "Epoch 6/100, Batch 39, Step 864: Loss = 4.059093\n",
            "Epoch 6/100, Batch 40, Step 865: Loss = 3.862544\n",
            "Epoch 6/100, Batch 41, Step 866: Loss = 4.227151\n",
            "Epoch 6/100, Batch 42, Step 867: Loss = 4.021385\n",
            "Epoch 6/100, Batch 43, Step 868: Loss = 4.028813\n",
            "Epoch 6/100, Batch 44, Step 869: Loss = 4.123124\n",
            "Epoch 6/100, Batch 45, Step 870: Loss = 3.888542\n",
            "Epoch 6/100, Batch 46, Step 871: Loss = 4.368103\n",
            "Epoch 6/100, Batch 47, Step 872: Loss = 4.174810\n",
            "Epoch 6/100, Batch 48, Step 873: Loss = 4.040753\n",
            "Epoch 6/100, Batch 49, Step 874: Loss = 3.907855\n",
            "Epoch 6/100, Batch 50, Step 875: Loss = 3.963223\n",
            "Epoch 6/100, Batch 51, Step 876: Loss = 3.864929\n",
            "Epoch 6/100, Batch 52, Step 877: Loss = 4.145199\n",
            "Epoch 6/100, Batch 53, Step 878: Loss = 3.918994\n",
            "Epoch 6/100, Batch 54, Step 879: Loss = 3.900693\n",
            "Epoch 6/100, Batch 55, Step 880: Loss = 3.984450\n",
            "Epoch 6/100, Batch 56, Step 881: Loss = 4.242440\n",
            "Epoch 6/100, Batch 57, Step 882: Loss = 3.872601\n",
            "Epoch 6/100, Batch 58, Step 883: Loss = 3.474119\n",
            "Epoch 6/100, Batch 59, Step 884: Loss = 3.912666\n",
            "Epoch 6/100, Batch 60, Step 885: Loss = 3.934783\n",
            "Epoch 6/100, Batch 61, Step 886: Loss = 3.582244\n",
            "Epoch 6/100, Batch 62, Step 887: Loss = 3.840806\n",
            "Epoch 6/100, Batch 63, Step 888: Loss = 3.795651\n",
            "Epoch 6/100, Batch 64, Step 889: Loss = 3.954994\n",
            "Epoch 6/100, Batch 65, Step 890: Loss = 3.779743\n",
            "Epoch 6/100, Batch 66, Step 891: Loss = 4.406677\n",
            "Epoch 6/100, Batch 67, Step 892: Loss = 4.034991\n",
            "Epoch 6/100, Batch 68, Step 893: Loss = 4.024987\n",
            "Epoch 6/100, Batch 69, Step 894: Loss = 3.898892\n",
            "Epoch 6/100, Batch 70, Step 895: Loss = 3.847600\n",
            "Epoch 6/100, Batch 71, Step 896: Loss = 4.176322\n",
            "Epoch 6/100, Batch 72, Step 897: Loss = 3.870507\n",
            "Epoch 6/100, Batch 73, Step 898: Loss = 3.811904\n",
            "Epoch 6/100, Batch 74, Step 899: Loss = 4.023958\n",
            "Epoch 6/100, Batch 75, Step 900: Loss = 4.381541\n",
            "Epoch 6/100, Batch 76, Step 901: Loss = 4.058095\n",
            "Epoch 6/100, Batch 77, Step 902: Loss = 3.981796\n",
            "Epoch 6/100, Batch 78, Step 903: Loss = 4.137875\n",
            "Epoch 6/100, Batch 79, Step 904: Loss = 3.786753\n",
            "Epoch 6/100, Batch 80, Step 905: Loss = 3.988805\n",
            "Epoch 6/100, Batch 81, Step 906: Loss = 3.672769\n",
            "Epoch 6/100, Batch 82, Step 907: Loss = 3.832738\n",
            "Epoch 6/100, Batch 83, Step 908: Loss = 3.891272\n",
            "Epoch 6/100, Batch 84, Step 909: Loss = 4.103983\n",
            "Epoch 6/100, Batch 85, Step 910: Loss = 3.992184\n",
            "Epoch 6/100, Batch 86, Step 911: Loss = 4.253285\n",
            "Epoch 6/100, Batch 87, Step 912: Loss = 3.992006\n",
            "Epoch 6/100, Batch 88, Step 913: Loss = 4.140308\n",
            "Epoch 6/100, Batch 89, Step 914: Loss = 4.033978\n",
            "Epoch 6/100, Batch 90, Step 915: Loss = 4.332728\n",
            "Epoch 6/100, Batch 91, Step 916: Loss = 4.068451\n",
            "Epoch 6/100, Batch 92, Step 917: Loss = 3.542419\n",
            "Epoch 6/100, Batch 93, Step 918: Loss = 4.009858\n",
            "Epoch 6/100, Batch 94, Step 919: Loss = 3.848719\n",
            "Epoch 6/100, Batch 95, Step 920: Loss = 3.935866\n",
            "Epoch 6/100, Batch 96, Step 921: Loss = 3.941401\n",
            "Epoch 6/100, Batch 97, Step 922: Loss = 3.976665\n",
            "Epoch 6/100, Batch 98, Step 923: Loss = 3.744800\n",
            "Epoch 6/100, Batch 99, Step 924: Loss = 3.892851\n",
            "Epoch 6/100, Batch 100, Step 925: Loss = 3.961612\n",
            "Epoch 6/100, Batch 101, Step 926: Loss = 4.032634\n",
            "Epoch 6/100, Batch 102, Step 927: Loss = 4.145072\n",
            "Epoch 6/100, Batch 103, Step 928: Loss = 4.182632\n",
            "Epoch 6/100, Batch 104, Step 929: Loss = 4.068559\n",
            "Epoch 6/100, Batch 105, Step 930: Loss = 4.023105\n",
            "Epoch 6/100, Batch 106, Step 931: Loss = 4.098507\n",
            "Epoch 6/100, Batch 107, Step 932: Loss = 3.831652\n",
            "Epoch 6/100, Batch 108, Step 933: Loss = 3.897887\n",
            "Epoch 6/100, Batch 109, Step 934: Loss = 3.553282\n",
            "Epoch 6/100, Batch 110, Step 935: Loss = 4.021390\n",
            "Epoch 6/100, Batch 111, Step 936: Loss = 4.083250\n",
            "Epoch 6/100, Batch 112, Step 937: Loss = 3.980233\n",
            "Epoch 6/100, Batch 113, Step 938: Loss = 3.818084\n",
            "Epoch 6/100, Batch 114, Step 939: Loss = 4.184426\n",
            "Epoch 6/100, Batch 115, Step 940: Loss = 4.059129\n",
            "Epoch 6/100, Batch 116, Step 941: Loss = 3.997975\n",
            "Epoch 6/100, Batch 117, Step 942: Loss = 4.045815\n",
            "Epoch 6/100, Batch 118, Step 943: Loss = 3.775306\n",
            "Epoch 6/100, Batch 119, Step 944: Loss = 3.901483\n",
            "Epoch 6/100, Batch 120, Step 945: Loss = 4.222633\n",
            "Epoch 6/100, Batch 121, Step 946: Loss = 4.190678\n",
            "Epoch 6/100, Batch 122, Step 947: Loss = 4.056237\n",
            "Epoch 6/100, Batch 123, Step 948: Loss = 3.872313\n",
            "Epoch 6/100, Batch 124, Step 949: Loss = 4.184293\n",
            "Epoch 6/100, Batch 125, Step 950: Loss = 4.026719\n",
            "Epoch 6/100, Batch 126, Step 951: Loss = 3.926370\n",
            "Epoch 6/100, Batch 127, Step 952: Loss = 4.022311\n",
            "Epoch 6/100, Batch 128, Step 953: Loss = 4.190660\n",
            "Epoch 6/100, Batch 129, Step 954: Loss = 3.783077\n",
            "Epoch 6/100, Batch 130, Step 955: Loss = 3.844643\n",
            "Epoch 6/100, Batch 131, Step 956: Loss = 4.295406\n",
            "Epoch 6/100, Batch 132, Step 957: Loss = 4.192954\n",
            "Epoch 6/100, Batch 133, Step 958: Loss = 3.914889\n",
            "Epoch 6/100, Batch 134, Step 959: Loss = 3.837386\n",
            "Epoch 6/100, Batch 135, Step 960: Loss = 4.198668\n",
            "Epoch 6/100, Batch 136, Step 961: Loss = 3.843529\n",
            "Epoch 6/100, Batch 137, Step 962: Loss = 3.784207\n",
            "Epoch 6/100, Batch 138, Step 963: Loss = 3.677649\n",
            "Epoch 6/100, Batch 139, Step 964: Loss = 3.942930\n",
            "Epoch 6/100, Batch 140, Step 965: Loss = 3.992706\n",
            "Epoch 6/100, Batch 141, Step 966: Loss = 4.333158\n",
            "Epoch 6/100, Batch 142, Step 967: Loss = 3.717191\n",
            "Epoch 6/100, Batch 143, Step 968: Loss = 4.055539\n",
            "Epoch 6/100, Batch 144, Step 969: Loss = 4.029182\n",
            "Epoch 6/100, Batch 145, Step 970: Loss = 4.211397\n",
            "Epoch 6/100, Batch 146, Step 971: Loss = 4.114340\n",
            "Epoch 6/100, Batch 147, Step 972: Loss = 3.890599\n",
            "Epoch 6/100, Batch 148, Step 973: Loss = 3.724897\n",
            "Epoch 6/100, Batch 149, Step 974: Loss = 3.919802\n",
            "Epoch 6/100, Batch 150, Step 975: Loss = 3.919023\n",
            "Epoch 6/100, Batch 151, Step 976: Loss = 3.941951\n",
            "Epoch 6/100, Batch 152, Step 977: Loss = 4.368666\n",
            "Epoch 6/100, Batch 153, Step 978: Loss = 4.091069\n",
            "Epoch 6/100, Batch 154, Step 979: Loss = 4.128293\n",
            "Epoch 6/100, Batch 155, Step 980: Loss = 3.770572\n",
            "Epoch 6/100, Batch 156, Step 981: Loss = 4.002715\n",
            "Epoch 6/100, Batch 157, Step 982: Loss = 3.866661\n",
            "Epoch 6/100, Batch 158, Step 983: Loss = 3.922563\n",
            "Epoch 6/100, Batch 159, Step 984: Loss = 4.103845\n",
            "Epoch 6/100, Batch 160, Step 985: Loss = 4.313352\n",
            "Epoch 6/100, Batch 161, Step 986: Loss = 4.059176\n",
            "Epoch 6/100, Batch 162, Step 987: Loss = 4.477854\n",
            "Epoch 6/100, Batch 163, Step 988: Loss = 4.109132\n",
            "Epoch 6/100, Batch 164, Step 989: Loss = 3.876599\n",
            "Epoch 6/100, Batch 165, Step 990: Loss = 4.285098\n",
            " Epoch 6/100, Avg Loss: 3.998415\n",
            "Checkpoint saved..\n",
            "Epoch 7/100, Batch 1, Step 991: Loss = 3.689415\n",
            "Epoch 7/100, Batch 2, Step 992: Loss = 3.874175\n",
            "Epoch 7/100, Batch 3, Step 993: Loss = 3.861426\n",
            "Epoch 7/100, Batch 4, Step 994: Loss = 3.550693\n",
            "Epoch 7/100, Batch 5, Step 995: Loss = 3.664123\n",
            "Epoch 7/100, Batch 6, Step 996: Loss = 3.896984\n",
            "Epoch 7/100, Batch 7, Step 997: Loss = 3.977407\n",
            "Epoch 7/100, Batch 8, Step 998: Loss = 3.840353\n",
            "Epoch 7/100, Batch 9, Step 999: Loss = 3.586483\n",
            "Epoch 7/100, Batch 10, Step 1000: Loss = 3.537310\n",
            "Epoch 7/100, Batch 11, Step 1001: Loss = 3.937786\n",
            "Epoch 7/100, Batch 12, Step 1002: Loss = 3.670706\n",
            "Epoch 7/100, Batch 13, Step 1003: Loss = 3.784472\n",
            "Epoch 7/100, Batch 14, Step 1004: Loss = 3.824854\n",
            "Epoch 7/100, Batch 15, Step 1005: Loss = 3.735324\n",
            "Epoch 7/100, Batch 16, Step 1006: Loss = 3.615786\n",
            "Epoch 7/100, Batch 17, Step 1007: Loss = 3.570475\n",
            "Epoch 7/100, Batch 18, Step 1008: Loss = 3.727185\n",
            "Epoch 7/100, Batch 19, Step 1009: Loss = 3.545547\n",
            "Epoch 7/100, Batch 20, Step 1010: Loss = 3.721520\n",
            "Epoch 7/100, Batch 21, Step 1011: Loss = 3.785753\n",
            "Epoch 7/100, Batch 22, Step 1012: Loss = 3.733909\n",
            "Epoch 7/100, Batch 23, Step 1013: Loss = 3.683313\n",
            "Epoch 7/100, Batch 24, Step 1014: Loss = 3.958087\n",
            "Epoch 7/100, Batch 25, Step 1015: Loss = 3.509530\n",
            "Epoch 7/100, Batch 26, Step 1016: Loss = 3.896817\n",
            "Epoch 7/100, Batch 27, Step 1017: Loss = 3.909024\n",
            "Epoch 7/100, Batch 28, Step 1018: Loss = 4.166307\n",
            "Epoch 7/100, Batch 29, Step 1019: Loss = 3.531511\n",
            "Epoch 7/100, Batch 30, Step 1020: Loss = 3.767554\n",
            "Epoch 7/100, Batch 31, Step 1021: Loss = 3.689786\n",
            "Epoch 7/100, Batch 32, Step 1022: Loss = 3.957543\n",
            "Epoch 7/100, Batch 33, Step 1023: Loss = 3.723794\n",
            "Epoch 7/100, Batch 34, Step 1024: Loss = 3.789864\n",
            "Epoch 7/100, Batch 35, Step 1025: Loss = 4.124465\n",
            "Epoch 7/100, Batch 36, Step 1026: Loss = 4.162201\n",
            "Epoch 7/100, Batch 37, Step 1027: Loss = 3.671678\n",
            "Epoch 7/100, Batch 38, Step 1028: Loss = 3.930953\n",
            "Epoch 7/100, Batch 39, Step 1029: Loss = 3.682053\n",
            "Epoch 7/100, Batch 40, Step 1030: Loss = 3.851981\n",
            "Epoch 7/100, Batch 41, Step 1031: Loss = 3.867820\n",
            "Epoch 7/100, Batch 42, Step 1032: Loss = 3.866769\n",
            "Epoch 7/100, Batch 43, Step 1033: Loss = 3.935356\n",
            "Epoch 7/100, Batch 44, Step 1034: Loss = 3.866288\n",
            "Epoch 7/100, Batch 45, Step 1035: Loss = 3.907684\n",
            "Epoch 7/100, Batch 46, Step 1036: Loss = 4.031868\n",
            "Epoch 7/100, Batch 47, Step 1037: Loss = 3.860503\n",
            "Epoch 7/100, Batch 48, Step 1038: Loss = 3.461755\n",
            "Epoch 7/100, Batch 49, Step 1039: Loss = 3.828663\n",
            "Epoch 7/100, Batch 50, Step 1040: Loss = 3.956181\n",
            "Epoch 7/100, Batch 51, Step 1041: Loss = 3.662934\n",
            "Epoch 7/100, Batch 52, Step 1042: Loss = 3.910732\n",
            "Epoch 7/100, Batch 53, Step 1043: Loss = 3.825165\n",
            "Epoch 7/100, Batch 54, Step 1044: Loss = 3.907606\n",
            "Epoch 7/100, Batch 55, Step 1045: Loss = 3.612609\n",
            "Epoch 7/100, Batch 56, Step 1046: Loss = 3.725394\n",
            "Epoch 7/100, Batch 57, Step 1047: Loss = 3.690016\n",
            "Epoch 7/100, Batch 58, Step 1048: Loss = 3.657191\n",
            "Epoch 7/100, Batch 59, Step 1049: Loss = 3.826641\n",
            "Epoch 7/100, Batch 60, Step 1050: Loss = 3.704655\n",
            "Epoch 7/100, Batch 61, Step 1051: Loss = 3.895905\n",
            "Epoch 7/100, Batch 62, Step 1052: Loss = 3.821951\n",
            "Epoch 7/100, Batch 63, Step 1053: Loss = 3.867881\n",
            "Epoch 7/100, Batch 64, Step 1054: Loss = 3.787393\n",
            "Epoch 7/100, Batch 65, Step 1055: Loss = 3.922121\n",
            "Epoch 7/100, Batch 66, Step 1056: Loss = 4.244827\n",
            "Epoch 7/100, Batch 67, Step 1057: Loss = 3.920550\n",
            "Epoch 7/100, Batch 68, Step 1058: Loss = 3.840734\n",
            "Epoch 7/100, Batch 69, Step 1059: Loss = 3.740614\n",
            "Epoch 7/100, Batch 70, Step 1060: Loss = 3.993706\n",
            "Epoch 7/100, Batch 71, Step 1061: Loss = 3.787250\n",
            "Epoch 7/100, Batch 72, Step 1062: Loss = 3.967937\n",
            "Epoch 7/100, Batch 73, Step 1063: Loss = 3.534556\n",
            "Epoch 7/100, Batch 74, Step 1064: Loss = 3.782684\n",
            "Epoch 7/100, Batch 75, Step 1065: Loss = 3.849630\n",
            "Epoch 7/100, Batch 76, Step 1066: Loss = 3.571512\n",
            "Epoch 7/100, Batch 77, Step 1067: Loss = 4.104664\n",
            "Epoch 7/100, Batch 78, Step 1068: Loss = 3.818130\n",
            "Epoch 7/100, Batch 79, Step 1069: Loss = 3.905633\n",
            "Epoch 7/100, Batch 80, Step 1070: Loss = 3.756704\n",
            "Epoch 7/100, Batch 81, Step 1071: Loss = 4.151078\n",
            "Epoch 7/100, Batch 82, Step 1072: Loss = 3.879667\n",
            "Epoch 7/100, Batch 83, Step 1073: Loss = 4.103682\n",
            "Epoch 7/100, Batch 84, Step 1074: Loss = 3.663345\n",
            "Epoch 7/100, Batch 85, Step 1075: Loss = 3.835982\n",
            "Epoch 7/100, Batch 86, Step 1076: Loss = 3.601556\n",
            "Epoch 7/100, Batch 87, Step 1077: Loss = 3.928252\n",
            "Epoch 7/100, Batch 88, Step 1078: Loss = 3.873769\n",
            "Epoch 7/100, Batch 89, Step 1079: Loss = 3.702200\n",
            "Epoch 7/100, Batch 90, Step 1080: Loss = 3.812935\n",
            "Epoch 7/100, Batch 91, Step 1081: Loss = 3.921539\n",
            "Epoch 7/100, Batch 92, Step 1082: Loss = 3.740021\n",
            "Epoch 7/100, Batch 93, Step 1083: Loss = 4.122104\n",
            "Epoch 7/100, Batch 94, Step 1084: Loss = 3.703922\n",
            "Epoch 7/100, Batch 95, Step 1085: Loss = 3.644072\n",
            "Epoch 7/100, Batch 96, Step 1086: Loss = 4.296336\n",
            "Epoch 7/100, Batch 97, Step 1087: Loss = 3.780484\n",
            "Epoch 7/100, Batch 98, Step 1088: Loss = 3.480777\n",
            "Epoch 7/100, Batch 99, Step 1089: Loss = 3.824304\n",
            "Epoch 7/100, Batch 100, Step 1090: Loss = 3.911086\n",
            "Epoch 7/100, Batch 101, Step 1091: Loss = 3.808322\n",
            "Epoch 7/100, Batch 102, Step 1092: Loss = 4.039387\n",
            "Epoch 7/100, Batch 103, Step 1093: Loss = 3.966976\n",
            "Epoch 7/100, Batch 104, Step 1094: Loss = 3.918846\n",
            "Epoch 7/100, Batch 105, Step 1095: Loss = 3.972955\n",
            "Epoch 7/100, Batch 106, Step 1096: Loss = 4.242586\n",
            "Epoch 7/100, Batch 107, Step 1097: Loss = 3.875908\n",
            "Epoch 7/100, Batch 108, Step 1098: Loss = 3.506217\n",
            "Epoch 7/100, Batch 109, Step 1099: Loss = 3.809492\n",
            "Epoch 7/100, Batch 110, Step 1100: Loss = 3.998124\n",
            "Epoch 7/100, Batch 111, Step 1101: Loss = 3.861856\n",
            "Epoch 7/100, Batch 112, Step 1102: Loss = 4.203960\n",
            "Epoch 7/100, Batch 113, Step 1103: Loss = 3.947050\n",
            "Epoch 7/100, Batch 114, Step 1104: Loss = 3.959211\n",
            "Epoch 7/100, Batch 115, Step 1105: Loss = 3.925564\n",
            "Epoch 7/100, Batch 116, Step 1106: Loss = 3.747707\n",
            "Epoch 7/100, Batch 117, Step 1107: Loss = 3.578346\n",
            "Epoch 7/100, Batch 118, Step 1108: Loss = 3.929234\n",
            "Epoch 7/100, Batch 119, Step 1109: Loss = 3.629099\n",
            "Epoch 7/100, Batch 120, Step 1110: Loss = 4.111293\n",
            "Epoch 7/100, Batch 121, Step 1111: Loss = 3.650231\n",
            "Epoch 7/100, Batch 122, Step 1112: Loss = 3.752104\n",
            "Epoch 7/100, Batch 123, Step 1113: Loss = 3.793360\n",
            "Epoch 7/100, Batch 124, Step 1114: Loss = 3.495394\n",
            "Epoch 7/100, Batch 125, Step 1115: Loss = 3.734806\n",
            "Epoch 7/100, Batch 126, Step 1116: Loss = 3.871137\n",
            "Epoch 7/100, Batch 127, Step 1117: Loss = 3.687829\n",
            "Epoch 7/100, Batch 128, Step 1118: Loss = 3.864623\n",
            "Epoch 7/100, Batch 129, Step 1119: Loss = 3.821447\n",
            "Epoch 7/100, Batch 130, Step 1120: Loss = 3.842291\n",
            "Epoch 7/100, Batch 131, Step 1121: Loss = 4.226586\n",
            "Epoch 7/100, Batch 132, Step 1122: Loss = 3.834298\n",
            "Epoch 7/100, Batch 133, Step 1123: Loss = 4.035361\n",
            "Epoch 7/100, Batch 134, Step 1124: Loss = 3.491706\n",
            "Epoch 7/100, Batch 135, Step 1125: Loss = 3.867650\n",
            "Epoch 7/100, Batch 136, Step 1126: Loss = 3.800341\n",
            "Epoch 7/100, Batch 137, Step 1127: Loss = 3.893328\n",
            "Epoch 7/100, Batch 138, Step 1128: Loss = 3.670373\n",
            "Epoch 7/100, Batch 139, Step 1129: Loss = 3.663761\n",
            "Epoch 7/100, Batch 140, Step 1130: Loss = 3.574217\n",
            "Epoch 7/100, Batch 141, Step 1131: Loss = 3.808982\n",
            "Epoch 7/100, Batch 142, Step 1132: Loss = 3.866063\n",
            "Epoch 7/100, Batch 143, Step 1133: Loss = 4.128358\n",
            "Epoch 7/100, Batch 144, Step 1134: Loss = 3.449415\n",
            "Epoch 7/100, Batch 145, Step 1135: Loss = 4.178648\n",
            "Epoch 7/100, Batch 146, Step 1136: Loss = 3.786245\n",
            "Epoch 7/100, Batch 147, Step 1137: Loss = 3.851045\n",
            "Epoch 7/100, Batch 148, Step 1138: Loss = 3.929847\n",
            "Epoch 7/100, Batch 149, Step 1139: Loss = 4.046998\n",
            "Epoch 7/100, Batch 150, Step 1140: Loss = 3.667259\n",
            "Epoch 7/100, Batch 151, Step 1141: Loss = 3.691001\n",
            "Epoch 7/100, Batch 152, Step 1142: Loss = 3.764433\n",
            "Epoch 7/100, Batch 153, Step 1143: Loss = 3.876914\n",
            "Epoch 7/100, Batch 154, Step 1144: Loss = 3.745347\n",
            "Epoch 7/100, Batch 155, Step 1145: Loss = 4.107893\n",
            "Epoch 7/100, Batch 156, Step 1146: Loss = 3.610768\n",
            "Epoch 7/100, Batch 157, Step 1147: Loss = 3.944870\n",
            "Epoch 7/100, Batch 158, Step 1148: Loss = 3.917688\n",
            "Epoch 7/100, Batch 159, Step 1149: Loss = 3.516437\n",
            "Epoch 7/100, Batch 160, Step 1150: Loss = 3.962812\n",
            "Epoch 7/100, Batch 161, Step 1151: Loss = 3.855353\n",
            "Epoch 7/100, Batch 162, Step 1152: Loss = 3.371104\n",
            "Epoch 7/100, Batch 163, Step 1153: Loss = 3.767995\n",
            "Epoch 7/100, Batch 164, Step 1154: Loss = 3.782616\n",
            "Epoch 7/100, Batch 165, Step 1155: Loss = 3.665765\n",
            " Epoch 7/100, Avg Loss: 3.819590\n",
            "Checkpoint saved..\n",
            "Epoch 8/100, Batch 1, Step 1156: Loss = 3.504325\n",
            "Epoch 8/100, Batch 2, Step 1157: Loss = 3.443671\n",
            "Epoch 8/100, Batch 3, Step 1158: Loss = 3.558223\n",
            "Epoch 8/100, Batch 4, Step 1159: Loss = 3.715880\n",
            "Epoch 8/100, Batch 5, Step 1160: Loss = 3.622619\n",
            "Epoch 8/100, Batch 6, Step 1161: Loss = 3.395954\n",
            "Epoch 8/100, Batch 7, Step 1162: Loss = 3.596024\n",
            "Epoch 8/100, Batch 8, Step 1163: Loss = 3.973386\n",
            "Epoch 8/100, Batch 9, Step 1164: Loss = 3.598004\n",
            "Epoch 8/100, Batch 10, Step 1165: Loss = 3.605118\n",
            "Epoch 8/100, Batch 11, Step 1166: Loss = 3.358713\n",
            "Epoch 8/100, Batch 12, Step 1167: Loss = 3.503359\n",
            "Epoch 8/100, Batch 13, Step 1168: Loss = 3.550175\n",
            "Epoch 8/100, Batch 14, Step 1169: Loss = 3.478880\n",
            "Epoch 8/100, Batch 15, Step 1170: Loss = 3.824062\n",
            "Epoch 8/100, Batch 16, Step 1171: Loss = 3.451339\n",
            "Epoch 8/100, Batch 17, Step 1172: Loss = 3.517612\n",
            "Epoch 8/100, Batch 18, Step 1173: Loss = 3.602077\n",
            "Epoch 8/100, Batch 19, Step 1174: Loss = 3.724098\n",
            "Epoch 8/100, Batch 20, Step 1175: Loss = 3.766579\n",
            "Epoch 8/100, Batch 21, Step 1176: Loss = 3.661794\n",
            "Epoch 8/100, Batch 22, Step 1177: Loss = 3.613726\n",
            "Epoch 8/100, Batch 23, Step 1178: Loss = 3.521808\n",
            "Epoch 8/100, Batch 24, Step 1179: Loss = 3.555313\n",
            "Epoch 8/100, Batch 25, Step 1180: Loss = 3.406267\n",
            "Epoch 8/100, Batch 26, Step 1181: Loss = 3.542785\n",
            "Epoch 8/100, Batch 27, Step 1182: Loss = 3.756671\n",
            "Epoch 8/100, Batch 28, Step 1183: Loss = 3.741360\n",
            "Epoch 8/100, Batch 29, Step 1184: Loss = 3.682069\n",
            "Epoch 8/100, Batch 30, Step 1185: Loss = 3.533688\n",
            "Epoch 8/100, Batch 31, Step 1186: Loss = 3.397098\n",
            "Epoch 8/100, Batch 32, Step 1187: Loss = 3.697446\n",
            "Epoch 8/100, Batch 33, Step 1188: Loss = 3.852120\n",
            "Epoch 8/100, Batch 34, Step 1189: Loss = 3.782360\n",
            "Epoch 8/100, Batch 35, Step 1190: Loss = 3.587886\n",
            "Epoch 8/100, Batch 36, Step 1191: Loss = 3.691443\n",
            "Epoch 8/100, Batch 37, Step 1192: Loss = 3.431707\n",
            "Epoch 8/100, Batch 38, Step 1193: Loss = 3.483581\n",
            "Epoch 8/100, Batch 39, Step 1194: Loss = 3.811107\n",
            "Epoch 8/100, Batch 40, Step 1195: Loss = 3.611989\n",
            "Epoch 8/100, Batch 41, Step 1196: Loss = 3.736086\n",
            "Epoch 8/100, Batch 42, Step 1197: Loss = 3.665376\n",
            "Epoch 8/100, Batch 43, Step 1198: Loss = 3.627505\n",
            "Epoch 8/100, Batch 44, Step 1199: Loss = 3.680382\n",
            "Epoch 8/100, Batch 45, Step 1200: Loss = 3.375141\n",
            "Epoch 8/100, Batch 46, Step 1201: Loss = 3.540628\n",
            "Epoch 8/100, Batch 47, Step 1202: Loss = 3.586098\n",
            "Epoch 8/100, Batch 48, Step 1203: Loss = 3.443412\n",
            "Epoch 8/100, Batch 49, Step 1204: Loss = 3.726294\n",
            "Epoch 8/100, Batch 50, Step 1205: Loss = 3.479646\n",
            "Epoch 8/100, Batch 51, Step 1206: Loss = 3.232154\n",
            "Epoch 8/100, Batch 52, Step 1207: Loss = 3.692463\n",
            "Epoch 8/100, Batch 53, Step 1208: Loss = 3.647693\n",
            "Epoch 8/100, Batch 54, Step 1209: Loss = 3.742020\n",
            "Epoch 8/100, Batch 55, Step 1210: Loss = 3.649832\n",
            "Epoch 8/100, Batch 56, Step 1211: Loss = 3.887957\n",
            "Epoch 8/100, Batch 57, Step 1212: Loss = 3.466366\n",
            "Epoch 8/100, Batch 58, Step 1213: Loss = 3.832783\n",
            "Epoch 8/100, Batch 59, Step 1214: Loss = 3.863201\n",
            "Epoch 8/100, Batch 60, Step 1215: Loss = 3.745399\n",
            "Epoch 8/100, Batch 61, Step 1216: Loss = 3.717315\n",
            "Epoch 8/100, Batch 62, Step 1217: Loss = 3.583762\n",
            "Epoch 8/100, Batch 63, Step 1218: Loss = 3.621161\n",
            "Epoch 8/100, Batch 64, Step 1219: Loss = 3.691647\n",
            "Epoch 8/100, Batch 65, Step 1220: Loss = 3.456207\n",
            "Epoch 8/100, Batch 66, Step 1221: Loss = 3.488762\n",
            "Epoch 8/100, Batch 67, Step 1222: Loss = 3.715473\n",
            "Epoch 8/100, Batch 68, Step 1223: Loss = 3.692082\n",
            "Epoch 8/100, Batch 69, Step 1224: Loss = 3.827635\n",
            "Epoch 8/100, Batch 70, Step 1225: Loss = 3.602490\n",
            "Epoch 8/100, Batch 71, Step 1226: Loss = 3.719865\n",
            "Epoch 8/100, Batch 72, Step 1227: Loss = 3.608479\n",
            "Epoch 8/100, Batch 73, Step 1228: Loss = 3.833684\n",
            "Epoch 8/100, Batch 74, Step 1229: Loss = 3.487353\n",
            "Epoch 8/100, Batch 75, Step 1230: Loss = 3.675497\n",
            "Epoch 8/100, Batch 76, Step 1231: Loss = 3.605446\n",
            "Epoch 8/100, Batch 77, Step 1232: Loss = 3.662799\n",
            "Epoch 8/100, Batch 78, Step 1233: Loss = 3.701742\n",
            "Epoch 8/100, Batch 79, Step 1234: Loss = 3.737571\n",
            "Epoch 8/100, Batch 80, Step 1235: Loss = 3.571122\n",
            "Epoch 8/100, Batch 81, Step 1236: Loss = 3.655866\n",
            "Epoch 8/100, Batch 82, Step 1237: Loss = 3.306575\n",
            "Epoch 8/100, Batch 83, Step 1238: Loss = 3.465796\n",
            "Epoch 8/100, Batch 84, Step 1239: Loss = 3.464719\n",
            "Epoch 8/100, Batch 85, Step 1240: Loss = 3.705417\n",
            "Epoch 8/100, Batch 86, Step 1241: Loss = 3.873240\n",
            "Epoch 8/100, Batch 87, Step 1242: Loss = 3.430118\n",
            "Epoch 8/100, Batch 88, Step 1243: Loss = 3.729110\n",
            "Epoch 8/100, Batch 89, Step 1244: Loss = 3.485101\n",
            "Epoch 8/100, Batch 90, Step 1245: Loss = 3.322201\n",
            "Epoch 8/100, Batch 91, Step 1246: Loss = 3.160173\n",
            "Epoch 8/100, Batch 92, Step 1247: Loss = 3.683850\n",
            "Epoch 8/100, Batch 93, Step 1248: Loss = 3.632515\n",
            "Epoch 8/100, Batch 94, Step 1249: Loss = 3.694503\n",
            "Epoch 8/100, Batch 95, Step 1250: Loss = 3.809096\n",
            "Epoch 8/100, Batch 96, Step 1251: Loss = 3.703703\n",
            "Epoch 8/100, Batch 97, Step 1252: Loss = 3.570677\n",
            "Epoch 8/100, Batch 98, Step 1253: Loss = 3.701901\n",
            "Epoch 8/100, Batch 99, Step 1254: Loss = 3.540631\n",
            "Epoch 8/100, Batch 100, Step 1255: Loss = 3.529638\n",
            "Epoch 8/100, Batch 101, Step 1256: Loss = 3.774408\n",
            "Epoch 8/100, Batch 102, Step 1257: Loss = 3.870882\n",
            "Epoch 8/100, Batch 103, Step 1258: Loss = 3.907195\n",
            "Epoch 8/100, Batch 104, Step 1259: Loss = 3.303013\n",
            "Epoch 8/100, Batch 105, Step 1260: Loss = 3.813673\n",
            "Epoch 8/100, Batch 106, Step 1261: Loss = 3.668808\n",
            "Epoch 8/100, Batch 107, Step 1262: Loss = 3.481764\n",
            "Epoch 8/100, Batch 108, Step 1263: Loss = 3.527005\n",
            "Epoch 8/100, Batch 109, Step 1264: Loss = 3.768042\n",
            "Epoch 8/100, Batch 110, Step 1265: Loss = 3.647738\n",
            "Epoch 8/100, Batch 111, Step 1266: Loss = 3.751469\n",
            "Epoch 8/100, Batch 112, Step 1267: Loss = 3.980320\n",
            "Epoch 8/100, Batch 113, Step 1268: Loss = 3.821718\n",
            "Epoch 8/100, Batch 114, Step 1269: Loss = 3.649519\n",
            "Epoch 8/100, Batch 115, Step 1270: Loss = 3.580958\n",
            "Epoch 8/100, Batch 116, Step 1271: Loss = 3.668856\n",
            "Epoch 8/100, Batch 117, Step 1272: Loss = 3.778121\n",
            "Epoch 8/100, Batch 118, Step 1273: Loss = 3.637280\n",
            "Epoch 8/100, Batch 119, Step 1274: Loss = 3.656139\n",
            "Epoch 8/100, Batch 120, Step 1275: Loss = 3.833046\n",
            "Epoch 8/100, Batch 121, Step 1276: Loss = 3.609225\n",
            "Epoch 8/100, Batch 122, Step 1277: Loss = 3.671289\n",
            "Epoch 8/100, Batch 123, Step 1278: Loss = 3.680348\n",
            "Epoch 8/100, Batch 124, Step 1279: Loss = 3.648255\n",
            "Epoch 8/100, Batch 125, Step 1280: Loss = 3.699719\n",
            "Epoch 8/100, Batch 126, Step 1281: Loss = 3.438237\n",
            "Epoch 8/100, Batch 127, Step 1282: Loss = 3.760803\n",
            "Epoch 8/100, Batch 128, Step 1283: Loss = 3.675493\n",
            "Epoch 8/100, Batch 129, Step 1284: Loss = 3.729645\n",
            "Epoch 8/100, Batch 130, Step 1285: Loss = 3.858800\n",
            "Epoch 8/100, Batch 131, Step 1286: Loss = 4.111302\n",
            "Epoch 8/100, Batch 132, Step 1287: Loss = 3.353535\n",
            "Epoch 8/100, Batch 133, Step 1288: Loss = 3.716151\n",
            "Epoch 8/100, Batch 134, Step 1289: Loss = 3.764103\n",
            "Epoch 8/100, Batch 135, Step 1290: Loss = 3.765441\n",
            "Epoch 8/100, Batch 136, Step 1291: Loss = 3.901592\n",
            "Epoch 8/100, Batch 137, Step 1292: Loss = 3.621349\n",
            "Epoch 8/100, Batch 138, Step 1293: Loss = 3.520519\n",
            "Epoch 8/100, Batch 139, Step 1294: Loss = 3.893163\n",
            "Epoch 8/100, Batch 140, Step 1295: Loss = 3.676869\n",
            "Epoch 8/100, Batch 141, Step 1296: Loss = 3.586434\n",
            "Epoch 8/100, Batch 142, Step 1297: Loss = 3.365439\n",
            "Epoch 8/100, Batch 143, Step 1298: Loss = 3.995878\n",
            "Epoch 8/100, Batch 144, Step 1299: Loss = 3.533815\n",
            "Epoch 8/100, Batch 145, Step 1300: Loss = 3.794506\n",
            "Epoch 8/100, Batch 146, Step 1301: Loss = 3.552307\n",
            "Epoch 8/100, Batch 147, Step 1302: Loss = 3.562640\n",
            "Epoch 8/100, Batch 148, Step 1303: Loss = 3.708813\n",
            "Epoch 8/100, Batch 149, Step 1304: Loss = 3.657811\n",
            "Epoch 8/100, Batch 150, Step 1305: Loss = 3.610548\n",
            "Epoch 8/100, Batch 151, Step 1306: Loss = 3.490183\n",
            "Epoch 8/100, Batch 152, Step 1307: Loss = 3.629229\n",
            "Epoch 8/100, Batch 153, Step 1308: Loss = 3.806123\n",
            "Epoch 8/100, Batch 154, Step 1309: Loss = 3.610330\n",
            "Epoch 8/100, Batch 155, Step 1310: Loss = 3.458988\n",
            "Epoch 8/100, Batch 156, Step 1311: Loss = 3.783978\n",
            "Epoch 8/100, Batch 157, Step 1312: Loss = 3.778750\n",
            "Epoch 8/100, Batch 158, Step 1313: Loss = 3.500955\n",
            "Epoch 8/100, Batch 159, Step 1314: Loss = 3.379324\n",
            "Epoch 8/100, Batch 160, Step 1315: Loss = 3.708795\n",
            "Epoch 8/100, Batch 161, Step 1316: Loss = 3.586980\n",
            "Epoch 8/100, Batch 162, Step 1317: Loss = 3.468246\n",
            "Epoch 8/100, Batch 163, Step 1318: Loss = 3.757868\n",
            "Epoch 8/100, Batch 164, Step 1319: Loss = 3.605506\n",
            "Epoch 8/100, Batch 165, Step 1320: Loss = 3.824555\n",
            " Epoch 8/100, Avg Loss: 3.637645\n",
            "Checkpoint saved..\n",
            "Epoch 9/100, Batch 1, Step 1321: Loss = 3.295377\n",
            "Epoch 9/100, Batch 2, Step 1322: Loss = 3.386816\n",
            "Epoch 9/100, Batch 3, Step 1323: Loss = 3.327070\n",
            "Epoch 9/100, Batch 4, Step 1324: Loss = 3.232899\n",
            "Epoch 9/100, Batch 5, Step 1325: Loss = 3.452236\n",
            "Epoch 9/100, Batch 6, Step 1326: Loss = 3.303858\n",
            "Epoch 9/100, Batch 7, Step 1327: Loss = 3.138826\n",
            "Epoch 9/100, Batch 8, Step 1328: Loss = 3.362472\n",
            "Epoch 9/100, Batch 9, Step 1329: Loss = 3.521494\n",
            "Epoch 9/100, Batch 10, Step 1330: Loss = 3.287214\n",
            "Epoch 9/100, Batch 11, Step 1331: Loss = 3.436327\n",
            "Epoch 9/100, Batch 12, Step 1332: Loss = 3.463094\n",
            "Epoch 9/100, Batch 13, Step 1333: Loss = 3.233905\n",
            "Epoch 9/100, Batch 14, Step 1334: Loss = 3.513697\n",
            "Epoch 9/100, Batch 15, Step 1335: Loss = 3.399796\n",
            "Epoch 9/100, Batch 16, Step 1336: Loss = 3.304507\n",
            "Epoch 9/100, Batch 17, Step 1337: Loss = 3.470864\n",
            "Epoch 9/100, Batch 18, Step 1338: Loss = 3.467216\n",
            "Epoch 9/100, Batch 19, Step 1339: Loss = 3.382397\n",
            "Epoch 9/100, Batch 20, Step 1340: Loss = 3.302157\n",
            "Epoch 9/100, Batch 21, Step 1341: Loss = 3.465333\n",
            "Epoch 9/100, Batch 22, Step 1342: Loss = 3.624398\n",
            "Epoch 9/100, Batch 23, Step 1343: Loss = 3.518994\n",
            "Epoch 9/100, Batch 24, Step 1344: Loss = 3.500103\n",
            "Epoch 9/100, Batch 25, Step 1345: Loss = 3.441533\n",
            "Epoch 9/100, Batch 26, Step 1346: Loss = 3.307323\n",
            "Epoch 9/100, Batch 27, Step 1347: Loss = 3.446795\n",
            "Epoch 9/100, Batch 28, Step 1348: Loss = 3.351895\n",
            "Epoch 9/100, Batch 29, Step 1349: Loss = 3.427681\n",
            "Epoch 9/100, Batch 30, Step 1350: Loss = 3.289312\n",
            "Epoch 9/100, Batch 31, Step 1351: Loss = 3.435481\n",
            "Epoch 9/100, Batch 32, Step 1352: Loss = 3.455206\n",
            "Epoch 9/100, Batch 33, Step 1353: Loss = 3.407816\n",
            "Epoch 9/100, Batch 34, Step 1354: Loss = 3.610208\n",
            "Epoch 9/100, Batch 35, Step 1355: Loss = 3.470680\n",
            "Epoch 9/100, Batch 36, Step 1356: Loss = 3.431920\n",
            "Epoch 9/100, Batch 37, Step 1357: Loss = 3.672253\n",
            "Epoch 9/100, Batch 38, Step 1358: Loss = 3.378049\n",
            "Epoch 9/100, Batch 39, Step 1359: Loss = 3.217056\n",
            "Epoch 9/100, Batch 40, Step 1360: Loss = 3.368814\n",
            "Epoch 9/100, Batch 41, Step 1361: Loss = 3.398958\n",
            "Epoch 9/100, Batch 42, Step 1362: Loss = 3.253341\n",
            "Epoch 9/100, Batch 43, Step 1363: Loss = 3.462121\n",
            "Epoch 9/100, Batch 44, Step 1364: Loss = 3.200933\n",
            "Epoch 9/100, Batch 45, Step 1365: Loss = 3.371268\n",
            "Epoch 9/100, Batch 46, Step 1366: Loss = 3.257710\n",
            "Epoch 9/100, Batch 47, Step 1367: Loss = 3.613336\n",
            "Epoch 9/100, Batch 48, Step 1368: Loss = 3.522380\n",
            "Epoch 9/100, Batch 49, Step 1369: Loss = 3.477264\n",
            "Epoch 9/100, Batch 50, Step 1370: Loss = 3.330344\n",
            "Epoch 9/100, Batch 51, Step 1371: Loss = 3.468433\n",
            "Epoch 9/100, Batch 52, Step 1372: Loss = 3.506578\n",
            "Epoch 9/100, Batch 53, Step 1373: Loss = 3.377560\n",
            "Epoch 9/100, Batch 54, Step 1374: Loss = 3.503887\n",
            "Epoch 9/100, Batch 55, Step 1375: Loss = 3.413638\n",
            "Epoch 9/100, Batch 56, Step 1376: Loss = 3.379258\n",
            "Epoch 9/100, Batch 57, Step 1377: Loss = 3.320416\n",
            "Epoch 9/100, Batch 58, Step 1378: Loss = 3.359469\n",
            "Epoch 9/100, Batch 59, Step 1379: Loss = 3.320522\n",
            "Epoch 9/100, Batch 60, Step 1380: Loss = 3.568817\n",
            "Epoch 9/100, Batch 61, Step 1381: Loss = 3.556439\n",
            "Epoch 9/100, Batch 62, Step 1382: Loss = 3.560124\n",
            "Epoch 9/100, Batch 63, Step 1383: Loss = 3.296930\n",
            "Epoch 9/100, Batch 64, Step 1384: Loss = 3.524047\n",
            "Epoch 9/100, Batch 65, Step 1385: Loss = 3.554091\n",
            "Epoch 9/100, Batch 66, Step 1386: Loss = 2.968946\n",
            "Epoch 9/100, Batch 67, Step 1387: Loss = 3.425348\n",
            "Epoch 9/100, Batch 68, Step 1388: Loss = 3.402727\n",
            "Epoch 9/100, Batch 69, Step 1389: Loss = 3.568374\n",
            "Epoch 9/100, Batch 70, Step 1390: Loss = 3.441182\n",
            "Epoch 9/100, Batch 71, Step 1391: Loss = 3.324811\n",
            "Epoch 9/100, Batch 72, Step 1392: Loss = 3.302238\n",
            "Epoch 9/100, Batch 73, Step 1393: Loss = 3.511689\n",
            "Epoch 9/100, Batch 74, Step 1394: Loss = 3.569496\n",
            "Epoch 9/100, Batch 75, Step 1395: Loss = 3.298099\n",
            "Epoch 9/100, Batch 76, Step 1396: Loss = 3.548689\n",
            "Epoch 9/100, Batch 77, Step 1397: Loss = 3.312825\n",
            "Epoch 9/100, Batch 78, Step 1398: Loss = 3.135409\n",
            "Epoch 9/100, Batch 79, Step 1399: Loss = 3.576607\n",
            "Epoch 9/100, Batch 80, Step 1400: Loss = 3.473683\n",
            "Epoch 9/100, Batch 81, Step 1401: Loss = 3.432776\n",
            "Epoch 9/100, Batch 82, Step 1402: Loss = 3.651682\n",
            "Epoch 9/100, Batch 83, Step 1403: Loss = 3.666808\n",
            "Epoch 9/100, Batch 84, Step 1404: Loss = 3.758464\n",
            "Epoch 9/100, Batch 85, Step 1405: Loss = 3.490341\n",
            "Epoch 9/100, Batch 86, Step 1406: Loss = 3.593775\n",
            "Epoch 9/100, Batch 87, Step 1407: Loss = 3.613379\n",
            "Epoch 9/100, Batch 88, Step 1408: Loss = 3.501853\n",
            "Epoch 9/100, Batch 89, Step 1409: Loss = 3.167875\n",
            "Epoch 9/100, Batch 90, Step 1410: Loss = 3.338057\n",
            "Epoch 9/100, Batch 91, Step 1411: Loss = 3.756101\n",
            "Epoch 9/100, Batch 92, Step 1412: Loss = 3.316584\n",
            "Epoch 9/100, Batch 93, Step 1413: Loss = 3.238555\n",
            "Epoch 9/100, Batch 94, Step 1414: Loss = 3.610205\n",
            "Epoch 9/100, Batch 95, Step 1415: Loss = 3.553771\n",
            "Epoch 9/100, Batch 96, Step 1416: Loss = 3.486617\n",
            "Epoch 9/100, Batch 97, Step 1417: Loss = 3.401891\n",
            "Epoch 9/100, Batch 98, Step 1418: Loss = 3.264873\n",
            "Epoch 9/100, Batch 99, Step 1419: Loss = 3.489730\n",
            "Epoch 9/100, Batch 100, Step 1420: Loss = 3.855942\n",
            "Epoch 9/100, Batch 101, Step 1421: Loss = 3.399981\n",
            "Epoch 9/100, Batch 102, Step 1422: Loss = 3.128336\n",
            "Epoch 9/100, Batch 103, Step 1423: Loss = 3.330661\n",
            "Epoch 9/100, Batch 104, Step 1424: Loss = 3.466126\n",
            "Epoch 9/100, Batch 105, Step 1425: Loss = 3.646107\n",
            "Epoch 9/100, Batch 106, Step 1426: Loss = 3.374879\n",
            "Epoch 9/100, Batch 107, Step 1427: Loss = 3.354553\n",
            "Epoch 9/100, Batch 108, Step 1428: Loss = 3.422628\n",
            "Epoch 9/100, Batch 109, Step 1429: Loss = 3.429227\n",
            "Epoch 9/100, Batch 110, Step 1430: Loss = 3.500477\n",
            "Epoch 9/100, Batch 111, Step 1431: Loss = 3.588410\n",
            "Epoch 9/100, Batch 112, Step 1432: Loss = 3.670369\n",
            "Epoch 9/100, Batch 113, Step 1433: Loss = 3.650990\n",
            "Epoch 9/100, Batch 114, Step 1434: Loss = 3.676905\n",
            "Epoch 9/100, Batch 115, Step 1435: Loss = 3.506993\n",
            "Epoch 9/100, Batch 116, Step 1436: Loss = 3.350567\n",
            "Epoch 9/100, Batch 117, Step 1437: Loss = 3.741072\n",
            "Epoch 9/100, Batch 118, Step 1438: Loss = 3.171677\n",
            "Epoch 9/100, Batch 119, Step 1439: Loss = 3.480001\n",
            "Epoch 9/100, Batch 120, Step 1440: Loss = 3.556829\n",
            "Epoch 9/100, Batch 121, Step 1441: Loss = 3.540934\n",
            "Epoch 9/100, Batch 122, Step 1442: Loss = 3.651742\n",
            "Epoch 9/100, Batch 123, Step 1443: Loss = 3.165863\n",
            "Epoch 9/100, Batch 124, Step 1444: Loss = 3.594927\n",
            "Epoch 9/100, Batch 125, Step 1445: Loss = 3.423162\n",
            "Epoch 9/100, Batch 126, Step 1446: Loss = 3.489273\n",
            "Epoch 9/100, Batch 127, Step 1447: Loss = 3.392286\n",
            "Epoch 9/100, Batch 128, Step 1448: Loss = 3.393741\n",
            "Epoch 9/100, Batch 129, Step 1449: Loss = 3.644687\n",
            "Epoch 9/100, Batch 130, Step 1450: Loss = 3.364128\n",
            "Epoch 9/100, Batch 131, Step 1451: Loss = 3.533636\n",
            "Epoch 9/100, Batch 132, Step 1452: Loss = 2.810636\n",
            "Epoch 9/100, Batch 133, Step 1453: Loss = 3.549301\n",
            "Epoch 9/100, Batch 134, Step 1454: Loss = 3.765212\n",
            "Epoch 9/100, Batch 135, Step 1455: Loss = 3.514752\n",
            "Epoch 9/100, Batch 136, Step 1456: Loss = 3.331079\n",
            "Epoch 9/100, Batch 137, Step 1457: Loss = 3.562558\n",
            "Epoch 9/100, Batch 138, Step 1458: Loss = 3.643242\n",
            "Epoch 9/100, Batch 139, Step 1459: Loss = 3.680779\n",
            "Epoch 9/100, Batch 140, Step 1460: Loss = 3.403400\n",
            "Epoch 9/100, Batch 141, Step 1461: Loss = 3.408921\n",
            "Epoch 9/100, Batch 142, Step 1462: Loss = 3.440008\n",
            "Epoch 9/100, Batch 143, Step 1463: Loss = 3.341648\n",
            "Epoch 9/100, Batch 144, Step 1464: Loss = 3.628355\n",
            "Epoch 9/100, Batch 145, Step 1465: Loss = 3.579497\n",
            "Epoch 9/100, Batch 146, Step 1466: Loss = 3.333390\n",
            "Epoch 9/100, Batch 147, Step 1467: Loss = 3.264204\n",
            "Epoch 9/100, Batch 148, Step 1468: Loss = 3.526333\n",
            "Epoch 9/100, Batch 149, Step 1469: Loss = 3.460383\n",
            "Epoch 9/100, Batch 150, Step 1470: Loss = 3.433378\n",
            "Epoch 9/100, Batch 151, Step 1471: Loss = 3.251089\n",
            "Epoch 9/100, Batch 152, Step 1472: Loss = 3.335830\n",
            "Epoch 9/100, Batch 153, Step 1473: Loss = 3.467858\n",
            "Epoch 9/100, Batch 154, Step 1474: Loss = 3.549199\n",
            "Epoch 9/100, Batch 155, Step 1475: Loss = 3.254474\n",
            "Epoch 9/100, Batch 156, Step 1476: Loss = 3.251426\n",
            "Epoch 9/100, Batch 157, Step 1477: Loss = 3.744596\n",
            "Epoch 9/100, Batch 158, Step 1478: Loss = 3.329014\n",
            "Epoch 9/100, Batch 159, Step 1479: Loss = 3.463578\n",
            "Epoch 9/100, Batch 160, Step 1480: Loss = 3.495963\n",
            "Epoch 9/100, Batch 161, Step 1481: Loss = 3.572100\n",
            "Epoch 9/100, Batch 162, Step 1482: Loss = 3.521645\n",
            "Epoch 9/100, Batch 163, Step 1483: Loss = 3.341663\n",
            "Epoch 9/100, Batch 164, Step 1484: Loss = 3.595354\n",
            "Epoch 9/100, Batch 165, Step 1485: Loss = 3.559359\n",
            " Epoch 9/100, Avg Loss: 3.441223\n",
            "Checkpoint saved..\n",
            "Epoch 10/100, Batch 1, Step 1486: Loss = 3.213457\n",
            "Epoch 10/100, Batch 2, Step 1487: Loss = 3.103746\n",
            "Epoch 10/100, Batch 3, Step 1488: Loss = 3.279654\n",
            "Epoch 10/100, Batch 4, Step 1489: Loss = 3.185531\n",
            "Epoch 10/100, Batch 5, Step 1490: Loss = 3.064883\n",
            "Epoch 10/100, Batch 6, Step 1491: Loss = 3.217394\n",
            "Epoch 10/100, Batch 7, Step 1492: Loss = 3.294028\n",
            "Epoch 10/100, Batch 8, Step 1493: Loss = 3.074151\n",
            "Epoch 10/100, Batch 9, Step 1494: Loss = 3.364966\n",
            "Epoch 10/100, Batch 10, Step 1495: Loss = 3.037972\n",
            "Epoch 10/100, Batch 11, Step 1496: Loss = 3.173607\n",
            "Epoch 10/100, Batch 12, Step 1497: Loss = 3.353059\n",
            "Epoch 10/100, Batch 13, Step 1498: Loss = 2.697097\n",
            "Epoch 10/100, Batch 14, Step 1499: Loss = 3.029230\n",
            "Epoch 10/100, Batch 15, Step 1500: Loss = 3.067973\n",
            "Epoch 10/100, Batch 16, Step 1501: Loss = 3.092683\n",
            "Epoch 10/100, Batch 17, Step 1502: Loss = 3.027204\n",
            "Epoch 10/100, Batch 18, Step 1503: Loss = 3.135770\n",
            "Epoch 10/100, Batch 19, Step 1504: Loss = 2.965735\n",
            "Epoch 10/100, Batch 20, Step 1505: Loss = 3.111409\n",
            "Epoch 10/100, Batch 21, Step 1506: Loss = 3.256497\n",
            "Epoch 10/100, Batch 22, Step 1507: Loss = 3.128654\n",
            "Epoch 10/100, Batch 23, Step 1508: Loss = 3.139906\n",
            "Epoch 10/100, Batch 24, Step 1509: Loss = 3.207446\n",
            "Epoch 10/100, Batch 25, Step 1510: Loss = 3.429606\n",
            "Epoch 10/100, Batch 26, Step 1511: Loss = 3.245968\n",
            "Epoch 10/100, Batch 27, Step 1512: Loss = 3.202869\n",
            "Epoch 10/100, Batch 28, Step 1513: Loss = 3.144931\n",
            "Epoch 10/100, Batch 29, Step 1514: Loss = 3.141535\n",
            "Epoch 10/100, Batch 30, Step 1515: Loss = 2.987808\n",
            "Epoch 10/100, Batch 31, Step 1516: Loss = 3.233799\n",
            "Epoch 10/100, Batch 32, Step 1517: Loss = 2.827129\n",
            "Epoch 10/100, Batch 33, Step 1518: Loss = 2.934002\n",
            "Epoch 10/100, Batch 34, Step 1519: Loss = 3.135235\n",
            "Epoch 10/100, Batch 35, Step 1520: Loss = 3.230826\n",
            "Epoch 10/100, Batch 36, Step 1521: Loss = 3.053164\n",
            "Epoch 10/100, Batch 37, Step 1522: Loss = 3.533350\n",
            "Epoch 10/100, Batch 38, Step 1523: Loss = 3.096787\n",
            "Epoch 10/100, Batch 39, Step 1524: Loss = 3.183162\n",
            "Epoch 10/100, Batch 40, Step 1525: Loss = 3.000009\n",
            "Epoch 10/100, Batch 41, Step 1526: Loss = 3.312944\n",
            "Epoch 10/100, Batch 42, Step 1527: Loss = 3.086726\n",
            "Epoch 10/100, Batch 43, Step 1528: Loss = 3.105006\n",
            "Epoch 10/100, Batch 44, Step 1529: Loss = 3.186814\n",
            "Epoch 10/100, Batch 45, Step 1530: Loss = 3.320036\n",
            "Epoch 10/100, Batch 46, Step 1531: Loss = 3.167967\n",
            "Epoch 10/100, Batch 47, Step 1532: Loss = 2.842147\n",
            "Epoch 10/100, Batch 48, Step 1533: Loss = 2.993568\n",
            "Epoch 10/100, Batch 49, Step 1534: Loss = 3.103057\n",
            "Epoch 10/100, Batch 50, Step 1535: Loss = 3.316554\n",
            "Epoch 10/100, Batch 51, Step 1536: Loss = 3.073389\n",
            "Epoch 10/100, Batch 52, Step 1537: Loss = 3.011630\n",
            "Epoch 10/100, Batch 53, Step 1538: Loss = 3.036017\n",
            "Epoch 10/100, Batch 54, Step 1539: Loss = 3.134205\n",
            "Epoch 10/100, Batch 55, Step 1540: Loss = 3.036170\n",
            "Epoch 10/100, Batch 56, Step 1541: Loss = 3.230690\n",
            "Epoch 10/100, Batch 57, Step 1542: Loss = 3.192392\n",
            "Epoch 10/100, Batch 58, Step 1543: Loss = 3.095353\n",
            "Epoch 10/100, Batch 59, Step 1544: Loss = 3.165072\n",
            "Epoch 10/100, Batch 60, Step 1545: Loss = 3.018559\n",
            "Epoch 10/100, Batch 61, Step 1546: Loss = 3.127765\n",
            "Epoch 10/100, Batch 62, Step 1547: Loss = 3.207479\n",
            "Epoch 10/100, Batch 63, Step 1548: Loss = 3.169107\n",
            "Epoch 10/100, Batch 64, Step 1549: Loss = 3.171187\n",
            "Epoch 10/100, Batch 65, Step 1550: Loss = 3.174446\n",
            "Epoch 10/100, Batch 66, Step 1551: Loss = 3.169422\n",
            "Epoch 10/100, Batch 67, Step 1552: Loss = 3.364909\n",
            "Epoch 10/100, Batch 68, Step 1553: Loss = 3.293606\n",
            "Epoch 10/100, Batch 69, Step 1554: Loss = 3.235413\n",
            "Epoch 10/100, Batch 70, Step 1555: Loss = 3.508764\n",
            "Epoch 10/100, Batch 71, Step 1556: Loss = 3.064137\n",
            "Epoch 10/100, Batch 72, Step 1557: Loss = 3.254061\n",
            "Epoch 10/100, Batch 73, Step 1558: Loss = 3.231232\n",
            "Epoch 10/100, Batch 74, Step 1559: Loss = 3.096704\n",
            "Epoch 10/100, Batch 75, Step 1560: Loss = 3.107400\n",
            "Epoch 10/100, Batch 76, Step 1561: Loss = 3.105106\n",
            "Epoch 10/100, Batch 77, Step 1562: Loss = 3.342280\n",
            "Epoch 10/100, Batch 78, Step 1563: Loss = 2.975922\n",
            "Epoch 10/100, Batch 79, Step 1564: Loss = 3.206700\n",
            "Epoch 10/100, Batch 80, Step 1565: Loss = 3.186757\n",
            "Epoch 10/100, Batch 81, Step 1566: Loss = 3.111860\n",
            "Epoch 10/100, Batch 82, Step 1567: Loss = 3.113791\n",
            "Epoch 10/100, Batch 83, Step 1568: Loss = 3.068827\n",
            "Epoch 10/100, Batch 84, Step 1569: Loss = 3.140630\n",
            "Epoch 10/100, Batch 85, Step 1570: Loss = 3.229184\n",
            "Epoch 10/100, Batch 86, Step 1571: Loss = 3.366022\n",
            "Epoch 10/100, Batch 87, Step 1572: Loss = 3.133631\n",
            "Epoch 10/100, Batch 88, Step 1573: Loss = 3.108379\n",
            "Epoch 10/100, Batch 89, Step 1574: Loss = 2.997425\n",
            "Epoch 10/100, Batch 90, Step 1575: Loss = 2.999096\n",
            "Epoch 10/100, Batch 91, Step 1576: Loss = 2.991674\n",
            "Epoch 10/100, Batch 92, Step 1577: Loss = 2.986861\n",
            "Epoch 10/100, Batch 93, Step 1578: Loss = 2.861892\n",
            "Epoch 10/100, Batch 94, Step 1579: Loss = 3.105907\n",
            "Epoch 10/100, Batch 95, Step 1580: Loss = 3.212982\n",
            "Epoch 10/100, Batch 96, Step 1581: Loss = 3.319165\n",
            "Epoch 10/100, Batch 97, Step 1582: Loss = 3.282079\n",
            "Epoch 10/100, Batch 98, Step 1583: Loss = 3.084254\n",
            "Epoch 10/100, Batch 99, Step 1584: Loss = 2.970629\n",
            "Epoch 10/100, Batch 100, Step 1585: Loss = 3.067426\n",
            "Epoch 10/100, Batch 101, Step 1586: Loss = 3.117762\n",
            "Epoch 10/100, Batch 102, Step 1587: Loss = 2.999915\n",
            "Epoch 10/100, Batch 103, Step 1588: Loss = 3.222590\n",
            "Epoch 10/100, Batch 104, Step 1589: Loss = 2.686828\n",
            "Epoch 10/100, Batch 105, Step 1590: Loss = 2.926697\n",
            "Epoch 10/100, Batch 106, Step 1591: Loss = 2.679762\n",
            "Epoch 10/100, Batch 107, Step 1592: Loss = 2.925284\n",
            "Epoch 10/100, Batch 108, Step 1593: Loss = 2.973143\n",
            "Epoch 10/100, Batch 109, Step 1594: Loss = 3.160971\n",
            "Epoch 10/100, Batch 110, Step 1595: Loss = 3.128790\n",
            "Epoch 10/100, Batch 111, Step 1596: Loss = 3.031483\n",
            "Epoch 10/100, Batch 112, Step 1597: Loss = 3.216969\n",
            "Epoch 10/100, Batch 113, Step 1598: Loss = 3.101375\n",
            "Epoch 10/100, Batch 114, Step 1599: Loss = 3.115388\n",
            "Epoch 10/100, Batch 115, Step 1600: Loss = 3.105588\n",
            "Epoch 10/100, Batch 116, Step 1601: Loss = 3.151445\n",
            "Epoch 10/100, Batch 117, Step 1602: Loss = 3.068246\n",
            "Epoch 10/100, Batch 118, Step 1603: Loss = 3.192094\n",
            "Epoch 10/100, Batch 119, Step 1604: Loss = 3.176355\n",
            "Epoch 10/100, Batch 120, Step 1605: Loss = 3.117770\n",
            "Epoch 10/100, Batch 121, Step 1606: Loss = 3.284656\n",
            "Epoch 10/100, Batch 122, Step 1607: Loss = 3.030608\n",
            "Epoch 10/100, Batch 123, Step 1608: Loss = 3.180166\n",
            "Epoch 10/100, Batch 124, Step 1609: Loss = 3.299853\n",
            "Epoch 10/100, Batch 125, Step 1610: Loss = 3.137798\n",
            "Epoch 10/100, Batch 126, Step 1611: Loss = 3.140406\n",
            "Epoch 10/100, Batch 127, Step 1612: Loss = 3.046911\n",
            "Epoch 10/100, Batch 128, Step 1613: Loss = 2.814245\n",
            "Epoch 10/100, Batch 129, Step 1614: Loss = 2.753500\n",
            "Epoch 10/100, Batch 130, Step 1615: Loss = 3.155035\n",
            "Epoch 10/100, Batch 131, Step 1616: Loss = 3.153103\n",
            "Epoch 10/100, Batch 132, Step 1617: Loss = 2.972700\n",
            "Epoch 10/100, Batch 133, Step 1618: Loss = 2.905717\n",
            "Epoch 10/100, Batch 134, Step 1619: Loss = 3.146934\n",
            "Epoch 10/100, Batch 135, Step 1620: Loss = 2.971596\n",
            "Epoch 10/100, Batch 136, Step 1621: Loss = 3.070115\n",
            "Epoch 10/100, Batch 137, Step 1622: Loss = 2.747102\n",
            "Epoch 10/100, Batch 138, Step 1623: Loss = 2.933964\n",
            "Epoch 10/100, Batch 139, Step 1624: Loss = 3.052761\n",
            "Epoch 10/100, Batch 140, Step 1625: Loss = 3.097000\n",
            "Epoch 10/100, Batch 141, Step 1626: Loss = 3.068707\n",
            "Epoch 10/100, Batch 142, Step 1627: Loss = 2.977567\n",
            "Epoch 10/100, Batch 143, Step 1628: Loss = 3.212234\n",
            "Epoch 10/100, Batch 144, Step 1629: Loss = 3.059019\n",
            "Epoch 10/100, Batch 145, Step 1630: Loss = 3.205238\n",
            "Epoch 10/100, Batch 146, Step 1631: Loss = 3.195192\n",
            "Epoch 10/100, Batch 147, Step 1632: Loss = 3.126872\n",
            "Epoch 10/100, Batch 148, Step 1633: Loss = 3.090837\n",
            "Epoch 10/100, Batch 149, Step 1634: Loss = 3.033200\n",
            "Epoch 10/100, Batch 150, Step 1635: Loss = 2.731632\n",
            "Epoch 10/100, Batch 151, Step 1636: Loss = 3.057145\n",
            "Epoch 10/100, Batch 152, Step 1637: Loss = 2.732797\n",
            "Epoch 10/100, Batch 153, Step 1638: Loss = 3.123960\n",
            "Epoch 10/100, Batch 154, Step 1639: Loss = 2.905218\n",
            "Epoch 10/100, Batch 155, Step 1640: Loss = 2.817623\n",
            "Epoch 10/100, Batch 156, Step 1641: Loss = 2.887882\n",
            "Epoch 10/100, Batch 157, Step 1642: Loss = 3.263734\n",
            "Epoch 10/100, Batch 158, Step 1643: Loss = 2.930977\n",
            "Epoch 10/100, Batch 159, Step 1644: Loss = 2.965904\n",
            "Epoch 10/100, Batch 160, Step 1645: Loss = 3.060197\n",
            "Epoch 10/100, Batch 161, Step 1646: Loss = 2.988353\n",
            "Epoch 10/100, Batch 162, Step 1647: Loss = 3.039031\n",
            "Epoch 10/100, Batch 163, Step 1648: Loss = 2.842755\n",
            "Epoch 10/100, Batch 164, Step 1649: Loss = 2.964884\n",
            "Epoch 10/100, Batch 165, Step 1650: Loss = 2.877343\n",
            " Epoch 10/100, Avg Loss: 3.098943\n",
            "Checkpoint saved..\n",
            "Epoch 11/100, Batch 1, Step 1651: Loss = 2.364228\n",
            "Epoch 11/100, Batch 2, Step 1652: Loss = 2.424595\n",
            "Epoch 11/100, Batch 3, Step 1653: Loss = 2.589118\n",
            "Epoch 11/100, Batch 4, Step 1654: Loss = 2.642574\n",
            "Epoch 11/100, Batch 5, Step 1655: Loss = 2.490928\n",
            "Epoch 11/100, Batch 6, Step 1656: Loss = 2.443068\n",
            "Epoch 11/100, Batch 7, Step 1657: Loss = 2.609201\n",
            "Epoch 11/100, Batch 8, Step 1658: Loss = 2.538837\n",
            "Epoch 11/100, Batch 9, Step 1659: Loss = 2.560745\n",
            "Epoch 11/100, Batch 10, Step 1660: Loss = 2.535507\n",
            "Epoch 11/100, Batch 11, Step 1661: Loss = 2.602246\n",
            "Epoch 11/100, Batch 12, Step 1662: Loss = 2.452405\n",
            "Epoch 11/100, Batch 13, Step 1663: Loss = 2.469520\n",
            "Epoch 11/100, Batch 14, Step 1664: Loss = 2.491663\n",
            "Epoch 11/100, Batch 15, Step 1665: Loss = 2.397771\n",
            "Epoch 11/100, Batch 16, Step 1666: Loss = 2.415238\n",
            "Epoch 11/100, Batch 17, Step 1667: Loss = 2.386168\n",
            "Epoch 11/100, Batch 18, Step 1668: Loss = 2.662583\n",
            "Epoch 11/100, Batch 19, Step 1669: Loss = 2.212101\n",
            "Epoch 11/100, Batch 20, Step 1670: Loss = 2.339156\n",
            "Epoch 11/100, Batch 21, Step 1671: Loss = 2.303640\n",
            "Epoch 11/100, Batch 22, Step 1672: Loss = 2.519229\n",
            "Epoch 11/100, Batch 23, Step 1673: Loss = 2.577688\n",
            "Epoch 11/100, Batch 24, Step 1674: Loss = 2.263833\n",
            "Epoch 11/100, Batch 25, Step 1675: Loss = 2.376237\n",
            "Epoch 11/100, Batch 26, Step 1676: Loss = 2.450662\n",
            "Epoch 11/100, Batch 27, Step 1677: Loss = 2.345494\n",
            "Epoch 11/100, Batch 28, Step 1678: Loss = 2.230861\n",
            "Epoch 11/100, Batch 29, Step 1679: Loss = 2.251588\n",
            "Epoch 11/100, Batch 30, Step 1680: Loss = 2.313040\n",
            "Epoch 11/100, Batch 31, Step 1681: Loss = 2.142122\n",
            "Epoch 11/100, Batch 32, Step 1682: Loss = 2.364509\n",
            "Epoch 11/100, Batch 33, Step 1683: Loss = 2.330580\n",
            "Epoch 11/100, Batch 34, Step 1684: Loss = 2.081622\n",
            "Epoch 11/100, Batch 35, Step 1685: Loss = 2.197956\n",
            "Epoch 11/100, Batch 36, Step 1686: Loss = 2.308685\n",
            "Epoch 11/100, Batch 37, Step 1687: Loss = 2.228911\n",
            "Epoch 11/100, Batch 38, Step 1688: Loss = 2.239312\n",
            "Epoch 11/100, Batch 39, Step 1689: Loss = 2.296419\n",
            "Epoch 11/100, Batch 40, Step 1690: Loss = 2.269238\n",
            "Epoch 11/100, Batch 41, Step 1691: Loss = 2.130727\n",
            "Epoch 11/100, Batch 42, Step 1692: Loss = 2.357374\n",
            "Epoch 11/100, Batch 43, Step 1693: Loss = 2.200583\n",
            "Epoch 11/100, Batch 44, Step 1694: Loss = 2.310462\n",
            "Epoch 11/100, Batch 45, Step 1695: Loss = 2.212978\n",
            "Epoch 11/100, Batch 46, Step 1696: Loss = 2.351867\n",
            "Epoch 11/100, Batch 47, Step 1697: Loss = 2.195058\n",
            "Epoch 11/100, Batch 48, Step 1698: Loss = 2.092642\n",
            "Epoch 11/100, Batch 49, Step 1699: Loss = 2.061324\n",
            "Epoch 11/100, Batch 50, Step 1700: Loss = 2.054590\n",
            "Epoch 11/100, Batch 51, Step 1701: Loss = 2.126517\n",
            "Epoch 11/100, Batch 52, Step 1702: Loss = 2.281122\n",
            "Epoch 11/100, Batch 53, Step 1703: Loss = 2.239645\n",
            "Epoch 11/100, Batch 54, Step 1704: Loss = 2.282561\n",
            "Epoch 11/100, Batch 55, Step 1705: Loss = 1.907093\n",
            "Epoch 11/100, Batch 56, Step 1706: Loss = 2.042847\n",
            "Epoch 11/100, Batch 57, Step 1707: Loss = 2.207709\n",
            "Epoch 11/100, Batch 58, Step 1708: Loss = 2.134774\n",
            "Epoch 11/100, Batch 59, Step 1709: Loss = 2.081053\n",
            "Epoch 11/100, Batch 60, Step 1710: Loss = 2.044433\n",
            "Epoch 11/100, Batch 61, Step 1711: Loss = 2.321508\n",
            "Epoch 11/100, Batch 62, Step 1712: Loss = 2.220076\n",
            "Epoch 11/100, Batch 63, Step 1713: Loss = 1.999429\n",
            "Epoch 11/100, Batch 64, Step 1714: Loss = 1.967266\n",
            "Epoch 11/100, Batch 65, Step 1715: Loss = 2.094061\n",
            "Epoch 11/100, Batch 66, Step 1716: Loss = 1.693566\n",
            "Epoch 11/100, Batch 67, Step 1717: Loss = 2.038646\n",
            "Epoch 11/100, Batch 68, Step 1718: Loss = 1.906907\n",
            "Epoch 11/100, Batch 69, Step 1719: Loss = 1.956217\n",
            "Epoch 11/100, Batch 70, Step 1720: Loss = 1.905098\n",
            "Epoch 11/100, Batch 71, Step 1721: Loss = 2.047143\n",
            "Epoch 11/100, Batch 72, Step 1722: Loss = 1.868367\n",
            "Epoch 11/100, Batch 73, Step 1723: Loss = 1.874900\n",
            "Epoch 11/100, Batch 74, Step 1724: Loss = 2.077646\n",
            "Epoch 11/100, Batch 75, Step 1725: Loss = 2.049029\n",
            "Epoch 11/100, Batch 76, Step 1726: Loss = 2.190353\n",
            "Epoch 11/100, Batch 77, Step 1727: Loss = 1.993753\n",
            "Epoch 11/100, Batch 78, Step 1728: Loss = 1.800725\n",
            "Epoch 11/100, Batch 79, Step 1729: Loss = 1.912880\n",
            "Epoch 11/100, Batch 80, Step 1730: Loss = 1.980129\n",
            "Epoch 11/100, Batch 81, Step 1731: Loss = 1.808552\n",
            "Epoch 11/100, Batch 82, Step 1732: Loss = 2.017165\n",
            "Epoch 11/100, Batch 83, Step 1733: Loss = 2.145224\n",
            "Epoch 11/100, Batch 84, Step 1734: Loss = 1.851234\n",
            "Epoch 11/100, Batch 85, Step 1735: Loss = 2.171902\n",
            "Epoch 11/100, Batch 86, Step 1736: Loss = 1.644501\n",
            "Epoch 11/100, Batch 87, Step 1737: Loss = 1.744038\n",
            "Epoch 11/100, Batch 88, Step 1738: Loss = 1.902529\n",
            "Epoch 11/100, Batch 89, Step 1739: Loss = 1.822970\n",
            "Epoch 11/100, Batch 90, Step 1740: Loss = 1.869632\n",
            "Epoch 11/100, Batch 91, Step 1741: Loss = 1.882747\n",
            "Epoch 11/100, Batch 92, Step 1742: Loss = 1.858192\n",
            "Epoch 11/100, Batch 93, Step 1743: Loss = 2.045446\n",
            "Epoch 11/100, Batch 94, Step 1744: Loss = 1.844322\n",
            "Epoch 11/100, Batch 95, Step 1745: Loss = 1.905466\n",
            "Epoch 11/100, Batch 96, Step 1746: Loss = 1.880562\n",
            "Epoch 11/100, Batch 97, Step 1747: Loss = 1.821513\n",
            "Epoch 11/100, Batch 98, Step 1748: Loss = 1.606559\n",
            "Epoch 11/100, Batch 99, Step 1749: Loss = 1.480157\n",
            "Epoch 11/100, Batch 100, Step 1750: Loss = 1.775179\n",
            "Epoch 11/100, Batch 101, Step 1751: Loss = 1.827485\n",
            "Epoch 11/100, Batch 102, Step 1752: Loss = 1.823835\n",
            "Epoch 11/100, Batch 103, Step 1753: Loss = 1.701308\n",
            "Epoch 11/100, Batch 104, Step 1754: Loss = 1.806328\n",
            "Epoch 11/100, Batch 105, Step 1755: Loss = 1.738286\n",
            "Epoch 11/100, Batch 106, Step 1756: Loss = 1.711077\n",
            "Epoch 11/100, Batch 107, Step 1757: Loss = 1.863125\n",
            "Epoch 11/100, Batch 108, Step 1758: Loss = 1.534506\n",
            "Epoch 11/100, Batch 109, Step 1759: Loss = 1.964164\n",
            "Epoch 11/100, Batch 110, Step 1760: Loss = 1.892276\n",
            "Epoch 11/100, Batch 111, Step 1761: Loss = 1.667588\n",
            "Epoch 11/100, Batch 112, Step 1762: Loss = 1.871179\n",
            "Epoch 11/100, Batch 113, Step 1763: Loss = 1.754632\n",
            "Epoch 11/100, Batch 114, Step 1764: Loss = 1.655371\n",
            "Epoch 11/100, Batch 115, Step 1765: Loss = 1.723976\n",
            "Epoch 11/100, Batch 116, Step 1766: Loss = 1.639229\n",
            "Epoch 11/100, Batch 117, Step 1767: Loss = 1.631266\n",
            "Epoch 11/100, Batch 118, Step 1768: Loss = 1.668841\n",
            "Epoch 11/100, Batch 119, Step 1769: Loss = 1.693837\n",
            "Epoch 11/100, Batch 120, Step 1770: Loss = 1.707096\n",
            "Epoch 11/100, Batch 121, Step 1771: Loss = 1.590084\n",
            "Epoch 11/100, Batch 122, Step 1772: Loss = 1.777783\n",
            "Epoch 11/100, Batch 123, Step 1773: Loss = 1.519718\n",
            "Epoch 11/100, Batch 124, Step 1774: Loss = 1.539718\n",
            "Epoch 11/100, Batch 125, Step 1775: Loss = 1.657173\n",
            "Epoch 11/100, Batch 126, Step 1776: Loss = 1.520208\n",
            "Epoch 11/100, Batch 127, Step 1777: Loss = 1.719549\n",
            "Epoch 11/100, Batch 128, Step 1778: Loss = 1.721871\n",
            "Epoch 11/100, Batch 129, Step 1779: Loss = 1.685764\n",
            "Epoch 11/100, Batch 130, Step 1780: Loss = 1.530471\n",
            "Epoch 11/100, Batch 131, Step 1781: Loss = 1.376269\n",
            "Epoch 11/100, Batch 132, Step 1782: Loss = 1.562160\n",
            "Epoch 11/100, Batch 133, Step 1783: Loss = 1.451625\n",
            "Epoch 11/100, Batch 134, Step 1784: Loss = 1.638749\n",
            "Epoch 11/100, Batch 135, Step 1785: Loss = 1.472423\n",
            "Epoch 11/100, Batch 136, Step 1786: Loss = 1.487230\n",
            "Epoch 11/100, Batch 137, Step 1787: Loss = 1.634148\n",
            "Epoch 11/100, Batch 138, Step 1788: Loss = 1.532864\n",
            "Epoch 11/100, Batch 139, Step 1789: Loss = 1.367485\n",
            "Epoch 11/100, Batch 140, Step 1790: Loss = 1.645781\n",
            "Epoch 11/100, Batch 141, Step 1791: Loss = 1.412858\n",
            "Epoch 11/100, Batch 142, Step 1792: Loss = 1.409086\n",
            "Epoch 11/100, Batch 143, Step 1793: Loss = 1.436000\n",
            "Epoch 11/100, Batch 144, Step 1794: Loss = 1.594532\n",
            "Epoch 11/100, Batch 145, Step 1795: Loss = 1.376641\n",
            "Epoch 11/100, Batch 146, Step 1796: Loss = 1.558833\n",
            "Epoch 11/100, Batch 147, Step 1797: Loss = 1.503330\n",
            "Epoch 11/100, Batch 148, Step 1798: Loss = 1.545034\n",
            "Epoch 11/100, Batch 149, Step 1799: Loss = 1.471718\n",
            "Epoch 11/100, Batch 150, Step 1800: Loss = 1.463537\n",
            "Epoch 11/100, Batch 151, Step 1801: Loss = 1.548860\n",
            "Epoch 11/100, Batch 152, Step 1802: Loss = 1.350104\n",
            "Epoch 11/100, Batch 153, Step 1803: Loss = 1.690649\n",
            "Epoch 11/100, Batch 154, Step 1804: Loss = 1.414589\n",
            "Epoch 11/100, Batch 155, Step 1805: Loss = 1.646119\n",
            "Epoch 11/100, Batch 156, Step 1806: Loss = 1.666417\n",
            "Epoch 11/100, Batch 157, Step 1807: Loss = 1.618682\n",
            "Epoch 11/100, Batch 158, Step 1808: Loss = 1.248594\n",
            "Epoch 11/100, Batch 159, Step 1809: Loss = 1.562394\n",
            "Epoch 11/100, Batch 160, Step 1810: Loss = 1.371834\n",
            "Epoch 11/100, Batch 161, Step 1811: Loss = 1.596877\n",
            "Epoch 11/100, Batch 162, Step 1812: Loss = 1.429413\n",
            "Epoch 11/100, Batch 163, Step 1813: Loss = 1.386198\n",
            "Epoch 11/100, Batch 164, Step 1814: Loss = 1.188423\n",
            "Epoch 11/100, Batch 165, Step 1815: Loss = 1.421050\n",
            " Epoch 11/100, Avg Loss: 1.934718\n",
            "Checkpoint saved..\n",
            "Epoch 12/100, Batch 1, Step 1816: Loss = 1.206992\n",
            "Epoch 12/100, Batch 2, Step 1817: Loss = 1.267694\n",
            "Epoch 12/100, Batch 3, Step 1818: Loss = 1.348556\n",
            "Epoch 12/100, Batch 4, Step 1819: Loss = 1.338498\n",
            "Epoch 12/100, Batch 5, Step 1820: Loss = 1.318581\n",
            "Epoch 12/100, Batch 6, Step 1821: Loss = 1.152179\n",
            "Epoch 12/100, Batch 7, Step 1822: Loss = 1.171684\n",
            "Epoch 12/100, Batch 8, Step 1823: Loss = 1.150881\n",
            "Epoch 12/100, Batch 9, Step 1824: Loss = 1.117075\n",
            "Epoch 12/100, Batch 10, Step 1825: Loss = 1.214329\n",
            "Epoch 12/100, Batch 11, Step 1826: Loss = 1.278105\n",
            "Epoch 12/100, Batch 12, Step 1827: Loss = 1.253391\n",
            "Epoch 12/100, Batch 13, Step 1828: Loss = 1.117742\n",
            "Epoch 12/100, Batch 14, Step 1829: Loss = 1.231302\n",
            "Epoch 12/100, Batch 15, Step 1830: Loss = 1.270538\n",
            "Epoch 12/100, Batch 16, Step 1831: Loss = 1.321506\n",
            "Epoch 12/100, Batch 17, Step 1832: Loss = 1.252814\n",
            "Epoch 12/100, Batch 18, Step 1833: Loss = 0.924182\n",
            "Epoch 12/100, Batch 19, Step 1834: Loss = 1.336689\n",
            "Epoch 12/100, Batch 20, Step 1835: Loss = 1.145127\n",
            "Epoch 12/100, Batch 21, Step 1836: Loss = 1.066143\n",
            "Epoch 12/100, Batch 22, Step 1837: Loss = 1.144273\n",
            "Epoch 12/100, Batch 23, Step 1838: Loss = 1.190639\n",
            "Epoch 12/100, Batch 24, Step 1839: Loss = 1.143240\n",
            "Epoch 12/100, Batch 25, Step 1840: Loss = 1.216317\n",
            "Epoch 12/100, Batch 26, Step 1841: Loss = 1.232019\n",
            "Epoch 12/100, Batch 27, Step 1842: Loss = 1.090087\n",
            "Epoch 12/100, Batch 28, Step 1843: Loss = 1.180318\n",
            "Epoch 12/100, Batch 29, Step 1844: Loss = 1.161084\n",
            "Epoch 12/100, Batch 30, Step 1845: Loss = 1.029959\n",
            "Epoch 12/100, Batch 31, Step 1846: Loss = 1.167446\n",
            "Epoch 12/100, Batch 32, Step 1847: Loss = 1.172758\n",
            "Epoch 12/100, Batch 33, Step 1848: Loss = 1.170959\n",
            "Epoch 12/100, Batch 34, Step 1849: Loss = 1.090076\n",
            "Epoch 12/100, Batch 35, Step 1850: Loss = 1.117854\n",
            "Epoch 12/100, Batch 36, Step 1851: Loss = 1.297120\n",
            "Epoch 12/100, Batch 37, Step 1852: Loss = 1.233948\n",
            "Epoch 12/100, Batch 38, Step 1853: Loss = 0.940144\n",
            "Epoch 12/100, Batch 39, Step 1854: Loss = 1.071440\n",
            "Epoch 12/100, Batch 40, Step 1855: Loss = 1.216386\n",
            "Epoch 12/100, Batch 41, Step 1856: Loss = 1.173915\n",
            "Epoch 12/100, Batch 42, Step 1857: Loss = 1.178702\n",
            "Epoch 12/100, Batch 43, Step 1858: Loss = 0.976965\n",
            "Epoch 12/100, Batch 44, Step 1859: Loss = 1.314765\n",
            "Epoch 12/100, Batch 45, Step 1860: Loss = 1.007763\n",
            "Epoch 12/100, Batch 46, Step 1861: Loss = 1.117723\n",
            "Epoch 12/100, Batch 47, Step 1862: Loss = 0.771608\n",
            "Epoch 12/100, Batch 48, Step 1863: Loss = 1.314190\n",
            "Epoch 12/100, Batch 49, Step 1864: Loss = 1.118395\n",
            "Epoch 12/100, Batch 50, Step 1865: Loss = 0.985561\n",
            "Epoch 12/100, Batch 51, Step 1866: Loss = 1.110613\n",
            "Epoch 12/100, Batch 52, Step 1867: Loss = 1.103514\n",
            "Epoch 12/100, Batch 53, Step 1868: Loss = 1.271000\n",
            "Epoch 12/100, Batch 54, Step 1869: Loss = 1.092707\n",
            "Epoch 12/100, Batch 55, Step 1870: Loss = 0.957378\n",
            "Epoch 12/100, Batch 56, Step 1871: Loss = 1.175162\n",
            "Epoch 12/100, Batch 57, Step 1872: Loss = 1.096788\n",
            "Epoch 12/100, Batch 58, Step 1873: Loss = 1.078299\n",
            "Epoch 12/100, Batch 59, Step 1874: Loss = 0.966721\n",
            "Epoch 12/100, Batch 60, Step 1875: Loss = 1.108614\n",
            "Epoch 12/100, Batch 61, Step 1876: Loss = 1.176671\n",
            "Epoch 12/100, Batch 62, Step 1877: Loss = 1.091541\n",
            "Epoch 12/100, Batch 63, Step 1878: Loss = 1.408104\n",
            "Epoch 12/100, Batch 64, Step 1879: Loss = 1.081524\n",
            "Epoch 12/100, Batch 65, Step 1880: Loss = 1.133183\n",
            "Epoch 12/100, Batch 66, Step 1881: Loss = 1.139339\n",
            "Epoch 12/100, Batch 67, Step 1882: Loss = 1.035114\n",
            "Epoch 12/100, Batch 68, Step 1883: Loss = 1.189945\n",
            "Epoch 12/100, Batch 69, Step 1884: Loss = 0.933611\n",
            "Epoch 12/100, Batch 70, Step 1885: Loss = 1.110482\n",
            "Epoch 12/100, Batch 71, Step 1886: Loss = 1.075709\n",
            "Epoch 12/100, Batch 72, Step 1887: Loss = 0.956980\n",
            "Epoch 12/100, Batch 73, Step 1888: Loss = 0.950171\n",
            "Epoch 12/100, Batch 74, Step 1889: Loss = 1.057401\n",
            "Epoch 12/100, Batch 75, Step 1890: Loss = 1.075031\n",
            "Epoch 12/100, Batch 76, Step 1891: Loss = 1.101923\n",
            "Epoch 12/100, Batch 77, Step 1892: Loss = 1.002254\n",
            "Epoch 12/100, Batch 78, Step 1893: Loss = 0.904887\n",
            "Epoch 12/100, Batch 79, Step 1894: Loss = 1.066784\n",
            "Epoch 12/100, Batch 80, Step 1895: Loss = 1.220737\n",
            "Epoch 12/100, Batch 81, Step 1896: Loss = 0.966285\n",
            "Epoch 12/100, Batch 82, Step 1897: Loss = 1.013733\n",
            "Epoch 12/100, Batch 83, Step 1898: Loss = 1.153108\n",
            "Epoch 12/100, Batch 84, Step 1899: Loss = 1.066104\n",
            "Epoch 12/100, Batch 85, Step 1900: Loss = 0.794947\n",
            "Epoch 12/100, Batch 86, Step 1901: Loss = 0.911596\n",
            "Epoch 12/100, Batch 87, Step 1902: Loss = 0.954256\n",
            "Epoch 12/100, Batch 88, Step 1903: Loss = 1.051545\n",
            "Epoch 12/100, Batch 89, Step 1904: Loss = 0.845121\n",
            "Epoch 12/100, Batch 90, Step 1905: Loss = 0.875203\n",
            "Epoch 12/100, Batch 91, Step 1906: Loss = 0.885433\n",
            "Epoch 12/100, Batch 92, Step 1907: Loss = 1.012536\n",
            "Epoch 12/100, Batch 93, Step 1908: Loss = 0.936250\n",
            "Epoch 12/100, Batch 94, Step 1909: Loss = 1.082011\n",
            "Epoch 12/100, Batch 95, Step 1910: Loss = 1.034542\n",
            "Epoch 12/100, Batch 96, Step 1911: Loss = 0.816527\n",
            "Epoch 12/100, Batch 97, Step 1912: Loss = 0.761384\n",
            "Epoch 12/100, Batch 98, Step 1913: Loss = 0.990617\n",
            "Epoch 12/100, Batch 99, Step 1914: Loss = 0.993030\n",
            "Epoch 12/100, Batch 100, Step 1915: Loss = 0.779177\n",
            "Epoch 12/100, Batch 101, Step 1916: Loss = 0.862066\n",
            "Epoch 12/100, Batch 102, Step 1917: Loss = 0.849902\n",
            "Epoch 12/100, Batch 103, Step 1918: Loss = 1.103777\n",
            "Epoch 12/100, Batch 104, Step 1919: Loss = 0.953435\n",
            "Epoch 12/100, Batch 105, Step 1920: Loss = 0.883821\n",
            "Epoch 12/100, Batch 106, Step 1921: Loss = 0.887430\n",
            "Epoch 12/100, Batch 107, Step 1922: Loss = 0.985097\n",
            "Epoch 12/100, Batch 108, Step 1923: Loss = 0.895117\n",
            "Epoch 12/100, Batch 109, Step 1924: Loss = 0.993511\n",
            "Epoch 12/100, Batch 110, Step 1925: Loss = 0.896298\n",
            "Epoch 12/100, Batch 111, Step 1926: Loss = 0.883463\n",
            "Epoch 12/100, Batch 112, Step 1927: Loss = 0.969173\n",
            "Epoch 12/100, Batch 113, Step 1928: Loss = 0.949893\n",
            "Epoch 12/100, Batch 114, Step 1929: Loss = 0.903400\n",
            "Epoch 12/100, Batch 115, Step 1930: Loss = 0.994874\n",
            "Epoch 12/100, Batch 116, Step 1931: Loss = 0.981958\n",
            "Epoch 12/100, Batch 117, Step 1932: Loss = 0.901574\n",
            "Epoch 12/100, Batch 118, Step 1933: Loss = 1.014499\n",
            "Epoch 12/100, Batch 119, Step 1934: Loss = 0.876528\n",
            "Epoch 12/100, Batch 120, Step 1935: Loss = 0.998763\n",
            "Epoch 12/100, Batch 121, Step 1936: Loss = 0.881090\n",
            "Epoch 12/100, Batch 122, Step 1937: Loss = 0.912317\n",
            "Epoch 12/100, Batch 123, Step 1938: Loss = 1.054990\n",
            "Epoch 12/100, Batch 124, Step 1939: Loss = 0.923707\n",
            "Epoch 12/100, Batch 125, Step 1940: Loss = 0.764732\n",
            "Epoch 12/100, Batch 126, Step 1941: Loss = 0.688708\n",
            "Epoch 12/100, Batch 127, Step 1942: Loss = 1.113435\n",
            "Epoch 12/100, Batch 128, Step 1943: Loss = 0.883517\n",
            "Epoch 12/100, Batch 129, Step 1944: Loss = 0.720403\n",
            "Epoch 12/100, Batch 130, Step 1945: Loss = 0.730869\n",
            "Epoch 12/100, Batch 131, Step 1946: Loss = 0.803771\n",
            "Epoch 12/100, Batch 132, Step 1947: Loss = 0.799108\n",
            "Epoch 12/100, Batch 133, Step 1948: Loss = 0.903064\n",
            "Epoch 12/100, Batch 134, Step 1949: Loss = 0.822544\n",
            "Epoch 12/100, Batch 135, Step 1950: Loss = 0.858301\n",
            "Epoch 12/100, Batch 136, Step 1951: Loss = 0.802304\n",
            "Epoch 12/100, Batch 137, Step 1952: Loss = 0.711040\n",
            "Epoch 12/100, Batch 138, Step 1953: Loss = 0.875783\n",
            "Epoch 12/100, Batch 139, Step 1954: Loss = 0.825010\n",
            "Epoch 12/100, Batch 140, Step 1955: Loss = 0.947284\n",
            "Epoch 12/100, Batch 141, Step 1956: Loss = 0.795249\n",
            "Epoch 12/100, Batch 142, Step 1957: Loss = 0.907865\n",
            "Epoch 12/100, Batch 143, Step 1958: Loss = 0.930822\n",
            "Epoch 12/100, Batch 144, Step 1959: Loss = 1.070296\n",
            "Epoch 12/100, Batch 145, Step 1960: Loss = 0.869612\n",
            "Epoch 12/100, Batch 146, Step 1961: Loss = 0.754263\n",
            "Epoch 12/100, Batch 147, Step 1962: Loss = 0.977517\n",
            "Epoch 12/100, Batch 148, Step 1963: Loss = 0.874263\n",
            "Epoch 12/100, Batch 149, Step 1964: Loss = 0.777303\n",
            "Epoch 12/100, Batch 150, Step 1965: Loss = 0.914263\n",
            "Epoch 12/100, Batch 151, Step 1966: Loss = 0.686234\n",
            "Epoch 12/100, Batch 152, Step 1967: Loss = 0.691705\n",
            "Epoch 12/100, Batch 153, Step 1968: Loss = 0.727422\n",
            "Epoch 12/100, Batch 154, Step 1969: Loss = 0.784322\n",
            "Epoch 12/100, Batch 155, Step 1970: Loss = 0.790064\n",
            "Epoch 12/100, Batch 156, Step 1971: Loss = 0.859789\n",
            "Epoch 12/100, Batch 157, Step 1972: Loss = 0.643319\n",
            "Epoch 12/100, Batch 158, Step 1973: Loss = 0.801686\n",
            "Epoch 12/100, Batch 159, Step 1974: Loss = 0.766448\n",
            "Epoch 12/100, Batch 160, Step 1975: Loss = 0.863495\n",
            "Epoch 12/100, Batch 161, Step 1976: Loss = 0.846603\n",
            "Epoch 12/100, Batch 162, Step 1977: Loss = 0.857697\n",
            "Epoch 12/100, Batch 163, Step 1978: Loss = 0.761121\n",
            "Epoch 12/100, Batch 164, Step 1979: Loss = 0.720397\n",
            "Epoch 12/100, Batch 165, Step 1980: Loss = 0.803722\n",
            " Epoch 12/100, Avg Loss: 1.007196\n",
            "Checkpoint saved..\n",
            "Epoch 13/100, Batch 1, Step 1981: Loss = 0.663823\n",
            "Epoch 13/100, Batch 2, Step 1982: Loss = 0.579647\n",
            "Epoch 13/100, Batch 3, Step 1983: Loss = 0.819382\n",
            "Epoch 13/100, Batch 4, Step 1984: Loss = 0.690484\n",
            "Epoch 13/100, Batch 5, Step 1985: Loss = 0.603200\n",
            "Epoch 13/100, Batch 6, Step 1986: Loss = 0.722996\n",
            "Epoch 13/100, Batch 7, Step 1987: Loss = 0.535683\n",
            "Epoch 13/100, Batch 8, Step 1988: Loss = 0.677967\n",
            "Epoch 13/100, Batch 9, Step 1989: Loss = 0.699039\n",
            "Epoch 13/100, Batch 10, Step 1990: Loss = 0.559430\n",
            "Epoch 13/100, Batch 11, Step 1991: Loss = 0.677888\n",
            "Epoch 13/100, Batch 12, Step 1992: Loss = 0.613188\n",
            "Epoch 13/100, Batch 13, Step 1993: Loss = 0.703009\n",
            "Epoch 13/100, Batch 14, Step 1994: Loss = 0.689858\n",
            "Epoch 13/100, Batch 15, Step 1995: Loss = 0.732546\n",
            "Epoch 13/100, Batch 16, Step 1996: Loss = 0.764698\n",
            "Epoch 13/100, Batch 17, Step 1997: Loss = 0.787357\n",
            "Epoch 13/100, Batch 18, Step 1998: Loss = 0.625345\n",
            "Epoch 13/100, Batch 19, Step 1999: Loss = 0.632612\n",
            "Epoch 13/100, Batch 20, Step 2000: Loss = 0.597654\n",
            "Epoch 13/100, Batch 21, Step 2001: Loss = 0.625879\n",
            "Epoch 13/100, Batch 22, Step 2002: Loss = 0.696624\n",
            "Epoch 13/100, Batch 23, Step 2003: Loss = 0.665963\n",
            "Epoch 13/100, Batch 24, Step 2004: Loss = 0.646442\n",
            "Epoch 13/100, Batch 25, Step 2005: Loss = 0.654959\n",
            "Epoch 13/100, Batch 26, Step 2006: Loss = 0.680060\n",
            "Epoch 13/100, Batch 27, Step 2007: Loss = 0.669834\n",
            "Epoch 13/100, Batch 28, Step 2008: Loss = 0.639372\n",
            "Epoch 13/100, Batch 29, Step 2009: Loss = 0.601391\n",
            "Epoch 13/100, Batch 30, Step 2010: Loss = 0.661930\n",
            "Epoch 13/100, Batch 31, Step 2011: Loss = 0.675114\n",
            "Epoch 13/100, Batch 32, Step 2012: Loss = 0.526203\n",
            "Epoch 13/100, Batch 33, Step 2013: Loss = 0.577421\n",
            "Epoch 13/100, Batch 34, Step 2014: Loss = 0.578720\n",
            "Epoch 13/100, Batch 35, Step 2015: Loss = 0.606330\n",
            "Epoch 13/100, Batch 36, Step 2016: Loss = 0.633598\n",
            "Epoch 13/100, Batch 37, Step 2017: Loss = 0.596461\n",
            "Epoch 13/100, Batch 38, Step 2018: Loss = 0.666716\n",
            "Epoch 13/100, Batch 39, Step 2019: Loss = 0.567518\n",
            "Epoch 13/100, Batch 40, Step 2020: Loss = 0.577655\n",
            "Epoch 13/100, Batch 41, Step 2021: Loss = 0.699854\n",
            "Epoch 13/100, Batch 42, Step 2022: Loss = 0.529387\n",
            "Epoch 13/100, Batch 43, Step 2023: Loss = 0.759964\n",
            "Epoch 13/100, Batch 44, Step 2024: Loss = 0.690690\n",
            "Epoch 13/100, Batch 45, Step 2025: Loss = 0.575881\n",
            "Epoch 13/100, Batch 46, Step 2026: Loss = 0.580173\n",
            "Epoch 13/100, Batch 47, Step 2027: Loss = 0.750791\n",
            "Epoch 13/100, Batch 48, Step 2028: Loss = 0.775874\n",
            "Epoch 13/100, Batch 49, Step 2029: Loss = 0.598326\n",
            "Epoch 13/100, Batch 50, Step 2030: Loss = 0.546438\n",
            "Epoch 13/100, Batch 51, Step 2031: Loss = 0.699454\n",
            "Epoch 13/100, Batch 52, Step 2032: Loss = 0.594047\n",
            "Epoch 13/100, Batch 53, Step 2033: Loss = 0.623805\n",
            "Epoch 13/100, Batch 54, Step 2034: Loss = 0.595101\n",
            "Epoch 13/100, Batch 55, Step 2035: Loss = 0.583958\n",
            "Epoch 13/100, Batch 56, Step 2036: Loss = 0.634518\n",
            "Epoch 13/100, Batch 57, Step 2037: Loss = 0.533973\n",
            "Epoch 13/100, Batch 58, Step 2038: Loss = 0.708955\n",
            "Epoch 13/100, Batch 59, Step 2039: Loss = 0.500186\n",
            "Epoch 13/100, Batch 60, Step 2040: Loss = 0.525190\n",
            "Epoch 13/100, Batch 61, Step 2041: Loss = 0.611546\n",
            "Epoch 13/100, Batch 62, Step 2042: Loss = 0.485009\n",
            "Epoch 13/100, Batch 63, Step 2043: Loss = 0.608423\n",
            "Epoch 13/100, Batch 64, Step 2044: Loss = 0.578041\n",
            "Epoch 13/100, Batch 65, Step 2045: Loss = 0.585452\n",
            "Epoch 13/100, Batch 66, Step 2046: Loss = 0.598452\n",
            "Epoch 13/100, Batch 67, Step 2047: Loss = 0.532911\n",
            "Epoch 13/100, Batch 68, Step 2048: Loss = 0.762161\n",
            "Epoch 13/100, Batch 69, Step 2049: Loss = 0.618291\n",
            "Epoch 13/100, Batch 70, Step 2050: Loss = 0.608820\n",
            "Epoch 13/100, Batch 71, Step 2051: Loss = 0.628407\n",
            "Epoch 13/100, Batch 72, Step 2052: Loss = 0.548896\n",
            "Epoch 13/100, Batch 73, Step 2053: Loss = 0.562644\n",
            "Epoch 13/100, Batch 74, Step 2054: Loss = 0.613054\n",
            "Epoch 13/100, Batch 75, Step 2055: Loss = 0.566848\n",
            "Epoch 13/100, Batch 76, Step 2056: Loss = 0.456930\n",
            "Epoch 13/100, Batch 77, Step 2057: Loss = 0.595721\n",
            "Epoch 13/100, Batch 78, Step 2058: Loss = 0.551179\n",
            "Epoch 13/100, Batch 79, Step 2059: Loss = 0.498478\n",
            "Epoch 13/100, Batch 80, Step 2060: Loss = 0.514217\n",
            "Epoch 13/100, Batch 81, Step 2061: Loss = 0.559956\n",
            "Epoch 13/100, Batch 82, Step 2062: Loss = 0.551822\n",
            "Epoch 13/100, Batch 83, Step 2063: Loss = 0.579430\n",
            "Epoch 13/100, Batch 84, Step 2064: Loss = 0.708729\n",
            "Epoch 13/100, Batch 85, Step 2065: Loss = 0.651478\n",
            "Epoch 13/100, Batch 86, Step 2066: Loss = 0.586732\n",
            "Epoch 13/100, Batch 87, Step 2067: Loss = 0.441361\n",
            "Epoch 13/100, Batch 88, Step 2068: Loss = 0.612331\n",
            "Epoch 13/100, Batch 89, Step 2069: Loss = 0.457807\n",
            "Epoch 13/100, Batch 90, Step 2070: Loss = 0.597277\n",
            "Epoch 13/100, Batch 91, Step 2071: Loss = 0.548324\n",
            "Epoch 13/100, Batch 92, Step 2072: Loss = 0.645173\n",
            "Epoch 13/100, Batch 93, Step 2073: Loss = 0.553423\n",
            "Epoch 13/100, Batch 94, Step 2074: Loss = 0.522380\n",
            "Epoch 13/100, Batch 95, Step 2075: Loss = 0.520047\n",
            "Epoch 13/100, Batch 96, Step 2076: Loss = 0.620411\n",
            "Epoch 13/100, Batch 97, Step 2077: Loss = 0.504232\n",
            "Epoch 13/100, Batch 98, Step 2078: Loss = 0.494010\n",
            "Epoch 13/100, Batch 99, Step 2079: Loss = 0.532351\n",
            "Epoch 13/100, Batch 100, Step 2080: Loss = 0.523661\n",
            "Epoch 13/100, Batch 101, Step 2081: Loss = 0.646021\n",
            "Epoch 13/100, Batch 102, Step 2082: Loss = 0.529740\n",
            "Epoch 13/100, Batch 103, Step 2083: Loss = 0.452405\n",
            "Epoch 13/100, Batch 104, Step 2084: Loss = 0.577341\n",
            "Epoch 13/100, Batch 105, Step 2085: Loss = 0.378655\n",
            "Epoch 13/100, Batch 106, Step 2086: Loss = 0.617732\n",
            "Epoch 13/100, Batch 107, Step 2087: Loss = 0.568083\n",
            "Epoch 13/100, Batch 108, Step 2088: Loss = 0.540789\n",
            "Epoch 13/100, Batch 109, Step 2089: Loss = 0.524324\n",
            "Epoch 13/100, Batch 110, Step 2090: Loss = 0.514752\n",
            "Epoch 13/100, Batch 111, Step 2091: Loss = 0.544492\n",
            "Epoch 13/100, Batch 112, Step 2092: Loss = 0.591600\n",
            "Epoch 13/100, Batch 113, Step 2093: Loss = 0.396638\n",
            "Epoch 13/100, Batch 114, Step 2094: Loss = 0.444560\n",
            "Epoch 13/100, Batch 115, Step 2095: Loss = 0.387922\n",
            "Epoch 13/100, Batch 116, Step 2096: Loss = 0.587197\n",
            "Epoch 13/100, Batch 117, Step 2097: Loss = 0.452883\n",
            "Epoch 13/100, Batch 118, Step 2098: Loss = 0.428495\n",
            "Epoch 13/100, Batch 119, Step 2099: Loss = 0.533268\n",
            "Epoch 13/100, Batch 120, Step 2100: Loss = 0.497013\n",
            "Epoch 13/100, Batch 121, Step 2101: Loss = 0.603554\n",
            "Epoch 13/100, Batch 122, Step 2102: Loss = 0.366855\n",
            "Epoch 13/100, Batch 123, Step 2103: Loss = 0.620530\n",
            "Epoch 13/100, Batch 124, Step 2104: Loss = 0.418448\n",
            "Epoch 13/100, Batch 125, Step 2105: Loss = 0.431975\n",
            "Epoch 13/100, Batch 126, Step 2106: Loss = 0.474534\n",
            "Epoch 13/100, Batch 127, Step 2107: Loss = 0.461592\n",
            "Epoch 13/100, Batch 128, Step 2108: Loss = 0.425312\n",
            "Epoch 13/100, Batch 129, Step 2109: Loss = 0.527454\n",
            "Epoch 13/100, Batch 130, Step 2110: Loss = 0.490332\n",
            "Epoch 13/100, Batch 131, Step 2111: Loss = 0.519510\n",
            "Epoch 13/100, Batch 132, Step 2112: Loss = 0.455356\n",
            "Epoch 13/100, Batch 133, Step 2113: Loss = 0.411050\n",
            "Epoch 13/100, Batch 134, Step 2114: Loss = 0.452902\n",
            "Epoch 13/100, Batch 135, Step 2115: Loss = 0.506397\n",
            "Epoch 13/100, Batch 136, Step 2116: Loss = 0.603066\n",
            "Epoch 13/100, Batch 137, Step 2117: Loss = 0.628488\n",
            "Epoch 13/100, Batch 138, Step 2118: Loss = 0.482325\n",
            "Epoch 13/100, Batch 139, Step 2119: Loss = 0.525131\n",
            "Epoch 13/100, Batch 140, Step 2120: Loss = 0.657894\n",
            "Epoch 13/100, Batch 141, Step 2121: Loss = 0.527489\n",
            "Epoch 13/100, Batch 142, Step 2122: Loss = 0.510588\n",
            "Epoch 13/100, Batch 143, Step 2123: Loss = 0.519408\n",
            "Epoch 13/100, Batch 144, Step 2124: Loss = 0.440767\n",
            "Epoch 13/100, Batch 145, Step 2125: Loss = 0.382147\n",
            "Epoch 13/100, Batch 146, Step 2126: Loss = 0.525994\n",
            "Epoch 13/100, Batch 147, Step 2127: Loss = 0.481919\n",
            "Epoch 13/100, Batch 148, Step 2128: Loss = 0.462122\n",
            "Epoch 13/100, Batch 149, Step 2129: Loss = 0.529646\n",
            "Epoch 13/100, Batch 150, Step 2130: Loss = 0.475698\n",
            "Epoch 13/100, Batch 151, Step 2131: Loss = 0.482464\n",
            "Epoch 13/100, Batch 152, Step 2132: Loss = 0.476792\n",
            "Epoch 13/100, Batch 153, Step 2133: Loss = 0.359388\n",
            "Epoch 13/100, Batch 154, Step 2134: Loss = 0.471257\n",
            "Epoch 13/100, Batch 155, Step 2135: Loss = 0.518259\n",
            "Epoch 13/100, Batch 156, Step 2136: Loss = 0.550718\n",
            "Epoch 13/100, Batch 157, Step 2137: Loss = 0.442389\n",
            "Epoch 13/100, Batch 158, Step 2138: Loss = 0.447101\n",
            "Epoch 13/100, Batch 159, Step 2139: Loss = 0.501204\n",
            "Epoch 13/100, Batch 160, Step 2140: Loss = 0.463648\n",
            "Epoch 13/100, Batch 161, Step 2141: Loss = 0.646164\n",
            "Epoch 13/100, Batch 162, Step 2142: Loss = 0.417963\n",
            "Epoch 13/100, Batch 163, Step 2143: Loss = 0.425100\n",
            "Epoch 13/100, Batch 164, Step 2144: Loss = 0.482086\n",
            "Epoch 13/100, Batch 165, Step 2145: Loss = 0.479963\n",
            " Epoch 13/100, Avg Loss: 0.566448\n",
            "Checkpoint saved..\n",
            "Epoch 14/100, Batch 1, Step 2146: Loss = 0.538497\n",
            "Epoch 14/100, Batch 2, Step 2147: Loss = 0.308158\n",
            "Epoch 14/100, Batch 3, Step 2148: Loss = 0.393682\n",
            "Epoch 14/100, Batch 4, Step 2149: Loss = 0.323057\n",
            "Epoch 14/100, Batch 5, Step 2150: Loss = 0.340183\n",
            "Epoch 14/100, Batch 6, Step 2151: Loss = 0.418870\n",
            "Epoch 14/100, Batch 7, Step 2152: Loss = 0.422402\n",
            "Epoch 14/100, Batch 8, Step 2153: Loss = 0.396515\n",
            "Epoch 14/100, Batch 9, Step 2154: Loss = 0.357359\n",
            "Epoch 14/100, Batch 10, Step 2155: Loss = 0.354366\n",
            "Epoch 14/100, Batch 11, Step 2156: Loss = 0.374671\n",
            "Epoch 14/100, Batch 12, Step 2157: Loss = 0.269048\n",
            "Epoch 14/100, Batch 13, Step 2158: Loss = 0.381896\n",
            "Epoch 14/100, Batch 14, Step 2159: Loss = 0.349144\n",
            "Epoch 14/100, Batch 15, Step 2160: Loss = 0.395937\n",
            "Epoch 14/100, Batch 16, Step 2161: Loss = 0.308832\n",
            "Epoch 14/100, Batch 17, Step 2162: Loss = 0.387441\n",
            "Epoch 14/100, Batch 18, Step 2163: Loss = 0.336386\n",
            "Epoch 14/100, Batch 19, Step 2164: Loss = 0.368861\n",
            "Epoch 14/100, Batch 20, Step 2165: Loss = 0.390866\n",
            "Epoch 14/100, Batch 21, Step 2166: Loss = 0.425976\n",
            "Epoch 14/100, Batch 22, Step 2167: Loss = 0.294933\n",
            "Epoch 14/100, Batch 23, Step 2168: Loss = 0.375870\n",
            "Epoch 14/100, Batch 24, Step 2169: Loss = 0.421595\n",
            "Epoch 14/100, Batch 25, Step 2170: Loss = 0.356682\n",
            "Epoch 14/100, Batch 26, Step 2171: Loss = 0.379002\n",
            "Epoch 14/100, Batch 27, Step 2172: Loss = 0.374752\n",
            "Epoch 14/100, Batch 28, Step 2173: Loss = 0.316994\n",
            "Epoch 14/100, Batch 29, Step 2174: Loss = 0.322106\n",
            "Epoch 14/100, Batch 30, Step 2175: Loss = 0.379704\n",
            "Epoch 14/100, Batch 31, Step 2176: Loss = 0.340946\n",
            "Epoch 14/100, Batch 32, Step 2177: Loss = 0.345756\n",
            "Epoch 14/100, Batch 33, Step 2178: Loss = 0.281460\n",
            "Epoch 14/100, Batch 34, Step 2179: Loss = 0.404542\n",
            "Epoch 14/100, Batch 35, Step 2180: Loss = 0.351756\n",
            "Epoch 14/100, Batch 36, Step 2181: Loss = 0.386412\n",
            "Epoch 14/100, Batch 37, Step 2182: Loss = 0.460333\n",
            "Epoch 14/100, Batch 38, Step 2183: Loss = 0.362444\n",
            "Epoch 14/100, Batch 39, Step 2184: Loss = 0.284809\n",
            "Epoch 14/100, Batch 40, Step 2185: Loss = 0.387864\n",
            "Epoch 14/100, Batch 41, Step 2186: Loss = 0.307723\n",
            "Epoch 14/100, Batch 42, Step 2187: Loss = 0.350483\n",
            "Epoch 14/100, Batch 43, Step 2188: Loss = 0.245068\n",
            "Epoch 14/100, Batch 44, Step 2189: Loss = 0.309544\n",
            "Epoch 14/100, Batch 45, Step 2190: Loss = 0.304949\n",
            "Epoch 14/100, Batch 46, Step 2191: Loss = 0.313773\n",
            "Epoch 14/100, Batch 47, Step 2192: Loss = 0.351580\n",
            "Epoch 14/100, Batch 48, Step 2193: Loss = 0.254302\n",
            "Epoch 14/100, Batch 49, Step 2194: Loss = 0.377933\n",
            "Epoch 14/100, Batch 50, Step 2195: Loss = 0.345529\n",
            "Epoch 14/100, Batch 51, Step 2196: Loss = 0.334382\n",
            "Epoch 14/100, Batch 52, Step 2197: Loss = 0.379382\n",
            "Epoch 14/100, Batch 53, Step 2198: Loss = 0.361359\n",
            "Epoch 14/100, Batch 54, Step 2199: Loss = 0.306220\n",
            "Epoch 14/100, Batch 55, Step 2200: Loss = 0.380347\n",
            "Epoch 14/100, Batch 56, Step 2201: Loss = 0.339187\n",
            "Epoch 14/100, Batch 57, Step 2202: Loss = 0.276757\n",
            "Epoch 14/100, Batch 58, Step 2203: Loss = 0.385808\n",
            "Epoch 14/100, Batch 59, Step 2204: Loss = 0.315672\n",
            "Epoch 14/100, Batch 60, Step 2205: Loss = 0.380314\n",
            "Epoch 14/100, Batch 61, Step 2206: Loss = 0.319480\n",
            "Epoch 14/100, Batch 62, Step 2207: Loss = 0.476440\n",
            "Epoch 14/100, Batch 63, Step 2208: Loss = 0.381003\n",
            "Epoch 14/100, Batch 64, Step 2209: Loss = 0.293501\n",
            "Epoch 14/100, Batch 65, Step 2210: Loss = 0.339019\n",
            "Epoch 14/100, Batch 66, Step 2211: Loss = 0.334739\n",
            "Epoch 14/100, Batch 67, Step 2212: Loss = 0.347029\n",
            "Epoch 14/100, Batch 68, Step 2213: Loss = 0.258153\n",
            "Epoch 14/100, Batch 69, Step 2214: Loss = 0.453290\n",
            "Epoch 14/100, Batch 70, Step 2215: Loss = 0.470386\n",
            "Epoch 14/100, Batch 71, Step 2216: Loss = 0.414597\n",
            "Epoch 14/100, Batch 72, Step 2217: Loss = 0.387508\n",
            "Epoch 14/100, Batch 73, Step 2218: Loss = 0.380178\n",
            "Epoch 14/100, Batch 74, Step 2219: Loss = 0.289735\n",
            "Epoch 14/100, Batch 75, Step 2220: Loss = 0.351583\n",
            "Epoch 14/100, Batch 76, Step 2221: Loss = 0.349179\n",
            "Epoch 14/100, Batch 77, Step 2222: Loss = 0.312064\n",
            "Epoch 14/100, Batch 78, Step 2223: Loss = 0.282118\n",
            "Epoch 14/100, Batch 79, Step 2224: Loss = 0.311267\n",
            "Epoch 14/100, Batch 80, Step 2225: Loss = 0.347130\n",
            "Epoch 14/100, Batch 81, Step 2226: Loss = 0.396284\n",
            "Epoch 14/100, Batch 82, Step 2227: Loss = 0.335555\n",
            "Epoch 14/100, Batch 83, Step 2228: Loss = 0.344282\n",
            "Epoch 14/100, Batch 84, Step 2229: Loss = 0.329335\n",
            "Epoch 14/100, Batch 85, Step 2230: Loss = 0.406056\n",
            "Epoch 14/100, Batch 86, Step 2231: Loss = 0.380482\n",
            "Epoch 14/100, Batch 87, Step 2232: Loss = 0.317463\n",
            "Epoch 14/100, Batch 88, Step 2233: Loss = 0.276434\n",
            "Epoch 14/100, Batch 89, Step 2234: Loss = 0.269777\n",
            "Epoch 14/100, Batch 90, Step 2235: Loss = 0.326389\n",
            "Epoch 14/100, Batch 91, Step 2236: Loss = 0.251884\n",
            "Epoch 14/100, Batch 92, Step 2237: Loss = 0.378784\n",
            "Epoch 14/100, Batch 93, Step 2238: Loss = 0.295443\n",
            "Epoch 14/100, Batch 94, Step 2239: Loss = 0.396231\n",
            "Epoch 14/100, Batch 95, Step 2240: Loss = 0.428226\n",
            "Epoch 14/100, Batch 96, Step 2241: Loss = 0.326067\n",
            "Epoch 14/100, Batch 97, Step 2242: Loss = 0.380830\n",
            "Epoch 14/100, Batch 98, Step 2243: Loss = 0.326414\n",
            "Epoch 14/100, Batch 99, Step 2244: Loss = 0.381666\n",
            "Epoch 14/100, Batch 100, Step 2245: Loss = 0.308503\n",
            "Epoch 14/100, Batch 101, Step 2246: Loss = 0.333719\n",
            "Epoch 14/100, Batch 102, Step 2247: Loss = 0.236232\n",
            "Epoch 14/100, Batch 103, Step 2248: Loss = 0.401358\n",
            "Epoch 14/100, Batch 104, Step 2249: Loss = 0.324862\n",
            "Epoch 14/100, Batch 105, Step 2250: Loss = 0.287907\n",
            "Epoch 14/100, Batch 106, Step 2251: Loss = 0.359905\n",
            "Epoch 14/100, Batch 107, Step 2252: Loss = 0.267415\n",
            "Epoch 14/100, Batch 108, Step 2253: Loss = 0.371019\n",
            "Epoch 14/100, Batch 109, Step 2254: Loss = 0.280353\n",
            "Epoch 14/100, Batch 110, Step 2255: Loss = 0.254992\n",
            "Epoch 14/100, Batch 111, Step 2256: Loss = 0.266142\n",
            "Epoch 14/100, Batch 112, Step 2257: Loss = 0.270095\n",
            "Epoch 14/100, Batch 113, Step 2258: Loss = 0.276730\n",
            "Epoch 14/100, Batch 114, Step 2259: Loss = 0.319059\n",
            "Epoch 14/100, Batch 115, Step 2260: Loss = 0.308450\n",
            "Epoch 14/100, Batch 116, Step 2261: Loss = 0.348790\n",
            "Epoch 14/100, Batch 117, Step 2262: Loss = 0.324571\n",
            "Epoch 14/100, Batch 118, Step 2263: Loss = 0.342530\n",
            "Epoch 14/100, Batch 119, Step 2264: Loss = 0.290991\n",
            "Epoch 14/100, Batch 120, Step 2265: Loss = 0.223531\n",
            "Epoch 14/100, Batch 121, Step 2266: Loss = 0.317819\n",
            "Epoch 14/100, Batch 122, Step 2267: Loss = 0.343439\n",
            "Epoch 14/100, Batch 123, Step 2268: Loss = 0.234151\n",
            "Epoch 14/100, Batch 124, Step 2269: Loss = 0.295033\n",
            "Epoch 14/100, Batch 125, Step 2270: Loss = 0.316676\n",
            "Epoch 14/100, Batch 126, Step 2271: Loss = 0.282700\n",
            "Epoch 14/100, Batch 127, Step 2272: Loss = 0.277690\n",
            "Epoch 14/100, Batch 128, Step 2273: Loss = 0.333573\n",
            "Epoch 14/100, Batch 129, Step 2274: Loss = 0.280833\n",
            "Epoch 14/100, Batch 130, Step 2275: Loss = 0.377370\n",
            "Epoch 14/100, Batch 131, Step 2276: Loss = 0.359388\n",
            "Epoch 14/100, Batch 132, Step 2277: Loss = 0.366338\n",
            "Epoch 14/100, Batch 133, Step 2278: Loss = 0.363943\n",
            "Epoch 14/100, Batch 134, Step 2279: Loss = 0.243465\n",
            "Epoch 14/100, Batch 135, Step 2280: Loss = 0.299143\n",
            "Epoch 14/100, Batch 136, Step 2281: Loss = 0.251197\n",
            "Epoch 14/100, Batch 137, Step 2282: Loss = 0.347305\n",
            "Epoch 14/100, Batch 138, Step 2283: Loss = 0.243892\n",
            "Epoch 14/100, Batch 139, Step 2284: Loss = 0.393106\n",
            "Epoch 14/100, Batch 140, Step 2285: Loss = 0.311446\n",
            "Epoch 14/100, Batch 141, Step 2286: Loss = 0.339983\n",
            "Epoch 14/100, Batch 142, Step 2287: Loss = 0.316603\n",
            "Epoch 14/100, Batch 143, Step 2288: Loss = 0.266980\n",
            "Epoch 14/100, Batch 144, Step 2289: Loss = 0.234080\n",
            "Epoch 14/100, Batch 145, Step 2290: Loss = 0.300860\n",
            "Epoch 14/100, Batch 146, Step 2291: Loss = 0.291045\n",
            "Epoch 14/100, Batch 147, Step 2292: Loss = 0.358115\n",
            "Epoch 14/100, Batch 148, Step 2293: Loss = 0.246309\n",
            "Epoch 14/100, Batch 149, Step 2294: Loss = 0.336690\n",
            "Epoch 14/100, Batch 150, Step 2295: Loss = 0.310551\n",
            "Epoch 14/100, Batch 151, Step 2296: Loss = 0.310778\n",
            "Epoch 14/100, Batch 152, Step 2297: Loss = 0.287351\n",
            "Epoch 14/100, Batch 153, Step 2298: Loss = 0.320348\n",
            "Epoch 14/100, Batch 154, Step 2299: Loss = 0.243811\n",
            "Epoch 14/100, Batch 155, Step 2300: Loss = 0.319126\n",
            "Epoch 14/100, Batch 156, Step 2301: Loss = 0.308409\n",
            "Epoch 14/100, Batch 157, Step 2302: Loss = 0.348558\n",
            "Epoch 14/100, Batch 158, Step 2303: Loss = 0.226545\n",
            "Epoch 14/100, Batch 159, Step 2304: Loss = 0.252848\n",
            "Epoch 14/100, Batch 160, Step 2305: Loss = 0.309612\n",
            "Epoch 14/100, Batch 161, Step 2306: Loss = 0.395762\n",
            "Epoch 14/100, Batch 162, Step 2307: Loss = 0.384005\n",
            "Epoch 14/100, Batch 163, Step 2308: Loss = 0.368399\n",
            "Epoch 14/100, Batch 164, Step 2309: Loss = 0.273212\n",
            "Epoch 14/100, Batch 165, Step 2310: Loss = 0.253589\n",
            " Epoch 14/100, Avg Loss: 0.334398\n",
            "Checkpoint saved..\n",
            "Epoch 15/100, Batch 1, Step 2311: Loss = 0.207457\n",
            "Epoch 15/100, Batch 2, Step 2312: Loss = 0.196084\n",
            "Epoch 15/100, Batch 3, Step 2313: Loss = 0.210788\n",
            "Epoch 15/100, Batch 4, Step 2314: Loss = 0.222163\n",
            "Epoch 15/100, Batch 5, Step 2315: Loss = 0.190051\n",
            "Epoch 15/100, Batch 6, Step 2316: Loss = 0.216725\n",
            "Epoch 15/100, Batch 7, Step 2317: Loss = 0.179765\n",
            "Epoch 15/100, Batch 8, Step 2318: Loss = 0.207999\n",
            "Epoch 15/100, Batch 9, Step 2319: Loss = 0.249812\n",
            "Epoch 15/100, Batch 10, Step 2320: Loss = 0.248170\n",
            "Epoch 15/100, Batch 11, Step 2321: Loss = 0.208345\n",
            "Epoch 15/100, Batch 12, Step 2322: Loss = 0.217776\n",
            "Epoch 15/100, Batch 13, Step 2323: Loss = 0.230006\n",
            "Epoch 15/100, Batch 14, Step 2324: Loss = 0.184872\n",
            "Epoch 15/100, Batch 15, Step 2325: Loss = 0.241854\n",
            "Epoch 15/100, Batch 16, Step 2326: Loss = 0.199777\n",
            "Epoch 15/100, Batch 17, Step 2327: Loss = 0.201428\n",
            "Epoch 15/100, Batch 18, Step 2328: Loss = 0.205797\n",
            "Epoch 15/100, Batch 19, Step 2329: Loss = 0.211981\n",
            "Epoch 15/100, Batch 20, Step 2330: Loss = 0.188789\n",
            "Epoch 15/100, Batch 21, Step 2331: Loss = 0.242740\n",
            "Epoch 15/100, Batch 22, Step 2332: Loss = 0.167232\n",
            "Epoch 15/100, Batch 23, Step 2333: Loss = 0.192221\n",
            "Epoch 15/100, Batch 24, Step 2334: Loss = 0.215501\n",
            "Epoch 15/100, Batch 25, Step 2335: Loss = 0.220694\n",
            "Epoch 15/100, Batch 26, Step 2336: Loss = 0.289570\n",
            "Epoch 15/100, Batch 27, Step 2337: Loss = 0.200915\n",
            "Epoch 15/100, Batch 28, Step 2338: Loss = 0.177295\n",
            "Epoch 15/100, Batch 29, Step 2339: Loss = 0.214096\n",
            "Epoch 15/100, Batch 30, Step 2340: Loss = 0.260286\n",
            "Epoch 15/100, Batch 31, Step 2341: Loss = 0.197310\n",
            "Epoch 15/100, Batch 32, Step 2342: Loss = 0.243119\n",
            "Epoch 15/100, Batch 33, Step 2343: Loss = 0.232768\n",
            "Epoch 15/100, Batch 34, Step 2344: Loss = 0.190897\n",
            "Epoch 15/100, Batch 35, Step 2345: Loss = 0.189703\n",
            "Epoch 15/100, Batch 36, Step 2346: Loss = 0.224362\n",
            "Epoch 15/100, Batch 37, Step 2347: Loss = 0.215414\n",
            "Epoch 15/100, Batch 38, Step 2348: Loss = 0.197059\n",
            "Epoch 15/100, Batch 39, Step 2349: Loss = 0.220086\n",
            "Epoch 15/100, Batch 40, Step 2350: Loss = 0.228769\n",
            "Epoch 15/100, Batch 41, Step 2351: Loss = 0.224305\n",
            "Epoch 15/100, Batch 42, Step 2352: Loss = 0.222315\n",
            "Epoch 15/100, Batch 43, Step 2353: Loss = 0.167890\n",
            "Epoch 15/100, Batch 44, Step 2354: Loss = 0.209160\n",
            "Epoch 15/100, Batch 45, Step 2355: Loss = 0.207487\n",
            "Epoch 15/100, Batch 46, Step 2356: Loss = 0.210181\n",
            "Epoch 15/100, Batch 47, Step 2357: Loss = 0.218047\n",
            "Epoch 15/100, Batch 48, Step 2358: Loss = 0.255346\n",
            "Epoch 15/100, Batch 49, Step 2359: Loss = 0.169227\n",
            "Epoch 15/100, Batch 50, Step 2360: Loss = 0.264427\n",
            "Epoch 15/100, Batch 51, Step 2361: Loss = 0.188537\n",
            "Epoch 15/100, Batch 52, Step 2362: Loss = 0.230215\n",
            "Epoch 15/100, Batch 53, Step 2363: Loss = 0.193241\n",
            "Epoch 15/100, Batch 54, Step 2364: Loss = 0.227319\n",
            "Epoch 15/100, Batch 55, Step 2365: Loss = 0.260555\n",
            "Epoch 15/100, Batch 56, Step 2366: Loss = 0.211618\n",
            "Epoch 15/100, Batch 57, Step 2367: Loss = 0.211217\n",
            "Epoch 15/100, Batch 58, Step 2368: Loss = 0.209518\n",
            "Epoch 15/100, Batch 59, Step 2369: Loss = 0.179542\n",
            "Epoch 15/100, Batch 60, Step 2370: Loss = 0.215529\n",
            "Epoch 15/100, Batch 61, Step 2371: Loss = 0.164636\n",
            "Epoch 15/100, Batch 62, Step 2372: Loss = 0.192752\n",
            "Epoch 15/100, Batch 63, Step 2373: Loss = 0.264308\n",
            "Epoch 15/100, Batch 64, Step 2374: Loss = 0.180255\n",
            "Epoch 15/100, Batch 65, Step 2375: Loss = 0.234055\n",
            "Epoch 15/100, Batch 66, Step 2376: Loss = 0.194167\n",
            "Epoch 15/100, Batch 67, Step 2377: Loss = 0.225352\n",
            "Epoch 15/100, Batch 68, Step 2378: Loss = 0.192931\n",
            "Epoch 15/100, Batch 69, Step 2379: Loss = 0.206320\n",
            "Epoch 15/100, Batch 70, Step 2380: Loss = 0.293122\n",
            "Epoch 15/100, Batch 71, Step 2381: Loss = 0.199796\n",
            "Epoch 15/100, Batch 72, Step 2382: Loss = 0.169593\n",
            "Epoch 15/100, Batch 73, Step 2383: Loss = 0.238032\n",
            "Epoch 15/100, Batch 74, Step 2384: Loss = 0.202587\n",
            "Epoch 15/100, Batch 75, Step 2385: Loss = 0.164625\n",
            "Epoch 15/100, Batch 76, Step 2386: Loss = 0.217265\n",
            "Epoch 15/100, Batch 77, Step 2387: Loss = 0.240449\n",
            "Epoch 15/100, Batch 78, Step 2388: Loss = 0.282630\n",
            "Epoch 15/100, Batch 79, Step 2389: Loss = 0.212120\n",
            "Epoch 15/100, Batch 80, Step 2390: Loss = 0.182789\n",
            "Epoch 15/100, Batch 81, Step 2391: Loss = 0.171155\n",
            "Epoch 15/100, Batch 82, Step 2392: Loss = 0.173263\n",
            "Epoch 15/100, Batch 83, Step 2393: Loss = 0.213702\n",
            "Epoch 15/100, Batch 84, Step 2394: Loss = 0.216364\n",
            "Epoch 15/100, Batch 85, Step 2395: Loss = 0.223959\n",
            "Epoch 15/100, Batch 86, Step 2396: Loss = 0.260804\n",
            "Epoch 15/100, Batch 87, Step 2397: Loss = 0.179756\n",
            "Epoch 15/100, Batch 88, Step 2398: Loss = 0.178968\n",
            "Epoch 15/100, Batch 89, Step 2399: Loss = 0.231300\n",
            "Epoch 15/100, Batch 90, Step 2400: Loss = 0.204800\n",
            "Epoch 15/100, Batch 91, Step 2401: Loss = 0.157699\n",
            "Epoch 15/100, Batch 92, Step 2402: Loss = 0.248880\n",
            "Epoch 15/100, Batch 93, Step 2403: Loss = 0.177039\n",
            "Epoch 15/100, Batch 94, Step 2404: Loss = 0.194801\n",
            "Epoch 15/100, Batch 95, Step 2405: Loss = 0.220768\n",
            "Epoch 15/100, Batch 96, Step 2406: Loss = 0.193189\n",
            "Epoch 15/100, Batch 97, Step 2407: Loss = 0.180342\n",
            "Epoch 15/100, Batch 98, Step 2408: Loss = 0.174454\n",
            "Epoch 15/100, Batch 99, Step 2409: Loss = 0.163216\n",
            "Epoch 15/100, Batch 100, Step 2410: Loss = 0.219991\n",
            "Epoch 15/100, Batch 101, Step 2411: Loss = 0.206322\n",
            "Epoch 15/100, Batch 102, Step 2412: Loss = 0.224251\n",
            "Epoch 15/100, Batch 103, Step 2413: Loss = 0.232797\n",
            "Epoch 15/100, Batch 104, Step 2414: Loss = 0.178723\n",
            "Epoch 15/100, Batch 105, Step 2415: Loss = 0.210261\n",
            "Epoch 15/100, Batch 106, Step 2416: Loss = 0.256309\n",
            "Epoch 15/100, Batch 107, Step 2417: Loss = 0.189099\n",
            "Epoch 15/100, Batch 108, Step 2418: Loss = 0.222838\n",
            "Epoch 15/100, Batch 109, Step 2419: Loss = 0.160729\n",
            "Epoch 15/100, Batch 110, Step 2420: Loss = 0.143685\n",
            "Epoch 15/100, Batch 111, Step 2421: Loss = 0.200473\n",
            "Epoch 15/100, Batch 112, Step 2422: Loss = 0.198841\n",
            "Epoch 15/100, Batch 113, Step 2423: Loss = 0.230900\n",
            "Epoch 15/100, Batch 114, Step 2424: Loss = 0.159305\n",
            "Epoch 15/100, Batch 115, Step 2425: Loss = 0.184007\n",
            "Epoch 15/100, Batch 116, Step 2426: Loss = 0.265299\n",
            "Epoch 15/100, Batch 117, Step 2427: Loss = 0.247774\n",
            "Epoch 15/100, Batch 118, Step 2428: Loss = 0.226026\n",
            "Epoch 15/100, Batch 119, Step 2429: Loss = 0.209320\n",
            "Epoch 15/100, Batch 120, Step 2430: Loss = 0.142430\n",
            "Epoch 15/100, Batch 121, Step 2431: Loss = 0.179973\n",
            "Epoch 15/100, Batch 122, Step 2432: Loss = 0.147791\n",
            "Epoch 15/100, Batch 123, Step 2433: Loss = 0.161774\n",
            "Epoch 15/100, Batch 124, Step 2434: Loss = 0.197454\n",
            "Epoch 15/100, Batch 125, Step 2435: Loss = 0.242673\n",
            "Epoch 15/100, Batch 126, Step 2436: Loss = 0.188705\n",
            "Epoch 15/100, Batch 127, Step 2437: Loss = 0.214361\n",
            "Epoch 15/100, Batch 128, Step 2438: Loss = 0.224539\n",
            "Epoch 15/100, Batch 129, Step 2439: Loss = 0.157236\n",
            "Epoch 15/100, Batch 130, Step 2440: Loss = 0.225512\n",
            "Epoch 15/100, Batch 131, Step 2441: Loss = 0.182339\n",
            "Epoch 15/100, Batch 132, Step 2442: Loss = 0.194294\n",
            "Epoch 15/100, Batch 133, Step 2443: Loss = 0.174814\n",
            "Epoch 15/100, Batch 134, Step 2444: Loss = 0.181186\n",
            "Epoch 15/100, Batch 135, Step 2445: Loss = 0.176369\n",
            "Epoch 15/100, Batch 136, Step 2446: Loss = 0.162533\n",
            "Epoch 15/100, Batch 137, Step 2447: Loss = 0.224035\n",
            "Epoch 15/100, Batch 138, Step 2448: Loss = 0.227408\n",
            "Epoch 15/100, Batch 139, Step 2449: Loss = 0.201221\n",
            "Epoch 15/100, Batch 140, Step 2450: Loss = 0.186795\n",
            "Epoch 15/100, Batch 141, Step 2451: Loss = 0.150514\n",
            "Epoch 15/100, Batch 142, Step 2452: Loss = 0.171857\n",
            "Epoch 15/100, Batch 143, Step 2453: Loss = 0.161804\n",
            "Epoch 15/100, Batch 144, Step 2454: Loss = 0.185229\n",
            "Epoch 15/100, Batch 145, Step 2455: Loss = 0.149948\n",
            "Epoch 15/100, Batch 146, Step 2456: Loss = 0.147658\n",
            "Epoch 15/100, Batch 147, Step 2457: Loss = 0.197649\n",
            "Epoch 15/100, Batch 148, Step 2458: Loss = 0.194560\n",
            "Epoch 15/100, Batch 149, Step 2459: Loss = 0.217204\n",
            "Epoch 15/100, Batch 150, Step 2460: Loss = 0.150155\n",
            "Epoch 15/100, Batch 151, Step 2461: Loss = 0.195897\n",
            "Epoch 15/100, Batch 152, Step 2462: Loss = 0.229427\n",
            "Epoch 15/100, Batch 153, Step 2463: Loss = 0.209939\n",
            "Epoch 15/100, Batch 154, Step 2464: Loss = 0.217761\n",
            "Epoch 15/100, Batch 155, Step 2465: Loss = 0.180474\n",
            "Epoch 15/100, Batch 156, Step 2466: Loss = 0.169036\n",
            "Epoch 15/100, Batch 157, Step 2467: Loss = 0.176557\n",
            "Epoch 15/100, Batch 158, Step 2468: Loss = 0.199973\n",
            "Epoch 15/100, Batch 159, Step 2469: Loss = 0.192981\n",
            "Epoch 15/100, Batch 160, Step 2470: Loss = 0.213299\n",
            "Epoch 15/100, Batch 161, Step 2471: Loss = 0.154199\n",
            "Epoch 15/100, Batch 162, Step 2472: Loss = 0.176956\n",
            "Epoch 15/100, Batch 163, Step 2473: Loss = 0.171272\n",
            "Epoch 15/100, Batch 164, Step 2474: Loss = 0.201770\n",
            "Epoch 15/100, Batch 165, Step 2475: Loss = 0.200458\n",
            " Epoch 15/100, Avg Loss: 0.203725\n",
            "Checkpoint saved..\n",
            "Epoch 16/100, Batch 1, Step 2476: Loss = 0.126866\n",
            "Epoch 16/100, Batch 2, Step 2477: Loss = 0.112042\n",
            "Epoch 16/100, Batch 3, Step 2478: Loss = 0.106600\n",
            "Epoch 16/100, Batch 4, Step 2479: Loss = 0.126654\n",
            "Epoch 16/100, Batch 5, Step 2480: Loss = 0.140335\n",
            "Epoch 16/100, Batch 6, Step 2481: Loss = 0.137409\n",
            "Epoch 16/100, Batch 7, Step 2482: Loss = 0.138569\n",
            "Epoch 16/100, Batch 8, Step 2483: Loss = 0.122319\n",
            "Epoch 16/100, Batch 9, Step 2484: Loss = 0.169614\n",
            "Epoch 16/100, Batch 10, Step 2485: Loss = 0.150573\n",
            "Epoch 16/100, Batch 11, Step 2486: Loss = 0.163075\n",
            "Epoch 16/100, Batch 12, Step 2487: Loss = 0.140342\n",
            "Epoch 16/100, Batch 13, Step 2488: Loss = 0.104621\n",
            "Epoch 16/100, Batch 14, Step 2489: Loss = 0.107773\n",
            "Epoch 16/100, Batch 15, Step 2490: Loss = 0.123906\n",
            "Epoch 16/100, Batch 16, Step 2491: Loss = 0.158173\n",
            "Epoch 16/100, Batch 17, Step 2492: Loss = 0.155985\n",
            "Epoch 16/100, Batch 18, Step 2493: Loss = 0.098892\n",
            "Epoch 16/100, Batch 19, Step 2494: Loss = 0.114882\n",
            "Epoch 16/100, Batch 20, Step 2495: Loss = 0.125747\n",
            "Epoch 16/100, Batch 21, Step 2496: Loss = 0.127393\n",
            "Epoch 16/100, Batch 22, Step 2497: Loss = 0.130957\n",
            "Epoch 16/100, Batch 23, Step 2498: Loss = 0.116873\n",
            "Epoch 16/100, Batch 24, Step 2499: Loss = 0.125463\n",
            "Epoch 16/100, Batch 25, Step 2500: Loss = 0.143221\n",
            "Epoch 16/100, Batch 26, Step 2501: Loss = 0.137243\n",
            "Epoch 16/100, Batch 27, Step 2502: Loss = 0.156956\n",
            "Epoch 16/100, Batch 28, Step 2503: Loss = 0.136117\n",
            "Epoch 16/100, Batch 29, Step 2504: Loss = 0.147668\n",
            "Epoch 16/100, Batch 30, Step 2505: Loss = 0.164076\n",
            "Epoch 16/100, Batch 31, Step 2506: Loss = 0.123242\n",
            "Epoch 16/100, Batch 32, Step 2507: Loss = 0.142509\n",
            "Epoch 16/100, Batch 33, Step 2508: Loss = 0.142117\n",
            "Epoch 16/100, Batch 34, Step 2509: Loss = 0.153952\n",
            "Epoch 16/100, Batch 35, Step 2510: Loss = 0.129419\n",
            "Epoch 16/100, Batch 36, Step 2511: Loss = 0.117011\n",
            "Epoch 16/100, Batch 37, Step 2512: Loss = 0.115452\n",
            "Epoch 16/100, Batch 38, Step 2513: Loss = 0.171500\n",
            "Epoch 16/100, Batch 39, Step 2514: Loss = 0.117592\n",
            "Epoch 16/100, Batch 40, Step 2515: Loss = 0.096162\n",
            "Epoch 16/100, Batch 41, Step 2516: Loss = 0.122125\n",
            "Epoch 16/100, Batch 42, Step 2517: Loss = 0.140457\n",
            "Epoch 16/100, Batch 43, Step 2518: Loss = 0.144538\n",
            "Epoch 16/100, Batch 44, Step 2519: Loss = 0.122391\n",
            "Epoch 16/100, Batch 45, Step 2520: Loss = 0.122846\n",
            "Epoch 16/100, Batch 46, Step 2521: Loss = 0.171925\n",
            "Epoch 16/100, Batch 47, Step 2522: Loss = 0.137024\n",
            "Epoch 16/100, Batch 48, Step 2523: Loss = 0.164026\n",
            "Epoch 16/100, Batch 49, Step 2524: Loss = 0.094531\n",
            "Epoch 16/100, Batch 50, Step 2525: Loss = 0.146892\n",
            "Epoch 16/100, Batch 51, Step 2526: Loss = 0.106202\n",
            "Epoch 16/100, Batch 52, Step 2527: Loss = 0.159835\n",
            "Epoch 16/100, Batch 53, Step 2528: Loss = 0.121956\n",
            "Epoch 16/100, Batch 54, Step 2529: Loss = 0.148827\n",
            "Epoch 16/100, Batch 55, Step 2530: Loss = 0.124278\n",
            "Epoch 16/100, Batch 56, Step 2531: Loss = 0.147763\n",
            "Epoch 16/100, Batch 57, Step 2532: Loss = 0.106223\n",
            "Epoch 16/100, Batch 58, Step 2533: Loss = 0.131031\n",
            "Epoch 16/100, Batch 59, Step 2534: Loss = 0.131545\n",
            "Epoch 16/100, Batch 60, Step 2535: Loss = 0.113444\n",
            "Epoch 16/100, Batch 61, Step 2536: Loss = 0.121565\n",
            "Epoch 16/100, Batch 62, Step 2537: Loss = 0.152410\n",
            "Epoch 16/100, Batch 63, Step 2538: Loss = 0.112672\n",
            "Epoch 16/100, Batch 64, Step 2539: Loss = 0.107861\n",
            "Epoch 16/100, Batch 65, Step 2540: Loss = 0.105620\n",
            "Epoch 16/100, Batch 66, Step 2541: Loss = 0.138133\n",
            "Epoch 16/100, Batch 67, Step 2542: Loss = 0.136282\n",
            "Epoch 16/100, Batch 68, Step 2543: Loss = 0.120318\n",
            "Epoch 16/100, Batch 69, Step 2544: Loss = 0.154885\n",
            "Epoch 16/100, Batch 70, Step 2545: Loss = 0.121662\n",
            "Epoch 16/100, Batch 71, Step 2546: Loss = 0.091568\n",
            "Epoch 16/100, Batch 72, Step 2547: Loss = 0.132857\n",
            "Epoch 16/100, Batch 73, Step 2548: Loss = 0.150033\n",
            "Epoch 16/100, Batch 74, Step 2549: Loss = 0.117105\n",
            "Epoch 16/100, Batch 75, Step 2550: Loss = 0.121720\n",
            "Epoch 16/100, Batch 76, Step 2551: Loss = 0.133076\n",
            "Epoch 16/100, Batch 77, Step 2552: Loss = 0.125348\n",
            "Epoch 16/100, Batch 78, Step 2553: Loss = 0.118255\n",
            "Epoch 16/100, Batch 79, Step 2554: Loss = 0.094396\n",
            "Epoch 16/100, Batch 80, Step 2555: Loss = 0.137609\n",
            "Epoch 16/100, Batch 81, Step 2556: Loss = 0.120886\n",
            "Epoch 16/100, Batch 82, Step 2557: Loss = 0.123628\n",
            "Epoch 16/100, Batch 83, Step 2558: Loss = 0.137052\n",
            "Epoch 16/100, Batch 84, Step 2559: Loss = 0.146223\n",
            "Epoch 16/100, Batch 85, Step 2560: Loss = 0.145733\n",
            "Epoch 16/100, Batch 86, Step 2561: Loss = 0.104700\n",
            "Epoch 16/100, Batch 87, Step 2562: Loss = 0.138675\n",
            "Epoch 16/100, Batch 88, Step 2563: Loss = 0.129834\n",
            "Epoch 16/100, Batch 89, Step 2564: Loss = 0.121039\n",
            "Epoch 16/100, Batch 90, Step 2565: Loss = 0.113638\n",
            "Epoch 16/100, Batch 91, Step 2566: Loss = 0.153939\n",
            "Epoch 16/100, Batch 92, Step 2567: Loss = 0.137046\n",
            "Epoch 16/100, Batch 93, Step 2568: Loss = 0.133656\n",
            "Epoch 16/100, Batch 94, Step 2569: Loss = 0.141144\n",
            "Epoch 16/100, Batch 95, Step 2570: Loss = 0.123659\n",
            "Epoch 16/100, Batch 96, Step 2571: Loss = 0.129396\n",
            "Epoch 16/100, Batch 97, Step 2572: Loss = 0.131313\n",
            "Epoch 16/100, Batch 98, Step 2573: Loss = 0.112491\n",
            "Epoch 16/100, Batch 99, Step 2574: Loss = 0.158585\n",
            "Epoch 16/100, Batch 100, Step 2575: Loss = 0.122409\n",
            "Epoch 16/100, Batch 101, Step 2576: Loss = 0.094952\n",
            "Epoch 16/100, Batch 102, Step 2577: Loss = 0.097084\n",
            "Epoch 16/100, Batch 103, Step 2578: Loss = 0.122777\n",
            "Epoch 16/100, Batch 104, Step 2579: Loss = 0.134494\n",
            "Epoch 16/100, Batch 105, Step 2580: Loss = 0.126177\n",
            "Epoch 16/100, Batch 106, Step 2581: Loss = 0.140212\n",
            "Epoch 16/100, Batch 107, Step 2582: Loss = 0.161496\n",
            "Epoch 16/100, Batch 108, Step 2583: Loss = 0.106829\n",
            "Epoch 16/100, Batch 109, Step 2584: Loss = 0.111678\n",
            "Epoch 16/100, Batch 110, Step 2585: Loss = 0.114064\n",
            "Epoch 16/100, Batch 111, Step 2586: Loss = 0.102910\n",
            "Epoch 16/100, Batch 112, Step 2587: Loss = 0.128386\n",
            "Epoch 16/100, Batch 113, Step 2588: Loss = 0.138337\n",
            "Epoch 16/100, Batch 114, Step 2589: Loss = 0.145413\n",
            "Epoch 16/100, Batch 115, Step 2590: Loss = 0.146998\n",
            "Epoch 16/100, Batch 116, Step 2591: Loss = 0.093928\n",
            "Epoch 16/100, Batch 117, Step 2592: Loss = 0.109882\n",
            "Epoch 16/100, Batch 118, Step 2593: Loss = 0.125522\n",
            "Epoch 16/100, Batch 119, Step 2594: Loss = 0.113194\n",
            "Epoch 16/100, Batch 120, Step 2595: Loss = 0.132585\n",
            "Epoch 16/100, Batch 121, Step 2596: Loss = 0.160333\n",
            "Epoch 16/100, Batch 122, Step 2597: Loss = 0.091147\n",
            "Epoch 16/100, Batch 123, Step 2598: Loss = 0.118094\n",
            "Epoch 16/100, Batch 124, Step 2599: Loss = 0.107725\n",
            "Epoch 16/100, Batch 125, Step 2600: Loss = 0.134406\n",
            "Epoch 16/100, Batch 126, Step 2601: Loss = 0.129052\n",
            "Epoch 16/100, Batch 127, Step 2602: Loss = 0.126808\n",
            "Epoch 16/100, Batch 128, Step 2603: Loss = 0.117869\n",
            "Epoch 16/100, Batch 129, Step 2604: Loss = 0.117515\n",
            "Epoch 16/100, Batch 130, Step 2605: Loss = 0.156289\n",
            "Epoch 16/100, Batch 131, Step 2606: Loss = 0.117429\n",
            "Epoch 16/100, Batch 132, Step 2607: Loss = 0.143846\n",
            "Epoch 16/100, Batch 133, Step 2608: Loss = 0.183841\n",
            "Epoch 16/100, Batch 134, Step 2609: Loss = 0.132746\n",
            "Epoch 16/100, Batch 135, Step 2610: Loss = 0.100467\n",
            "Epoch 16/100, Batch 136, Step 2611: Loss = 0.103300\n",
            "Epoch 16/100, Batch 137, Step 2612: Loss = 0.095461\n",
            "Epoch 16/100, Batch 138, Step 2613: Loss = 0.114986\n",
            "Epoch 16/100, Batch 139, Step 2614: Loss = 0.145767\n",
            "Epoch 16/100, Batch 140, Step 2615: Loss = 0.100505\n",
            "Epoch 16/100, Batch 141, Step 2616: Loss = 0.151284\n",
            "Epoch 16/100, Batch 142, Step 2617: Loss = 0.142047\n",
            "Epoch 16/100, Batch 143, Step 2618: Loss = 0.135961\n",
            "Epoch 16/100, Batch 144, Step 2619: Loss = 0.129707\n",
            "Epoch 16/100, Batch 145, Step 2620: Loss = 0.123251\n",
            "Epoch 16/100, Batch 146, Step 2621: Loss = 0.105466\n",
            "Epoch 16/100, Batch 147, Step 2622: Loss = 0.093661\n",
            "Epoch 16/100, Batch 148, Step 2623: Loss = 0.083857\n",
            "Epoch 16/100, Batch 149, Step 2624: Loss = 0.109224\n",
            "Epoch 16/100, Batch 150, Step 2625: Loss = 0.149925\n",
            "Epoch 16/100, Batch 151, Step 2626: Loss = 0.106738\n",
            "Epoch 16/100, Batch 152, Step 2627: Loss = 0.151195\n",
            "Epoch 16/100, Batch 153, Step 2628: Loss = 0.113573\n",
            "Epoch 16/100, Batch 154, Step 2629: Loss = 0.133098\n",
            "Epoch 16/100, Batch 155, Step 2630: Loss = 0.160856\n",
            "Epoch 16/100, Batch 156, Step 2631: Loss = 0.104719\n",
            "Epoch 16/100, Batch 157, Step 2632: Loss = 0.124913\n",
            "Epoch 16/100, Batch 158, Step 2633: Loss = 0.137897\n",
            "Epoch 16/100, Batch 159, Step 2634: Loss = 0.114293\n",
            "Epoch 16/100, Batch 160, Step 2635: Loss = 0.165153\n",
            "Epoch 16/100, Batch 161, Step 2636: Loss = 0.129285\n",
            "Epoch 16/100, Batch 162, Step 2637: Loss = 0.133236\n",
            "Epoch 16/100, Batch 163, Step 2638: Loss = 0.109432\n",
            "Epoch 16/100, Batch 164, Step 2639: Loss = 0.119517\n",
            "Epoch 16/100, Batch 165, Step 2640: Loss = 0.082925\n",
            " Epoch 16/100, Avg Loss: 0.128190\n",
            "Checkpoint saved..\n",
            "Epoch 17/100, Batch 1, Step 2641: Loss = 0.091791\n",
            "Epoch 17/100, Batch 2, Step 2642: Loss = 0.081715\n",
            "Epoch 17/100, Batch 3, Step 2643: Loss = 0.087462\n",
            "Epoch 17/100, Batch 4, Step 2644: Loss = 0.092875\n",
            "Epoch 17/100, Batch 5, Step 2645: Loss = 0.075297\n",
            "Epoch 17/100, Batch 6, Step 2646: Loss = 0.085479\n",
            "Epoch 17/100, Batch 7, Step 2647: Loss = 0.093221\n",
            "Epoch 17/100, Batch 8, Step 2648: Loss = 0.084518\n",
            "Epoch 17/100, Batch 9, Step 2649: Loss = 0.077335\n",
            "Epoch 17/100, Batch 10, Step 2650: Loss = 0.104824\n",
            "Epoch 17/100, Batch 11, Step 2651: Loss = 0.062824\n",
            "Epoch 17/100, Batch 12, Step 2652: Loss = 0.080705\n",
            "Epoch 17/100, Batch 13, Step 2653: Loss = 0.075412\n",
            "Epoch 17/100, Batch 14, Step 2654: Loss = 0.048366\n",
            "Epoch 17/100, Batch 15, Step 2655: Loss = 0.092059\n",
            "Epoch 17/100, Batch 16, Step 2656: Loss = 0.062060\n",
            "Epoch 17/100, Batch 17, Step 2657: Loss = 0.076576\n",
            "Epoch 17/100, Batch 18, Step 2658: Loss = 0.094142\n",
            "Epoch 17/100, Batch 19, Step 2659: Loss = 0.092164\n",
            "Epoch 17/100, Batch 20, Step 2660: Loss = 0.087405\n",
            "Epoch 17/100, Batch 21, Step 2661: Loss = 0.088799\n",
            "Epoch 17/100, Batch 22, Step 2662: Loss = 0.069056\n",
            "Epoch 17/100, Batch 23, Step 2663: Loss = 0.079868\n",
            "Epoch 17/100, Batch 24, Step 2664: Loss = 0.069970\n",
            "Epoch 17/100, Batch 25, Step 2665: Loss = 0.076881\n",
            "Epoch 17/100, Batch 26, Step 2666: Loss = 0.103298\n",
            "Epoch 17/100, Batch 27, Step 2667: Loss = 0.071273\n",
            "Epoch 17/100, Batch 28, Step 2668: Loss = 0.087619\n",
            "Epoch 17/100, Batch 29, Step 2669: Loss = 0.088246\n",
            "Epoch 17/100, Batch 30, Step 2670: Loss = 0.080310\n",
            "Epoch 17/100, Batch 31, Step 2671: Loss = 0.087444\n",
            "Epoch 17/100, Batch 32, Step 2672: Loss = 0.074184\n",
            "Epoch 17/100, Batch 33, Step 2673: Loss = 0.086924\n",
            "Epoch 17/100, Batch 34, Step 2674: Loss = 0.076936\n",
            "Epoch 17/100, Batch 35, Step 2675: Loss = 0.100997\n",
            "Epoch 17/100, Batch 36, Step 2676: Loss = 0.073911\n",
            "Epoch 17/100, Batch 37, Step 2677: Loss = 0.088213\n",
            "Epoch 17/100, Batch 38, Step 2678: Loss = 0.110002\n",
            "Epoch 17/100, Batch 39, Step 2679: Loss = 0.065700\n",
            "Epoch 17/100, Batch 40, Step 2680: Loss = 0.097141\n",
            "Epoch 17/100, Batch 41, Step 2681: Loss = 0.080465\n",
            "Epoch 17/100, Batch 42, Step 2682: Loss = 0.084796\n",
            "Epoch 17/100, Batch 43, Step 2683: Loss = 0.074214\n",
            "Epoch 17/100, Batch 44, Step 2684: Loss = 0.077599\n",
            "Epoch 17/100, Batch 45, Step 2685: Loss = 0.109089\n",
            "Epoch 17/100, Batch 46, Step 2686: Loss = 0.080664\n",
            "Epoch 17/100, Batch 47, Step 2687: Loss = 0.071622\n",
            "Epoch 17/100, Batch 48, Step 2688: Loss = 0.108498\n",
            "Epoch 17/100, Batch 49, Step 2689: Loss = 0.080595\n",
            "Epoch 17/100, Batch 50, Step 2690: Loss = 0.105688\n",
            "Epoch 17/100, Batch 51, Step 2691: Loss = 0.073384\n",
            "Epoch 17/100, Batch 52, Step 2692: Loss = 0.066533\n",
            "Epoch 17/100, Batch 53, Step 2693: Loss = 0.074988\n",
            "Epoch 17/100, Batch 54, Step 2694: Loss = 0.096184\n",
            "Epoch 17/100, Batch 55, Step 2695: Loss = 0.075546\n",
            "Epoch 17/100, Batch 56, Step 2696: Loss = 0.097895\n",
            "Epoch 17/100, Batch 57, Step 2697: Loss = 0.086144\n",
            "Epoch 17/100, Batch 58, Step 2698: Loss = 0.087666\n",
            "Epoch 17/100, Batch 59, Step 2699: Loss = 0.076859\n",
            "Epoch 17/100, Batch 60, Step 2700: Loss = 0.108392\n",
            "Epoch 17/100, Batch 61, Step 2701: Loss = 0.074821\n",
            "Epoch 17/100, Batch 62, Step 2702: Loss = 0.085052\n",
            "Epoch 17/100, Batch 63, Step 2703: Loss = 0.065842\n",
            "Epoch 17/100, Batch 64, Step 2704: Loss = 0.090008\n",
            "Epoch 17/100, Batch 65, Step 2705: Loss = 0.058395\n",
            "Epoch 17/100, Batch 66, Step 2706: Loss = 0.071455\n",
            "Epoch 17/100, Batch 67, Step 2707: Loss = 0.088220\n",
            "Epoch 17/100, Batch 68, Step 2708: Loss = 0.076411\n",
            "Epoch 17/100, Batch 69, Step 2709: Loss = 0.096954\n",
            "Epoch 17/100, Batch 70, Step 2710: Loss = 0.118693\n",
            "Epoch 17/100, Batch 71, Step 2711: Loss = 0.082075\n",
            "Epoch 17/100, Batch 72, Step 2712: Loss = 0.067012\n",
            "Epoch 17/100, Batch 73, Step 2713: Loss = 0.066595\n",
            "Epoch 17/100, Batch 74, Step 2714: Loss = 0.097628\n",
            "Epoch 17/100, Batch 75, Step 2715: Loss = 0.097265\n",
            "Epoch 17/100, Batch 76, Step 2716: Loss = 0.085000\n",
            "Epoch 17/100, Batch 77, Step 2717: Loss = 0.073346\n",
            "Epoch 17/100, Batch 78, Step 2718: Loss = 0.082041\n",
            "Epoch 17/100, Batch 79, Step 2719: Loss = 0.074879\n",
            "Epoch 17/100, Batch 80, Step 2720: Loss = 0.101532\n",
            "Epoch 17/100, Batch 81, Step 2721: Loss = 0.091056\n",
            "Epoch 17/100, Batch 82, Step 2722: Loss = 0.104887\n",
            "Epoch 17/100, Batch 83, Step 2723: Loss = 0.079931\n",
            "Epoch 17/100, Batch 84, Step 2724: Loss = 0.086903\n",
            "Epoch 17/100, Batch 85, Step 2725: Loss = 0.124308\n",
            "Epoch 17/100, Batch 86, Step 2726: Loss = 0.091030\n",
            "Epoch 17/100, Batch 87, Step 2727: Loss = 0.082311\n",
            "Epoch 17/100, Batch 88, Step 2728: Loss = 0.096918\n",
            "Epoch 17/100, Batch 89, Step 2729: Loss = 0.064151\n",
            "Epoch 17/100, Batch 90, Step 2730: Loss = 0.108157\n",
            "Epoch 17/100, Batch 91, Step 2731: Loss = 0.077637\n",
            "Epoch 17/100, Batch 92, Step 2732: Loss = 0.064223\n",
            "Epoch 17/100, Batch 93, Step 2733: Loss = 0.101538\n",
            "Epoch 17/100, Batch 94, Step 2734: Loss = 0.103348\n",
            "Epoch 17/100, Batch 95, Step 2735: Loss = 0.076097\n",
            "Epoch 17/100, Batch 96, Step 2736: Loss = 0.084536\n",
            "Epoch 17/100, Batch 97, Step 2737: Loss = 0.072995\n",
            "Epoch 17/100, Batch 98, Step 2738: Loss = 0.097870\n",
            "Epoch 17/100, Batch 99, Step 2739: Loss = 0.088807\n",
            "Epoch 17/100, Batch 100, Step 2740: Loss = 0.086316\n",
            "Epoch 17/100, Batch 101, Step 2741: Loss = 0.086440\n",
            "Epoch 17/100, Batch 102, Step 2742: Loss = 0.112030\n",
            "Epoch 17/100, Batch 103, Step 2743: Loss = 0.097274\n",
            "Epoch 17/100, Batch 104, Step 2744: Loss = 0.079872\n",
            "Epoch 17/100, Batch 105, Step 2745: Loss = 0.094477\n",
            "Epoch 17/100, Batch 106, Step 2746: Loss = 0.073067\n",
            "Epoch 17/100, Batch 107, Step 2747: Loss = 0.081521\n",
            "Epoch 17/100, Batch 108, Step 2748: Loss = 0.086625\n",
            "Epoch 17/100, Batch 109, Step 2749: Loss = 0.056709\n",
            "Epoch 17/100, Batch 110, Step 2750: Loss = 0.093415\n",
            "Epoch 17/100, Batch 111, Step 2751: Loss = 0.074737\n",
            "Epoch 17/100, Batch 112, Step 2752: Loss = 0.063097\n",
            "Epoch 17/100, Batch 113, Step 2753: Loss = 0.085559\n",
            "Epoch 17/100, Batch 114, Step 2754: Loss = 0.057387\n",
            "Epoch 17/100, Batch 115, Step 2755: Loss = 0.085460\n",
            "Epoch 17/100, Batch 116, Step 2756: Loss = 0.085286\n",
            "Epoch 17/100, Batch 117, Step 2757: Loss = 0.076604\n",
            "Epoch 17/100, Batch 118, Step 2758: Loss = 0.106442\n",
            "Epoch 17/100, Batch 119, Step 2759: Loss = 0.097876\n",
            "Epoch 17/100, Batch 120, Step 2760: Loss = 0.075003\n",
            "Epoch 17/100, Batch 121, Step 2761: Loss = 0.091143\n",
            "Epoch 17/100, Batch 122, Step 2762: Loss = 0.095056\n",
            "Epoch 17/100, Batch 123, Step 2763: Loss = 0.058186\n",
            "Epoch 17/100, Batch 124, Step 2764: Loss = 0.091515\n",
            "Epoch 17/100, Batch 125, Step 2765: Loss = 0.086572\n",
            "Epoch 17/100, Batch 126, Step 2766: Loss = 0.072393\n",
            "Epoch 17/100, Batch 127, Step 2767: Loss = 0.092764\n",
            "Epoch 17/100, Batch 128, Step 2768: Loss = 0.080875\n",
            "Epoch 17/100, Batch 129, Step 2769: Loss = 0.087227\n",
            "Epoch 17/100, Batch 130, Step 2770: Loss = 0.062820\n",
            "Epoch 17/100, Batch 131, Step 2771: Loss = 0.097385\n",
            "Epoch 17/100, Batch 132, Step 2772: Loss = 0.084778\n",
            "Epoch 17/100, Batch 133, Step 2773: Loss = 0.061321\n",
            "Epoch 17/100, Batch 134, Step 2774: Loss = 0.073173\n",
            "Epoch 17/100, Batch 135, Step 2775: Loss = 0.073981\n",
            "Epoch 17/100, Batch 136, Step 2776: Loss = 0.081298\n",
            "Epoch 17/100, Batch 137, Step 2777: Loss = 0.060572\n",
            "Epoch 17/100, Batch 138, Step 2778: Loss = 0.088213\n",
            "Epoch 17/100, Batch 139, Step 2779: Loss = 0.100465\n",
            "Epoch 17/100, Batch 140, Step 2780: Loss = 0.098877\n",
            "Epoch 17/100, Batch 141, Step 2781: Loss = 0.088313\n",
            "Epoch 17/100, Batch 142, Step 2782: Loss = 0.105236\n",
            "Epoch 17/100, Batch 143, Step 2783: Loss = 0.066854\n",
            "Epoch 17/100, Batch 144, Step 2784: Loss = 0.084024\n",
            "Epoch 17/100, Batch 145, Step 2785: Loss = 0.083820\n",
            "Epoch 17/100, Batch 146, Step 2786: Loss = 0.070654\n",
            "Epoch 17/100, Batch 147, Step 2787: Loss = 0.104209\n",
            "Epoch 17/100, Batch 148, Step 2788: Loss = 0.087477\n",
            "Epoch 17/100, Batch 149, Step 2789: Loss = 0.073742\n",
            "Epoch 17/100, Batch 150, Step 2790: Loss = 0.082352\n",
            "Epoch 17/100, Batch 151, Step 2791: Loss = 0.073396\n",
            "Epoch 17/100, Batch 152, Step 2792: Loss = 0.100960\n",
            "Epoch 17/100, Batch 153, Step 2793: Loss = 0.068230\n",
            "Epoch 17/100, Batch 154, Step 2794: Loss = 0.093271\n",
            "Epoch 17/100, Batch 155, Step 2795: Loss = 0.102762\n",
            "Epoch 17/100, Batch 156, Step 2796: Loss = 0.068988\n",
            "Epoch 17/100, Batch 157, Step 2797: Loss = 0.086154\n",
            "Epoch 17/100, Batch 158, Step 2798: Loss = 0.067422\n",
            "Epoch 17/100, Batch 159, Step 2799: Loss = 0.104092\n",
            "Epoch 17/100, Batch 160, Step 2800: Loss = 0.099087\n",
            "Epoch 17/100, Batch 161, Step 2801: Loss = 0.078227\n",
            "Epoch 17/100, Batch 162, Step 2802: Loss = 0.102288\n",
            "Epoch 17/100, Batch 163, Step 2803: Loss = 0.099492\n",
            "Epoch 17/100, Batch 164, Step 2804: Loss = 0.081635\n",
            "Epoch 17/100, Batch 165, Step 2805: Loss = 0.072278\n",
            " Epoch 17/100, Avg Loss: 0.084415\n",
            "Early stopping as loss is below target\n",
            "Model training complete. Upload to GitHub and Hugging Face Spaces.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    vocab_size: int = 50257\n",
        "    max_seq_len: int = 2048\n",
        "    dim: int = 768\n",
        "    num_layers: int = 8\n",
        "    num_heads: int = 12\n",
        "    dropout: float = 0.1\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_head = config.num_heads\n",
        "        self.n_embd = config.dim\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.c_attn = nn.Linear(config.dim, 3 * config.dim) # [n_embd, 3 * n_embd] # for q,k, v we are defined together and split later eg: instead of defining q,k,v separately 100*100,100*100,100*100 , we are defining as 100*300 and then split later.\n",
        "        self.c_proj = nn.Linear(config.dim, config.dim) # [n_embd, n_embd]\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # [B, T, n_embd] Batch - B, Token_len - T, embedding dim - C\n",
        "\n",
        "        # Linear projection and split into Q, K, V\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # [B, T, n_embd] each\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # [B, n_head, T, n_embd/n_head]\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # [B, n_head, T, n_embd/n_head]\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # [B, n_head, T, n_embd/n_head]\n",
        "\n",
        "        # Attention scores\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5)) # [B, n_head, T, T]\n",
        "        att = F.softmax(att, dim=-1) # [B, n_head, T, T]\n",
        "        att = self.attn_dropout(att) # [B, n_head, T, T]\n",
        "\n",
        "        # Weighted sum of values\n",
        "        y = att @ v # [B, n_head, T, n_embd/n_head]\n",
        "\n",
        "        # Reshape and project\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # [B, T, n_embd]\n",
        "        y = self.c_proj(y) # [B, T, n_embd]\n",
        "        y = self.resid_dropout(y) # [B, T, n_embd]\n",
        "\n",
        "        return y\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.dim, 4 * config.dim) # [n_embd, 4 * n_embd]\n",
        "        self.c_proj = nn.Linear(4 * config.dim, config.dim) # [4 * n_embd, n_embd]\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x) # [B, T, 4 * n_embd]\n",
        "        x = F.gelu(x) # [B, T, 4 * n_embd]\n",
        "        x = self.c_proj(x) # [B, T, n_embd]\n",
        "        x = self.dropout(x) # [B, T, n_embd]\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.dim) # [n_embd]\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.dim) # [n_embd]\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # [B, T, n_embd] #calls forward func of multihead attention\n",
        "        x = x + self.mlp(self.ln_2(x)) # [B, T, n_embd]\n",
        "        return x\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.dim) # [vocab_size, n_embd]\n",
        "        self.wpe = nn.Embedding(config.max_seq_len, config.dim) # [max_seq_len, n_embd]\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.dim) # [n_embd]\n",
        "        self.lm_head = nn.Linear(config.dim, config.vocab_size, bias=False) # [n_embd, vocab_size]\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size() # [B, T]  o/p-->torch.Size([4, 128])\n",
        "\n",
        "        # Positional embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0) # [1, T]\n",
        "\n",
        "        # Token and position embeddings\n",
        "        tok_emb = self.wte(idx) # [B, T, n_embd]\n",
        "        pos_emb = self.wpe(pos) # [1, T, n_embd]\n",
        "\n",
        "        # Combine embeddings and apply dropout\n",
        "        x = self.drop(tok_emb + pos_emb) # [B, T, n_embd]\n",
        "\n",
        "        # Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x) # [B, T, n_embd] #calls forward method inside TransformerBlock\n",
        "\n",
        "        # Final layer norm and linear projection\n",
        "        x = self.ln_f(x) # [B, T, n_embd]\n",
        "        logits = self.lm_head(x) # [B, T, vocab_size]\n",
        "\n",
        "        return logits\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_len):\n",
        "        self.text = text\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = len(text) // seq_len  # Non-overlapping sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.seq_len\n",
        "        return (\n",
        "            self.text[start:start+self.seq_len],    # Input sequence\n",
        "            self.text[start+1:start+self.seq_len+1] # Target sequence\n",
        "        )\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def estimate_memory(model):\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "    total_size = (param_size + buffer_size) / (1024 ** 2)  # Convert to MB\n",
        "    return total_size\n",
        "\n",
        "def train(model, dataset, config, epochs, lr=1e-4, log_dir=\"logs\"):\n",
        "\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"Model Size: {count_parameters(model) / 1e6:.2f}M parameters\")\n",
        "    print(f\"Estimated Model Memory Usage: {estimate_memory(model):.2f} MB\")\n",
        "\n",
        "    model.train()\n",
        "    step = 0  # Initialize step counter\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            step += 1  # Increment step counter\n",
        "\n",
        "            #print(f\"Step {step} (Epoch {epoch+1}, Batch {batch_idx+1})\")\n",
        "\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # print(f\"Outputs shape: {outputs.shape}\")  # Should be (batch_size, seq_len, vocab_size)\n",
        "            # print(f\"Targets shape: {targets.shape}\")  # Should be (batch_size, seq_len)\n",
        "\n",
        "            loss = criterion(outputs.view(-1, config.vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}, Step {step}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\" Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        if avg_loss < 0.099999:\n",
        "            print(\"Early stopping as loss is below target\")\n",
        "            break\n",
        "\n",
        "        # Save model checkpoint\n",
        "        torch.save(model.state_dict(), f\"{log_dir}/model_epoch_{epoch+1}.pt\")\n",
        "        print(\"Checkpoint saved..\")\n",
        "\n",
        "        # Log to file\n",
        "        with open(f\"{log_dir}/training_log.txt\", \"a\") as log_file:\n",
        "            log_file.write(f\"{datetime.now()} - Step {step} - Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.6f}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = Config()\n",
        "    model = DecoderOnlyTransformer(config)\n",
        "\n",
        "    with open('input.txt','r') as fp:\n",
        "        text = fp.read()\n",
        "\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    idx = torch.tensor(enc.encode(text), dtype=torch.long)\n",
        "    #print(idx.shape)\n",
        "    #print(idx)\n",
        "\n",
        "    dataset = TextDataset(idx, seq_len=512)\n",
        "    train(model, dataset, config, epochs=100)\n",
        "\n",
        "    torch.save(model.state_dict(), \"final_model.pt\")\n",
        "    print(\"Model training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWQmqTx7YSUO",
        "outputId": "483c3e7e-3340-4399-e724-3b9af03006c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-73d2ba46bc05>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/logs/final_model.pt\"))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DecoderOnlyTransformer(\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(2048, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (blocks): ModuleList(\n",
              "    (0-7): 8 x TransformerBlock(\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiHeadAttention(\n",
              "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FeedForward(\n",
              "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the model\n",
        "model = DecoderOnlyTransformer(config)\n",
        "model.load_state_dict(torch.load(\"final_model.pt\"))\n",
        "model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2_R2vmyzYT38"
      },
      "outputs": [],
      "source": [
        "# Example input text\n",
        "input_text = \"More learned than the ears--waving thy head, Which often, thus, correcting thy stout heart,\"\n",
        "\n",
        "# Encode the input text\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "input_ids = torch.tensor(enc.encode(input_text), dtype=torch.long).unsqueeze(0)  # Add batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezdDwBqYgfzR",
        "outputId": "21fd8c61-71a5-4156-b407-e366c1825d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4499,   621,   262, 11368,   438,    86,  2703, 11906,  1182,    11,\n",
            "          9022,  1690,    11,  4145,    11,  3613, 11906, 39171,  2612,    11,\n",
            "          4145]])\n",
            "Predicted text:  learned than the ears--waving thy head, Which often, thus, save thy stout heart, thus\n"
          ]
        }
      ],
      "source": [
        "# Forward pass\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    logits = model(input_ids)\n",
        "\n",
        "# Get the predicted token indices\n",
        "predicted_indices = torch.argmax(logits, dim=-1)\n",
        "print(predicted_indices)\n",
        "\n",
        "# Decode the predicted indices to text\n",
        "predicted_text = enc.decode(predicted_indices.view(-1).tolist())  # Flattens the tensor\n",
        "print(\"Predicted text:\", predicted_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVF1zX3igpJp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
